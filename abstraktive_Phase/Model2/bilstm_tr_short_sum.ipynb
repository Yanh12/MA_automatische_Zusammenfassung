{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bilstm_tr_short_sum",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSt1G0e0LzjJGqWaPS/BnM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yanh12/MA_automatische_Zusammenfassung/blob/master/abstraktive_Phase/Model2/bilstm_tr_short_sum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew5DrGdIo5Es",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3237a5fa-0c61-435d-8199-25833853b595"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "print(tf.__version__)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import re\n",
        "import zipfile\n",
        "import itertools\n",
        "#import dependencies\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional,LSTMCell\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMELLgYK2LDK",
        "colab_type": "text"
      },
      "source": [
        "This model is based on TF tuturial (https://www.tensorflow.org/tutorials/text/nmt_with_attention) for NMT using TF 2.0 with modifications.\n",
        "Specifically to say:\n",
        "1. modifying how texts are preprocessed; which to be tokenized which not;\n",
        "2. Using pretrained word2vec instead of the built-in keras embedding;\n",
        "3. Using BiLstm instead of LSTM for Encoder\n",
        "4. Other small modifications when needed (eg. some explanations)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY9klERzpBO6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "55f34948-690e-481f-9f96-0b5e84db487d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIbgLQFypJZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/MA_colab')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSJaLWq7pM3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import text files\n",
        "default_path = '/content/drive/My Drive/MA_colab/bilstm_attn_beam/tr_short_sum/'\n",
        "with open ('/content/drive/My Drive/MA_colab/abs_infos.p', 'rb') as filehandle:\n",
        "  abs_selected = pickle.load(filehandle)\n",
        "abs_selected = [x.lower() for x in abs_selected]\n",
        "with open('/content/drive/My Drive/MA_colab/textRank_texts.p', 'rb') as filehandle:\n",
        "  texts = pickle.load(filehandle)\n",
        "texts = [' '.join(x) for x in texts]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oci98xug10UZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "7bd8b3e1-6eca-456d-aea1-2c8be10f4edb"
      },
      "source": [
        "for i in range(5):\n",
        "    print(texts[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after annotating instances based on simple queries and mlsupported ranked queries, we considered the natural next step to be searching automatically for the phenomenon in question utilizing machine learning, using arbitrary sentences from the corpus as input for the classifier instead of only the results returned by a query. our approach is different from active learning as our goal is not to improve the training efficiency of the classifier but rather to allow the user to interactively find and label as many true instances of a phenomenon as possible in a large corpus. consider the following typical scenario incorporating the ranking: a user constructs a query based on various features, such as pos tags or lemmata, which are used to search for matching sentences, e.g. the result is a list of sentences presented in a keywordsincontext view, along with an annotation field . we interpret this as the classifier being successful in helping the annotators after a brief training phase identifying additional occurrences of particular variants of a phenomenon as covered by the queries, but not easily generalizing to variants substantially different from those covered by the queries. to reduce the time and effort required, we present an extension of the annotationbyquery approach that introduces a ranking of the query results by means of machine learning; we order the results by confidence of the used classifier. we demonstrate our novel approach on the task of locating noncanonical constructions and conduct an intrinsic evaluation of the accuracy of the system augmented with machine learning output on the data annotated by expert linguists. the classifier producing the ranking can be invoked by the user at any time; it can be configured in certain characteristics, e.g. the annotations of which users should be used as training data, or how many of the selected users have to agree on an annotation for it to be included. in addition, webanno supports automatic annotation similar to our approach, also employing machine learning to build models from the data annotated by users. repeatedly annotating those highestranking results and reranking allows for quickly annotating instances of the phenomenon, while also improving the classifier accuracy at the same time. we conducted a cursory, “realworld” test regarding the speed of the ranking system.7 training the svm on differently sized subsets of the 449 sentences returned by a test query, we measured the time from clicking the rank results button until the process was complete and the gui had updated to reorder the sentences . to tackle this problem, we use the annotations already created by users on the search results to train a classifier. in this mode, the svm is first trained from all previously labeled instances for a given phenomenon, without taking the queries into account that were used to find those instances.\n",
            "however, these scores show that having only three labeled tokens per class is sufficient to capture most of the necessary distinctions for the hdpccg. in all cases, we assume that we can identify punctuation marks, which are moved to their own cluster and ignored for the purposes of tagging and parsing evaluation. we will evaluate three clustering approaches: brown clusters brown clusters assign each word to a single cluster using an aglomerative clustering that maximizes the probability of the corpus under a bigram class conditional model. this is likely due to morphology distinguishing otherwise ambiguous lemmas. that pos tagging metrics are not correlated with parsing performance. was the only participant competing in the pascal challenge that operated over raw text . , we train and test our systems on the english and chinese ccgbanks, and report directed labeled f1 and undirected unlabeled f1 over ccg dependencies . might outperform the systems we use here. note that the systems in st do not have access to any goldstandard pos tags, whereas our system has access to the gold tags for best shared task baseline. setting this value will prove important when moving between datasets of drastically different sizes. but we will see that three per cluster is sufficient to label most tokens correctly. we do the same here, use the dev set for choosing a hdpccg hyperparameter, and then present final results for comparison on the test section. any word can act as a modifier, i.e. have a category of the form x|x our results align with this research, leading us to believe that this paradigm of guided minimal supervision is a fruitful direction for future work. if it is adjacent to a word with category x or x|y, and any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the national science foundation, the national institutes of health, darpa or the u.s. government. we induce 36 tags for english and 37 for chinese to match the number of tags present in the treebanks . all of this confirms the observations of headden et al. to incorporate multiple types of features either at a token level or at a type level ). how to most effectively use seed knowledge and annotation is still an open question. more importantly, we see that, at least for english, despite clear differences in tagging performance, the parsing results are much more similar. of more immediate relevance to our task is the ability to accurately identify nouns and verbs: noun, verb, and other recall we measure the recall of our threeway labeling scheme of clusters as noun/verb/other against the universal pos tags of each token. et results table 1 presents the parsing and tagging development results on the two ccg corpora. however, their approach did not outperform the six baseline systems provided.\n",
            "as shown in figure 1, we construct a wordgraph using all the sentences assigned to a specific class by the classifier. the solution to the optimization problem decides the paths that are selected in the final section summary. as the classes might be unbalanced, we apply resampling on the training set. each article wj has several sections denoted as sjicji where sji and cji refer to the section title and content of the ith section in the jth article, respectively. categories on wikipedia group together pages on similar subjects. in our experiments, we set smax to 5 to generate short concise summaries in each section. table 1 shows the most frequent sections in each category. setting d to 0.80 in our experiments provided the best content selection results. naturally, we leverage knowledge existing in the categories to build our text classifier. we also compare wikikreator’s output with an existing wikipedia generation system of sauper and barzilay 11 that employs a perceptron learning framework to learn topic specific extractors. the rf classifier classifies the excerpts into one of the most frequent classes . web content from the search results obtained in the previous step requires cleaning to retain only the relevant information. instead, we use a classifier trained using topic distribution features to identify relevant content for the stub. the number of instances denotes the total number of the most frequent sections in each category. svmwv performs better in the category on diseases than the other two categories and the performance is comparable to . further, we avoid redundant sentences in the summary that carry similar information. the set of topics are t1, t2, t3,..., tm while pji refers to the probability of topic m of content cji. the results of our evaluation confirm the benefits of using abstractive summarization for content generation over approaches that do not use summarization. we populate the sections on wikipedia using the final summaries generated for each section along with the section title. however, these methods suffer from severe deficiencies. given a category, the api is capable of extracting all the articles under it. we retrieve relevant excerpts from the web by formulating queries. figure 1 shows a simple example of the wordgraph generation technique. global informativeness: in this work, we presented wikikreator that can generate content automatically to improve wikipedia stubs. figure 1 shows an illustration of our approach where the set of sentences assigned to a particular section are used to create the wordgraph. we extract topic distribution vectors using latent dirichlet allocation and use the features to train a random forest classifier . to address the abovementioned issues, we present wikikreator – a system that can automatically generate content for wikipedia stubs. sji is the corresponding label for this training instance. hence, categories characterize wikipedia articles surprisingly well . contents from the most frequent sections are each considered as a document and lda is applied to generate documenttopic distributions. to address the issue of copyright violation, multidocument abstractive summarization is required.\n",
            "lowrank tensor representations following lei et al. to learn the parameters of the lowrank tensors, we use an alternating optimization method : performing gradient descent on one of the parameter matrices to minimize the loss function while holding the other two fixed , then repeating for the other parameter matrices in turn. tensor models for verbs we find that lowrank tensors for verbs achieve comparable or better performance than fullrank tensors on both verb disambiguation and sentence similarity tasks, while reducing the number of parameters that must be learned and stored for each verb by at least two orders of magnitude, and cutting training time in half. previous work on the transitive verb construction within the categorial framework includes a twostep linearregression method for the construction of the full verb tensors and a multilinear regression method combined with a twodimensional plausibility space . we compare the performance of the lowrank tensors against full tensors on two tasks. in the process, the model learns vector embeddings for both the svo triples and for the words in the sentences such that svo vectors have a high dot product with their contextual word vectors. we learn the component vectors and apply the composition without ever constructing the full tensors and thus we are able to improve on both memory usage and efficiency. while there are several possible definitions of the sentence space , we follow previous work by using a contextual sentence space consisting of content words that occur within the same sentences as the svo triple. comparing against averaged annotator scores, our best result on gs11 is 0.47 for the fullrank tensor with pv vectors, and our best nonaddition result on ks14 is 0.68 for the k=40 tensor with pv vectors . use vectors generated by a neural language model to construct verb matrices and several different composition operators to generate the composed subjectverbobject sentence representation. for a tensor in the form of eq. , the output svo vector is given by in this dataset, however, the two sentences in each pair have no lexical overlap: neither subjects, objects, nor verbs are shared. the parameter matrices are randomly initialized.3 each transitive verb has its own thirdorder tensor, which defines this bilinear function. let mv be the number of training instances for v , where the ith instance is a triple of vectors , o , t ), which are the distributional vectors for the subject noun, object noun, and the svo triple, respectively. tamara polajnar is supported by erc starting grant discotex . rather than placing a regularization penalty on the tensor parameters, we use early stopping if the loss increases on a validation set consisting of 10% of the available svo triples for each verb. this approach follows recent work on using lowrank tensors to parameterize models for dependency parsing and semantic role labelling .\n",
            "denero and uszkoreit presented a preordering method which builds a monolingual parsing model and a tree reordering model from parallel text. lemma 1. et al. these are the data obtained with the embased word alignment learning. v is the score of the state, which is the sum of the input: sentence x, huang et autorules this system uses the rulebased preordering method which automatically learns the rules from wordaligned data using the variant 1 learning algorithm described in . latticebased minimum error rate training is applied to optimize feature weights. scores for the nodes constructed so far. al. the class of the permutations which can be represented by btg is known as separable permutations in combinatorics. lader in all the experiments so far, the decoder was allowed to reorder even after preordering was carried out. neubig let 4b denote the vector of feature functions for the btg tree node m, and a denote the vector of feature weights. model parameters can be learned using latent variable perceptron with the early update technique , since the parsing method provides an easy way for checking the reachability of each parser state to valid final states. we assume that each training example consists of a sentence x and its word order in a target language y = y0 · · · y|x|−1, where yi is the position of xi in the target language. returns the validity of state s. one advantage of the topdown parsing algorithm table 2 lists several preordering systems were prepared in order to compare the following six systems: nopreordering this is a system without preordering. at the beginning, a tree which covers all the words in the sentence is considered. neubig the parallel data for each language pair consists of around 400 million source and target words. according to the relation of the lengths of and . the function valid in figure 6 however, there are differences in the two systems, and they had different accuracies. tromble and eisner studied preordering based on a linear ordering problem by defining a pairwise preference matrix. however, the method has the problem of computational efficiency. is that it is easy to track the validity of each state. feature weights a, beam width k. output: btg parse tree. visweswariah et word classes in table 2 are obtained by using brown clusters . in this study, we propose a topdown parsing algorithm in order to achieve fast btgbased preordering. the hash values are used as feature names. on termination, the parser has the final state , , v), because the stack is empty and there are actions in total. this formulation is equivalent to the original formulation based on chunk fragmentation by talbot et al. . we remove training examples which cannot be represented with btg beforehand and set w of the initial state to true. for other languages, we use a pos tagger based on pos projection which does not need pos annotated data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_7ukHhKpXKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tune the text for the further tokenization\n",
        "to_exclude = '!\"#$%&()*+/:;<=>@[\\\\]^_`{|}~\\t\\n'\n",
        "to_tokenize = '.,:;!?-'\n",
        "texts_modified = []\n",
        "for article in texts:\n",
        "  article = re.sub(r'(['+to_tokenize+'])', r' \\1 ', article)\n",
        "  article = re.sub(r'  +', ' ', article)\n",
        "  texts_modified.append(article)\n",
        "abs_modified = []\n",
        "for summary in abs_selected:\n",
        "  summary = re.sub(r'(['+to_tokenize+'])', r' \\1 ', summary)\n",
        "  summary = re.sub(r'  +', ' ', summary)\n",
        "  abs_modified.append(summary)\n",
        "\n",
        "#add START and END tag for each selected abstract\n",
        "abs_tagged = ['<start> ' + x + ' <end>' for x in abs_modified]\n",
        "texts_tagged = ['<start> ' + x + ' <end>' for x in texts_modified]\n",
        "\n",
        "max_len_text = 500\n",
        "max_len_summary = 50\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evjYCxVYHltl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train,validation & test parts\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(texts_tagged[:15000],abs_tagged[:15000], test_size=0.1,random_state=0,shuffle=False)\n",
        "x_test = texts_tagged[15000:]\n",
        "y_test = abs_modified[15000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDlspMbBqmlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#text Tokenizer by keras >> convert text sequences into integer representation\n",
        "x_tokenizer = Tokenizer(filters='')\n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "x_tr = x_tokenizer.texts_to_sequences(x_tr)\n",
        "x_val = x_tokenizer.texts_to_sequences(x_val)\n",
        "#padding zero to max length\n",
        "x_tr = pad_sequences(x_tr,  maxlen=max_len_text, padding='post')\n",
        "x_val = pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
        "#defing the vocabulary size\n",
        "x_voc_size = len(x_tokenizer.word_index) +1\n",
        "\n",
        "#summary Tokenizer\n",
        "y_tokenizer = Tokenizer(filters='')\n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "y_tr = y_tokenizer.texts_to_sequences(y_tr)\n",
        "y_val = y_tokenizer.texts_to_sequences(y_val)\n",
        "y_tr = pad_sequences(y_tr, maxlen=max_len_summary, padding='post')\n",
        "y_val = pad_sequences(y_val, maxlen=max_len_summary, padding='post')\n",
        "y_voc_size = len(y_tokenizer.word_index) +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWkRa-0eNVdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word2index\n",
        "x_word_index = x_tokenizer.word_index\n",
        "y_word_index = y_tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UXLzZnslqpI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2098a284-265b-4fdb-84b7-1e4a2928cf75"
      },
      "source": [
        "print(x_tokenizer.word_index['<start>']) \n",
        "x_voc_size = len(x_tokenizer.word_index)+1  \n",
        "y_voc_size = len(y_tokenizer.word_index)+ 1\n",
        "print(\"input_vocab_size : \", x_voc_size)\n",
        "print(\"output_vocab_size : \" ,y_voc_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54\n",
            "input_vocab_size :  136202\n",
            "output_vocab_size :  20640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTJ1-DHrYfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model parameters\n",
        "BATCH_SIZE = 32 #due to colab capacity, no bigger batch size possible\n",
        "BUFFER_SIZE = len(x_tr)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dims = 300\n",
        "rnn_units = 150\n",
        "rnn_units_decoder = 300\n",
        "dense_units = 300\n",
        "Dtype = tf.float32\n",
        "Tx = 500 #longst input\n",
        "Ty = 50 #longst output "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUyhGJrJrh7b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "03b7183f-ac4f-4510-f0c0-670f093d7e1c"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_tr, y_tr)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 500)\n",
            "(32, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUpgz4wtrtDd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b09b1193-ae54-40f9-8192-d59fffe05076"
      },
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 500)\n",
            "(32, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgndaBd41PG8",
        "colab_type": "text"
      },
      "source": [
        "Using Pre-trained embedding models instead of keras Embedding \n",
        "Note that there are lots of possible ways to embed words to dense vectors of fixed langth. Keras Embedding modul provides this embedding as other layers-- i.e. minimize loss as its ultimate goal.\n",
        "However, word2vec tech is the one that trys to capture the SEMANTIC MEANINGS of words, not only simply densing them as minimizing the loss.\n",
        "Therefore, for text understanding related tasks, better using pre-trained word2vec models as embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0dm7Pgt9RnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#use pre-trained word embedding as Embedding.layer of the model\n",
        "#load pretrained word embedding\n",
        "\"\"\"\n",
        "corpus-specific word embedding can provide some more precise information, but \n",
        "it also has the disadvantage that many rare word get ignored.\n",
        "So, here where use a combined way to better represent the words:\n",
        "   1. if the word is in the corpus-specific word-embedding file, use it.\n",
        "   2. if the work is not in this file but in the pretrained embeddings from the\n",
        "   commonly crawled documents, then use this.\n",
        "   3. if a word neither in the specific file nor in the common file, initialize\n",
        "   it with random numbers\n",
        "\"\"\"\n",
        "cl_embeddings_index = {}\n",
        "with open('/content/drive/My Drive/MA_colab/CL_word2vec_300dim.txt') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        cl_embeddings_index[word] = coefs\n",
        "\n",
        "\n",
        "embeddings_index = {}\n",
        "with open('/content/drive/My Drive/MA_colab/glove.6B.300d.txt') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "\n",
        "#embedding_layer for Encoder\n",
        "embedding_matrix_encoder = np.zeros((x_voc_size,embedding_dims))\n",
        "\"\"\"\n",
        "if the word is in the word embedding dict, embedding_vector gets the corresponding value\n",
        "if not, initilize the vector with random numbers\n",
        "\"\"\"\n",
        "for word, i in x_word_index.items():\n",
        "  if word in cl_embeddings_index.keys():\n",
        "     embedding_vector = cl_embeddings_index.get(word)\n",
        "     embedding_matrix_encoder[i] = embedding_vector\n",
        "  elif word in embeddings_index.keys():\n",
        "     embedding_vector = embeddings_index.get(word)\n",
        "     embedding_matrix_encoder[i] = embedding_vector\n",
        "  else:\n",
        "     embedding_vector=np.random.uniform(-1.0,1.0,(300,))\n",
        "     embedding_matrix_encoder[i] = embedding_vector\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "embedding_layer = Embedding ()\n",
        "initilize the embedding layer:\n",
        "params: input_dim = the vocabulary size\n",
        "        output_dim = embedding dimension\n",
        "        embeddings_initilizer: to initilize the embeddings\n",
        "        input_length = the max length of the source text\n",
        "        trainable = False, because we want the pre-trained model and not update it\n",
        "\"\"\"\n",
        "embedding_layer_encoder = tf.keras.layers.Embedding(input_dim = x_voc_size, \n",
        "                                    output_dim = embedding_dims,\n",
        "                                    weights=[embedding_matrix_encoder], \n",
        "                                    input_length = max_len_text, \n",
        "                                    trainable = False)\n",
        "\n",
        "#embedding_layer for Decoder\n",
        "embedding_matrix_decoder = np.zeros((y_voc_size,embedding_dims))\n",
        "for word, i in y_word_index.items():\n",
        "   if word in cl_embeddings_index.keys():\n",
        "     embedding_vector_y = cl_embeddings_index.get(word)\n",
        "     embedding_matrix_decoder[i] = embedding_vector_y\n",
        "   elif word in embeddings_index.keys():\n",
        "     embedding_vector_y = embeddings_index.get(word)\n",
        "     embedding_matrix_decoder[i] = embedding_vector_y\n",
        "   else:\n",
        "     embedding_vector_y = np.random.uniform(-1.0,1.0,(300,))\n",
        "     embedding_matrix_decoder[i] = embedding_vector_y\n",
        "\n",
        "embedding_layer_decoder = tf.keras.layers.Embedding(input_dim = y_voc_size, \n",
        "                                    output_dim = embedding_dims,\n",
        "                                    weights=[embedding_matrix_decoder],\n",
        "                                    input_length = max_len_summary, \n",
        "                                    trainable = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE5XTFGtr3CH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the model\n",
        "#ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,x_voc_size,embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = embedding_layer_encoder\n",
        "\n",
        "        #Bi-LSTM\n",
        "        #not like LSTM, Bi-LSTM has forward and backword hidden state and content state\n",
        "        #encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
        "        self.encoder_birnnlayer = Bidirectional(LSTM(rnn_units,return_state=True,return_sequences=True))\n",
        "        \n",
        "      \n",
        "#DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,y_voc_size, embedding_dims, rnn_units_decoder):\n",
        "        super().__init__()\n",
        "        self.decoder_embedding = embedding_layer_decoder\n",
        "        self.dense_layer = Dense(y_voc_size)\n",
        "        self.decoder_rnncell = LSTMCell(rnn_units_decoder)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        # Create attention mechanism with memory = None\n",
        "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell = self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
        "                                                output_layer=self.dense_layer)\n",
        "\n",
        "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "        #return tfa.seq2seq.LuongAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "    # wrap decode-rnn cell  \n",
        "    def build_rnn_cell(self, batch_size ):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
        "                                                attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
        "                                                                dtype = Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "        return decoder_initial_state\n",
        "\n",
        "\n",
        "\n",
        "encoderNetwork = EncoderNetwork(x_voc_size,embedding_dims, rnn_units)\n",
        "decoderNetwork = DecoderNetwork(y_voc_size,embedding_dims, rnn_units_decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiyPryBQsLvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optimizing\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlhB9MlZsSGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loss function\n",
        "def loss_function(y_pred, y):\n",
        "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                                                  reduction='none')\n",
        "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
        "    mask = tf.logical_not(tf.math.equal(y,0))  \n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss = mask* loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrlD2omyscHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6098f001-8525-4567-b11a-91f16b900457"
      },
      "source": [
        "decoderNetwork.attention_mechanism.memory_initialized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0DhCkIhsfIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#batch training & teacher-forcing\n",
        "def train_step(input_batch, output_batch, encoder_initial_cell_state):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "\n",
        "\n",
        "        #Outputs of BiLstm\n",
        "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
        "        encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoderNetwork.encoder_birnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state =encoder_initial_cell_state)\n",
        "        state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
        "        state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
        "       \n",
        "\n",
        "        \n",
        "        # Prepare correct Decoder input & output sequence data\n",
        "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "        #compare logits with timestepped +1 version of decoder_input\n",
        "        decoder_output = output_batch[:,1:] #ignore <start>\n",
        "\n",
        "\n",
        "        # Decoder Embeddings\n",
        "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
        "        decoderNetwork.attention_mechanism.setup_memory(encoder_outputs)\n",
        "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                           encoder_state=[state_h, state_c],\n",
        "                                                                           Dtype=tf.float32)\n",
        "        \n",
        "        #BasicDecoderOutput        \n",
        "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
        "                                               sequence_length=BATCH_SIZE*[Ty-1])\n",
        "\n",
        "        logits = outputs.rnn_output\n",
        "        #Calculate loss\n",
        "        loss = loss_function(logits, decoder_output)\n",
        "\n",
        "    #Returns the list of all layer variables / weights.\n",
        "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
        "    # differentiate loss wrt variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    #grads_and_vars – List of(gradient, variable) pairs.\n",
        "    grads_and_vars = zip(gradients,variables)\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gg8GpwbsveE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a9d930a-1b48-4078-dc3f-724055651b5b"
      },
      "source": [
        "#training\n",
        "chkpoint_prefix = os.path.join(default_path, \"chkpoint\")\n",
        "if not os.path.exists(default_path):\n",
        "    os.mkdir(default_path)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer = optimizer, encoderNetwork = encoderNetwork, \n",
        "                                 decoderNetwork = decoderNetwork)\n",
        "\n",
        "try:\n",
        "    status = checkpoint.restore(tf.train.latest_checkpoint(default_path))\n",
        "    print(\"Checkpoint found at {}\".format(tf.train.latest_checkpoint(default_path)))\n",
        "except:\n",
        "    print(\"No checkpoint found at {}\".format(default_path))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint found at /content/drive/My Drive/MA_colab/bilstm_attn_beam/tr_short_sum/chkpoint-40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEKsbQ0HtXyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RNN LSTM hidden and memory state initializer\n",
        "def initialize_initial_state():\n",
        "    #for Bi-LSTM\n",
        "    return [tf.zeros((BATCH_SIZE, rnn_units)) for i in range(4)]\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uetAmVBStYcp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "8e7d25e7-a9fa-4f2f-9192-0ecb46e21bc6"
      },
      "source": [
        "epochs = 10\n",
        "loss=[]\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "    #print(encoder_initial_cell_state)\n",
        "\n",
        "    \n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%100 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))\n",
        "            checkpoint.save(file_prefix = chkpoint_prefix)\n",
        "            loss.append(batch_loss.numpy())\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 1.8633198738098145 epoch 1 batch 100 \n",
            "total loss: 2.3184497356414795 epoch 1 batch 200 \n",
            "total loss: 2.2541329860687256 epoch 1 batch 300 \n",
            "total loss: 2.3567042350769043 epoch 1 batch 400 \n",
            "total loss: 1.8945003747940063 epoch 2 batch 100 \n",
            "total loss: 2.220964193344116 epoch 2 batch 200 \n",
            "total loss: 2.1953065395355225 epoch 2 batch 300 \n",
            "total loss: 1.6874101161956787 epoch 2 batch 400 \n",
            "total loss: 1.7913634777069092 epoch 3 batch 100 \n",
            "total loss: 1.9121739864349365 epoch 3 batch 200 \n",
            "total loss: 1.634926438331604 epoch 3 batch 300 \n",
            "total loss: 1.9151082038879395 epoch 3 batch 400 \n",
            "total loss: 1.7671014070510864 epoch 4 batch 100 \n",
            "total loss: 1.5831454992294312 epoch 4 batch 200 \n",
            "total loss: 1.7650971412658691 epoch 4 batch 300 \n",
            "total loss: 2.0390472412109375 epoch 4 batch 400 \n",
            "total loss: 1.557253360748291 epoch 5 batch 100 \n",
            "total loss: 1.943574070930481 epoch 5 batch 200 \n",
            "total loss: 1.8533164262771606 epoch 5 batch 300 \n",
            "total loss: 1.7511740922927856 epoch 5 batch 400 \n",
            "total loss: 1.5450069904327393 epoch 6 batch 100 \n",
            "total loss: 1.3841360807418823 epoch 6 batch 200 \n",
            "total loss: 1.7743676900863647 epoch 6 batch 300 \n",
            "total loss: 1.818036437034607 epoch 6 batch 400 \n",
            "total loss: 1.753938913345337 epoch 7 batch 100 \n",
            "total loss: 1.7615429162979126 epoch 7 batch 200 \n",
            "total loss: 1.6070631742477417 epoch 7 batch 300 \n",
            "total loss: 1.7811849117279053 epoch 7 batch 400 \n",
            "total loss: 1.3822706937789917 epoch 8 batch 100 \n",
            "total loss: 1.4377198219299316 epoch 8 batch 200 \n",
            "total loss: 1.3943272829055786 epoch 8 batch 300 \n",
            "total loss: 1.8382785320281982 epoch 8 batch 400 \n",
            "total loss: 1.3133422136306763 epoch 9 batch 100 \n",
            "total loss: 1.2199435234069824 epoch 9 batch 200 \n",
            "total loss: 1.8153225183486938 epoch 9 batch 300 \n",
            "total loss: 1.4142333269119263 epoch 9 batch 400 \n",
            "total loss: 1.457194447517395 epoch 10 batch 100 \n",
            "total loss: 1.4184138774871826 epoch 10 batch 200 \n",
            "total loss: 1.4777215719223022 epoch 10 batch 300 \n",
            "total loss: 1.3278096914291382 epoch 10 batch 400 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "516DAFgAXm_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "4c21747c-ca87-4cd1-f2b9-4183f41e2d93"
      },
      "source": [
        "#validation\n",
        "epochs = 10\n",
        "loss_val=[]\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "    #print(encoder_initial_cell_state)\n",
        "\n",
        "    \n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset_test.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%10 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))\n",
        "            checkpoint.save(file_prefix = chkpoint_prefix)\n",
        "            loss_val.append(batch_loss.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 3.4076809883117676 epoch 1 batch 10 \n",
            "total loss: 3.9578630924224854 epoch 1 batch 20 \n",
            "total loss: 3.5703859329223633 epoch 1 batch 30 \n",
            "total loss: 3.1080923080444336 epoch 1 batch 40 \n",
            "total loss: 2.8575570583343506 epoch 2 batch 10 \n",
            "total loss: 2.342149496078491 epoch 2 batch 20 \n",
            "total loss: 2.6401419639587402 epoch 2 batch 30 \n",
            "total loss: 2.9707398414611816 epoch 2 batch 40 \n",
            "total loss: 2.003048896789551 epoch 3 batch 10 \n",
            "total loss: 2.050762176513672 epoch 3 batch 20 \n",
            "total loss: 1.9545886516571045 epoch 3 batch 30 \n",
            "total loss: 2.19987416267395 epoch 3 batch 40 \n",
            "total loss: 1.7025526762008667 epoch 4 batch 10 \n",
            "total loss: 1.8114888668060303 epoch 4 batch 20 \n",
            "total loss: 1.8422714471817017 epoch 4 batch 30 \n",
            "total loss: 1.702797293663025 epoch 4 batch 40 \n",
            "total loss: 1.6770187616348267 epoch 5 batch 10 \n",
            "total loss: 1.7573459148406982 epoch 5 batch 20 \n",
            "total loss: 1.6712416410446167 epoch 5 batch 30 \n",
            "total loss: 1.8442585468292236 epoch 5 batch 40 \n",
            "total loss: 1.4954500198364258 epoch 6 batch 10 \n",
            "total loss: 1.5052729845046997 epoch 6 batch 20 \n",
            "total loss: 1.3568288087844849 epoch 6 batch 30 \n",
            "total loss: 1.2269556522369385 epoch 6 batch 40 \n",
            "total loss: 1.0021154880523682 epoch 7 batch 10 \n",
            "total loss: 1.2093428373336792 epoch 7 batch 20 \n",
            "total loss: 1.1660908460617065 epoch 7 batch 30 \n",
            "total loss: 1.3399192094802856 epoch 7 batch 40 \n",
            "total loss: 0.8945463299751282 epoch 8 batch 10 \n",
            "total loss: 1.0330849885940552 epoch 8 batch 20 \n",
            "total loss: 0.9919710159301758 epoch 8 batch 30 \n",
            "total loss: 1.1139793395996094 epoch 8 batch 40 \n",
            "total loss: 0.9905494451522827 epoch 9 batch 10 \n",
            "total loss: 0.6407089829444885 epoch 9 batch 20 \n",
            "total loss: 0.855213463306427 epoch 9 batch 30 \n",
            "total loss: 0.9333137273788452 epoch 9 batch 40 \n",
            "total loss: 0.8264651894569397 epoch 10 batch 10 \n",
            "total loss: 0.6180287003517151 epoch 10 batch 20 \n",
            "total loss: 0.726893961429596 epoch 10 batch 30 \n",
            "total loss: 0.6814926266670227 epoch 10 batch 40 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW-IAuAetcnD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07507984-8b7b-4212-d4b5-8f85a81abb9e"
      },
      "source": [
        "#inference\n",
        "#if trained in same session else use checkpoint variable\n",
        "#decoder_embedding_matrix = tf.train.load_variable(checkpointdir, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
        "print(decoderNetwork.decoder_embedding.variables[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20640, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAdxjl3ntqqd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d1c80190-02e3-4990-d60d-a4b51f263dfd"
      },
      "source": [
        "[print(var) for var in tf.train.list_variables(\n",
        "    default_path) if re.match(r'.*decoder_embedding.*',var[0])]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE', [20640, 300])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8n1UlEFtuJx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9aee93f2-525e-46c2-da41-ea14be5a5ae0"
      },
      "source": [
        "decoder_embedding_matrix = tf.train.load_variable(\n",
        "    default_path, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "print(decoder_embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20640, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpby4he_vSTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test=texts_tagged[15000:]\n",
        "x_test = x_tokenizer.texts_to_sequences(x_test)\n",
        "input_x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_len_text, padding='post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik6iBqYpMU4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "end_token = y_tokenizer.word_index['<end>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84TAvT78yu-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test (test_input):\n",
        "\n",
        "    beam_width = 8\n",
        "\n",
        "    inference_batch_size = test_input.shape[0]\n",
        "    inp = tf.convert_to_tensor(test_input)            \n",
        "    encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "\n",
        "    def initialize_initial_state_inference():\n",
        "        #for Bi-LSTM\n",
        "        return [tf.zeros((inference_batch_size, rnn_units)) for i in range(4)]\n",
        "\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoderNetwork.encoder_birnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state =initialize_initial_state_inference())\n",
        "    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
        "    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
        "\n",
        "    start_tokens = tf.fill([inference_batch_size],y_tokenizer.word_index['<start>'])\n",
        "    end_token = y_tokenizer.word_index['<end>']\n",
        "\n",
        "    decoder_input = tf.expand_dims([y_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
        "    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "    encoder_memory = tfa.seq2seq.tile_batch(encoder_outputs, beam_width)\n",
        "    decoderNetwork.attention_mechanism.setup_memory(encoder_memory)\n",
        "    print(\"beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] :\", encoder_memory.shape)\n",
        "    #set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
        "    decoder_initial_state = decoderNetwork.rnn_cell.get_initial_state(batch_size = inference_batch_size* beam_width,dtype = Dtype)\n",
        "    encoder_state = tfa.seq2seq.tile_batch([state_h, state_c], multiplier=beam_width)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "\n",
        "    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoderNetwork.rnn_cell,beam_width=beam_width,\n",
        "                                                 output_layer=decoderNetwork.dense_layer)\n",
        "\n",
        "\n",
        "    # Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "    # One heuristic is to decode up to two times the source sentence lengths.\n",
        "    maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "    #initialize inference decoder\n",
        "\n",
        "    (first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                                start_tokens = start_tokens,\n",
        "                                end_token=end_token,\n",
        "                                initial_state = decoder_initial_state)\n",
        "    #print( first_finished.shape)\n",
        "    #print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
        "\n",
        "    inputs = first_inputs\n",
        "    state = first_state  \n",
        "    predictions = np.empty((inference_batch_size, beam_width,0), dtype = np.int32)\n",
        "    beam_scores =  np.empty((inference_batch_size, beam_width,0), dtype = np.float32)                                                                            \n",
        "    for j in range(maximum_iterations):\n",
        "        beam_search_outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "        inputs = next_inputs\n",
        "        state = next_state\n",
        "        outputs = np.expand_dims(beam_search_outputs.predicted_ids,axis = -1)\n",
        "        scores = np.expand_dims(beam_search_outputs.scores,axis = -1)\n",
        "        predictions = np.append(predictions, outputs, axis = -1)\n",
        "        beam_scores = np.append(beam_scores, scores, axis = -1)\n",
        "    #print(predictions.shape) \n",
        "    #print(beam_scores.shape)\n",
        "    \n",
        "    return predictions, beam_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc8hjVUy39pA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "c2b0d9d1-4d1c-41a8-a7b1-d98251d3f0d9"
      },
      "source": [
        "#due to the limitattion of gpu memory, test is done in batchs\n",
        "test_batch = 32\n",
        "iters = len(x_test) // test_batch \n",
        "last_iter = iters+1\n",
        "all_predictions = []\n",
        "all_beam_scores = []\n",
        "for i in range(iters+1):\n",
        "    test_input = input_x_test[test_batch*i : test_batch*(i+1)]\n",
        "    predictions, beam_scores = test(test_input)\n",
        "    all_predictions.append(predictions)\n",
        "    all_beam_scores.append(beam_scores)\n",
        "    if i == last_iter:\n",
        "        test_input = input_x_test[test_batch*last_iter:]\n",
        "        predictions, beam_scores = test(test_input)\n",
        "        all_predictions.append(predictions)\n",
        "        all_beam_scores.append(beam_scores)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (136, 500, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdVEYjkf6yRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_sum = []\n",
        "for i in range (len(all_predictions)):\n",
        "    #select the predicted summary with highest beam score as the final result\n",
        "    predictions = all_predictions[i]\n",
        "    beam_scores = all_beam_scores[i]\n",
        "    for j in range(len(predictions)):\n",
        "        output_beams_per_sample = predictions[j,:,:]\n",
        "        score_beams_per_sample = beam_scores[j,:,:]\n",
        "        for beam, score in zip(output_beams_per_sample,score_beams_per_sample) :\n",
        "            seq = list(itertools.takewhile( lambda index: index !=end_token, beam))\n",
        "            score_indexes = np.arange(len(seq))\n",
        "            beam_score = score[score_indexes].sum()\n",
        "            predicted = sorted ({(\" \".join( [y_tokenizer.index_word[w] for w in seq])):beam_score}.keys(),reverse=True)[0]\n",
        "        predicted_sum.append(predicted)\n",
        "\n",
        "#store the results\n",
        "for i in range(len(predicted_sum)):\n",
        "    ref = open(default_path + \"ref/ref_{}.txt\".format(i), 'w')\n",
        "    ref.write(y_test[i])\n",
        "    ref.close()\n",
        "    summ = open(default_path + \"predicted/sum_{}.txt\".format(i), 'w')\n",
        "    summ.write(predicted_sum[i])\n",
        "    summ.close()\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xjRCCGasRrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "b3f9f51e-7570-4efd-cbed-c0e9eeb49d3b"
      },
      "source": [
        "#to see beam scores of a summary\n",
        "for i in range(len(predicted_sum[:5])):\n",
        "    print(\"-----------------\")\n",
        "    print(\"Original Summary:\")\n",
        "    print(y_test[i])\n",
        "    #print(\"-----------------\")\n",
        "    #print(\"\\nPredicted Summary:\")\n",
        "    #print(\"---------------------------------------------\")\n",
        "    output_beams_per_sample = predictions[i,:,:]\n",
        "    score_beams_per_sample = beam_scores[i,:,:]\n",
        "    for j,(beam, score) in enumerate(zip(output_beams_per_sample,score_beams_per_sample)) :\n",
        "        seq = list(itertools.takewhile( lambda index: index !=end_token, beam))\n",
        "        score_indexes = np.arange(len(seq))\n",
        "        beam_score = score[score_indexes].sum()\n",
        "        #print (\"version{}: \".format(j), (\" \".join( [y_tokenizer.index_word[w] for w in seq]), \" beam score: \", beam_score))\n",
        "        #print (\" \".join( [y_tokenizer.index_word[w] for w in seq]), \" beam score: \", beam_score)\n",
        "        predicted = sorted ({(\" \".join( [y_tokenizer.index_word[w] for w in seq])):beam_score}.keys(),reverse=True)[0]\n",
        "    print(\"---------------------------------------------\")\n",
        "    print(\"best summary: \" + predicted)\n",
        "    print('/n')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------\n",
            "Original Summary:\n",
            "this article focuses on symbolic preference . it presents the symbolic preference system of a hybrid parser that combines a shallow parser with an overlay parser that builds on the chunks . novel in using a simple , threevalued scoring method for assigning preferences to constituents viewed in the context of their containing constituents . problems associated with earlier preference systems , and has considerably facilitated development . it uses simple scoring . \n",
            "---------------------------------------------\n",
            "best summary: well article on on english speech speech and the an an building important that a words is a text processing , including using turkish .\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            "this article focuses on recognizing textual entailment . it presents a dependencybased paraphrasing algorithm , using the dirt data set , and its application in the context of a straightforward rte system based on aligning dependency trees . \n",
            "---------------------------------------------\n",
            "best summary: an article on . evaluation mt and machine translation it description mt an that systems from on machine , that the integration of\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            "this article focuses on joint phrasal translation modeling . it presents a phrasal inversion transduction grammar as an alternative to joint phrasal translation models . similar to its flatstring phrasal predecessors , but admits polynomialtime algorithms for viterbi alignment and em training . , for the first time , the utility of a joint phrasal translation model as a word alignment method . \n",
            "---------------------------------------------\n",
            "best summary: 10 article article focuses density assessment and features and it assess use utility of whether perspectives is label mt propose the is for show experts : the namely not with for performance on the whether the . of it . pseudo .\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            "this article focuses on geolinguistic change . it presents with the supply of 8 closely interpreted dialectometrical maps , this paper analyses the linguistic change of the geolinguistic deep structures in northern france between 1300 and 1900 . . \n",
            "---------------------------------------------\n",
            "best summary: based constructing a korean discriminative word . it presents an system webbased implemented for solving the an : demonstrated free system lowresource uses multiple high language commercially tool , lightweight valid base to english sentences forms .\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            "this article focuses on recognising textual . it presents latent semantic analysis has only recently been applied to textual entailment recognition . . \n",
            "---------------------------------------------\n",
            "best summary: on article translating and cross - and statistical to our use build machine novel between between uses machine translation such and highly machine . systems a\n",
            "/n\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}