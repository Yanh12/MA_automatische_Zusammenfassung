{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "todo: bilstm_ngram_ori_sum",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew5DrGdIo5Es",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4d5b1c43-4ee5-4d62-ffa4-254be6679caa"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "print(tf.__version__)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import re\n",
        "import shutil\n",
        "import zipfile\n",
        "import itertools\n",
        "#import dependencies\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional,LSTMCell\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMELLgYK2LDK",
        "colab_type": "text"
      },
      "source": [
        "This model is based on TF tuturial (https://www.tensorflow.org/tutorials/text/nmt_with_attention) for NMT using TF 2.0 with modifications.\n",
        "Specifically to say:\n",
        "1. modifying how texts are preprocessed; which to be tokenized which not;\n",
        "2. Using pretrained word2vec instead of the built-in keras embedding;\n",
        "3. Using BiLstm instead of LSTM for Encoder\n",
        "4. Other small modifications when needed (eg. some explanations)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY9klERzpBO6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e776324f-79df-4164-dee5-0d07e9989f5c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIbgLQFypJZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/MA_colab')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSJaLWq7pM3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import text files\n",
        "default_path = '/content/drive/My Drive/MA_colab/bilstm_attn_beam/ngram_ori_sum/'\n",
        "with open ('/content/drive/My Drive/MA_colab/abs_strV2.p', 'rb') as filehandle:\n",
        "  abs_selected = pickle.load(filehandle)\n",
        "abs_selected = [x.lower() for x in abs_selected]\n",
        "with open('/content/drive/My Drive/MA_colab/cores_ngram.p', 'rb') as filehandle:\n",
        "  texts = pickle.load(filehandle)\n",
        "texts = [' '.join(x) for x in texts]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oci98xug10UZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c1c5cc53-9a0b-4648-f881-3d271ac411f2"
      },
      "source": [
        "for i in range(5):\n",
        "    print(texts[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "to reduce the time and effort required, we present an extension of the annotationbyquery approach that introduces a ranking of the query results by means of machine learning; we order the results by confidence of the used classifier. to enable a selective manual annotation process, a linguistic search engine is used, allowing the creation of queries which single out potential instances of the phenomenon in question. this approach relieves the annotator from having to read through the corpus from the beginning to the end to look for instances of a phenomenon. this still leaves the annotator with the tedious task of clicking through the search results to mark the true instances. those potential instances are then displayed to the user, who annotates each one as being an instance of the phenomenon or not. to obtain a model for the classifier, we employ an intool learning approach, where we learn from the annotations that are made by users in the tool itself. for future work, we plan to speed up the learning process , and also add the ability for users to configure the features used to train the classifier, this process of searching and annotating can be performed by multiple users concurrently; the annotations are stored for each user separately. to pinpoint occurrences of such phenomena and to annotate them requires a new kind of annotation tool, since manual, sequential annotation is not feasible anymore for large amounts of texts. however, the search may yield many results that may superficially appear to be an instance of the desired phenomenon, but due to ambiguities or due to a broadly defined query only a small subset may be actual instances. with automatic ranking we introduced an extension to the annotationbyquery workflow which facilitates manual, selective annotation of large corpora.\n",
            "to provide a direct comparison to the dependency induction literature, we will also provide an unlabeled evaluation on the 10 dependency corpora that were used for the task of grammar induction from raw text in the pascal challenge on grammar induction .  despite significant progress on inducing partofspeech tags from raw text and a small number of notable exceptions , most approaches to grammar induction or unsupervised parsing are based on the assumption that gold pos tags are available to the induction system. in this paper we demonstrate that the simple “universal” knowledge of bisk and hockenmaier can be easily applied to induced clusters given a small number of words labeled as noun, verb or other, and that this small amount of knowledge is sufficient to produce labeled syntactic structures from raw text, something that has not yet been proposed in the literature. there remains a noticeable performance gap due to the use of induced clusters in lieu of gold tags. there remains a noticeable performance gap due to the use of induced clusters in lieu of gold tags. boonkwan and steedman train a parser that uses a semiautomatically constructed combinatory categorial grammar ) lexicon for pos tags, while bisk and hockenmaier show that ccg lexicons can be induced automatically if pos tags are used to identify nouns and verbs. in this paper, we have produced the first labeled syntactic structures from raw text. in this paper, we have produced the first labeled syntactic structures from raw text. specifically, we will provide a labeled evaluation of induced ccg parsers against the english and chinese ccgbanks.\n",
            "in this work, we presented wikikreator that can generate content automatically to improve wikipedia stubs. in this work, we presented wikikreator that can generate content automatically to improve wikipedia stubs. to address the abovementioned issues, we present wikikreator – a system that can automatically generate content for wikipedia stubs. ideally, we would like to create an automatic wikipedia content generator, which can generate a comprehensive overview on any topic using available information from the web and append the generated content to the stubs. further, we address the issue of abstractive text summarization for wikipedia content generation. to the best of our knowledge, this work is the first to address the issue of generating content automatically to transform wikipedia stubs into comprehensive articles. previous work only included the most informative excerpt in the article ; in contrast, our abstractive summarization approach minimizes loss of information that should ideally be in an wikipedia article by fusing content from several sentences. we use rouge to compare content generated by wikikreator and the corresponding wikipedia articles. our experiments reveal that wikikreator is capable of generating wellformed informative content. our experiments reveal that wikikreator is capable of generating wellformed informative content. in this work, we address the following research question: given the introductory content, the title of the stub and information on the categories how can we transform the stub into a comprehensive wikipedia article? addition of automatically generated content can provide a useful starting point for contributors on wikipedia, which can be improved upon later. the ilp based sentence generation strategy ensures that we generate novel content by synthesizing information from multiple sources and thereby improve content selection. the ilp based sentence generation strategy ensures that we generate novel content by synthesizing information from multiple sources and thereby improve content selection.\n",
            "we find that lowrank tensors for verbs achieve comparable or better performance than fullrank tensors on both verb disambiguation and sentence similarity tasks, while reducing the number of parameters that must be learned and stored for each verb by at least two orders of magnitude, and cutting training time in half. we aim to reduce the size of the models by demonstrating that reducedrank tensors, which can be represented in a form requiring fewer parameters, can capture the semantics of complex types as well as the fullrank tensors do. all of the more complex types have corresponding tensors of higher order, and therefore a barrier to the practical implementation of this framework is the large number of parameters required to represent an extended vocabulary and a variety of grammatical constructions. regardless, we show that the lowrank tensors are able to achieve performance comparable to the full rank for both types of vectors. previous work on the transitive verb construction within the categorial framework includes a twostep linearregression method for the construction of the full verb tensors and a multilinear regression method combined with a twodimensional plausibility space . we learn the component vectors and apply the composition without ever constructing the full tensors and thus we are able to improve on both memory usage and efficiency. the best performing method uses two matrices, one representing the subjectverb interactions and the other the verbobject interactions. in this paper, we use tensor rank decomposition to represent each verb’s tensor as a sum of tensor products of vectors. a transitive verb is a thirdorder tensor that takes the noun vectors representing the subject and object and returns a vector in the sentence space .\n",
            "in experiments, preordering using the topdown parsing algorithm was faster and gave higher bleu scores than btgbased preordering using the cyk algorithm. the topdown method had better bleu scores for 7 language pairs without relying on supervised syntactic parsers compared to other preordering methods. in the experiments, it was shown that the topdown parsing method is more than 10 times faster than a cykbased method. model parameters can be learned using latent variable perceptron with the early update technique , since the parsing method provides an easy way for checking the reachability of each parser state to valid final states. compared to existing preordering methods, our method had better or comparable bleu scores without using supervised parsers. in this paper, we propose an efficient incremental topdown btg parsing method which can be applied to preordering. in this paper, we proposed a topdown btg parsing method for preordering. the method can be applied to any language using only parallel text. future work includes developing a bottomup btg parser with latent variables, and comparing the results to the topdown parser. various methods for preordering have been studied, and a method based on bracketing transduction grammar was proposed by neubig et al. . the method provides an easy way to check the validity of each parser state, which allows us to use early update for latent variable perceptron with beam search. we also try to use forceddecoding instead of word alignment based on expectation maximization algorithms in order to create better training data for preordering. preordering is another approach for tackling the problem, which modifies the word order of an input sentence in a source language to have the word order in a target language ). however, the method has the problem of computational efficiency.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_7ukHhKpXKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tune the text for the further tokenization\n",
        "to_exclude = '!\"#$%&()*+/:;<=>@[\\\\]^_`{|}~\\t\\n'\n",
        "to_tokenize = '.,:;!?-'\n",
        "texts_modified = []\n",
        "for article in texts:\n",
        "  article = re.sub(r'(['+to_tokenize+'])', r' \\1 ', article)\n",
        "  article = re.sub(r'  +', ' ', article)\n",
        "  texts_modified.append(article)\n",
        "abs_modified = []\n",
        "for summary in abs_selected:\n",
        "  summary = re.sub(r'(['+to_tokenize+'])', r' \\1 ', summary)\n",
        "  summary = re.sub(r'  +', ' ', summary)\n",
        "  abs_modified.append(summary)\n",
        "\n",
        "#add START and END tag for each selected abstract\n",
        "abs_tagged = ['<start> ' + x + ' <end>' for x in abs_modified]\n",
        "texts_tagged = ['<start> ' + x + ' <end>' for x in texts_modified]\n",
        "\n",
        "max_len_text = 300\n",
        "max_len_summary = 50\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evjYCxVYHltl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train,validation & test parts\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(texts_tagged[:15000],abs_tagged[:15000], test_size=0.1,random_state=0,shuffle=False)\n",
        "x_test = texts_tagged[15000:]\n",
        "y_test = abs_modified[15000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDlspMbBqmlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#text Tokenizer by keras >> convert text sequences into integer representation\n",
        "x_tokenizer = Tokenizer(filters='')\n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "x_tr = x_tokenizer.texts_to_sequences(x_tr)\n",
        "x_val = x_tokenizer.texts_to_sequences(x_val)\n",
        "#padding zero to max length\n",
        "x_tr = pad_sequences(x_tr,  maxlen=max_len_text, padding='post')\n",
        "x_val = pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
        "#defing the vocabulary size\n",
        "x_voc_size = len(x_tokenizer.word_index) +1\n",
        "\n",
        "#summary Tokenizer\n",
        "y_tokenizer = Tokenizer(filters='')\n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "y_tr = y_tokenizer.texts_to_sequences(y_tr)\n",
        "y_val = y_tokenizer.texts_to_sequences(y_val)\n",
        "y_tr = pad_sequences(y_tr, maxlen=max_len_summary, padding='post')\n",
        "y_val = pad_sequences(y_val, maxlen=max_len_summary, padding='post')\n",
        "y_voc_size = len(y_tokenizer.word_index) +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWkRa-0eNVdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word2index\n",
        "x_word_index = x_tokenizer.word_index\n",
        "y_word_index = y_tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UXLzZnslqpI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6af01f4d-650e-4e88-e361-7b07f3cfa9bc"
      },
      "source": [
        "print(x_tokenizer.word_index['<start>']) \n",
        "x_voc_size = len(x_tokenizer.word_index)+1  \n",
        "y_voc_size = len(y_tokenizer.word_index)+ 1\n",
        "print(\"input_vocab_size : \", x_voc_size)\n",
        "print(\"output_vocab_size : \" ,y_voc_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n",
            "input_vocab_size :  55213\n",
            "output_vocab_size :  32987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTJ1-DHrYfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model parameters\n",
        "BATCH_SIZE = 32 #due to colab capacity, no bigger batch size possible\n",
        "BUFFER_SIZE = len(x_tr)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dims = 300\n",
        "rnn_units = 150\n",
        "rnn_units_decoder = 300\n",
        "dense_units = 300\n",
        "Dtype = tf.float32\n",
        "Tx = 300 #longst input\n",
        "Ty = 50 #longst output "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUyhGJrJrh7b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ade904e8-dfb6-42d6-e7d4-92b8e1d33808"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_tr, y_tr)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 300)\n",
            "(32, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUpgz4wtrtDd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a67dc1ae-4e3a-441b-d7cb-f2c2c7b94fdf"
      },
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 300)\n",
            "(32, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgndaBd41PG8",
        "colab_type": "text"
      },
      "source": [
        "Using Pre-trained embedding models instead of keras Embedding \n",
        "Note that there are lots of possible ways to embed words to dense vectors of fixed langth. Keras Embedding modul provides this embedding as other layers-- i.e. minimize loss as its ultimate goal.\n",
        "However, word2vec tech is the one that trys to capture the SEMANTIC MEANINGS of words, not only simply densing them as minimizing the loss.\n",
        "Therefore, for text understanding related tasks, better using pre-trained word2vec models as embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0dm7Pgt9RnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#use pre-trained word embedding as Embedding.layer of the model\n",
        "#load pretrained word embedding\n",
        "\"\"\"\n",
        "corpus-specific word embedding can provide some more precise information, but \n",
        "it also has the disadvantage that many rare word get ignored.\n",
        "So, here where use a combined way to better represent the words:\n",
        "   1. if the word is in the corpus-specific word-embedding file, use it.\n",
        "   2. if the work is not in this file but in the pretrained embeddings from the\n",
        "   commonly crawled documents, then use this.\n",
        "   3. if a word neither in the specific file nor in the common file, initialize\n",
        "   it with random numbers\n",
        "\"\"\"\n",
        "cl_embeddings_index = {}\n",
        "with open('/content/drive/My Drive/MA_colab/CL_word2vec_300dim.txt') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        cl_embeddings_index[word] = coefs\n",
        "\n",
        "\n",
        "embeddings_index = {}\n",
        "with open('/content/drive/My Drive/MA_colab/glove.6B.300d.txt') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "\n",
        "#embedding_layer for Encoder\n",
        "embedding_matrix_encoder = np.zeros((x_voc_size,embedding_dims))\n",
        "\"\"\"\n",
        "if the word is in the word embedding dict, embedding_vector gets the corresponding value\n",
        "if not, initilize the vector with random numbers\n",
        "\"\"\"\n",
        "for word, i in x_word_index.items():\n",
        "  if word in cl_embeddings_index.keys():\n",
        "     embedding_vector = cl_embeddings_index.get(word)\n",
        "     embedding_matrix_encoder[i] = embedding_vector\n",
        "  elif word in embeddings_index.keys():\n",
        "     embedding_vector = embeddings_index.get(word)\n",
        "     embedding_matrix_encoder[i] = embedding_vector\n",
        "  else:\n",
        "     embedding_vector=np.random.uniform(-1.0,1.0,(300,))\n",
        "     embedding_matrix_encoder[i] = embedding_vector\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "embedding_layer = Embedding ()\n",
        "initilize the embedding layer:\n",
        "params: input_dim = the vocabulary size\n",
        "        output_dim = embedding dimension\n",
        "        embeddings_initilizer: to initilize the embeddings\n",
        "        input_length = the max length of the source text\n",
        "        trainable = False, because we want the pre-trained model and not update it\n",
        "\"\"\"\n",
        "embedding_layer_encoder = tf.keras.layers.Embedding(input_dim = x_voc_size, \n",
        "                                    output_dim = embedding_dims,\n",
        "                                    weights=[embedding_matrix_encoder], \n",
        "                                    input_length = max_len_text, \n",
        "                                    trainable = False)\n",
        "\n",
        "#embedding_layer for Decoder\n",
        "embedding_matrix_decoder = np.zeros((y_voc_size,embedding_dims))\n",
        "for word, i in y_word_index.items():\n",
        "   if word in cl_embeddings_index.keys():\n",
        "     embedding_vector_y = cl_embeddings_index.get(word)\n",
        "     embedding_matrix_decoder[i] = embedding_vector_y\n",
        "   elif word in embeddings_index.keys():\n",
        "     embedding_vector_y = embeddings_index.get(word)\n",
        "     embedding_matrix_decoder[i] = embedding_vector_y\n",
        "   else:\n",
        "     embedding_vector_y = np.random.uniform(-1.0,1.0,(300,))\n",
        "     embedding_matrix_decoder[i] = embedding_vector_y\n",
        "\n",
        "embedding_layer_decoder = tf.keras.layers.Embedding(input_dim = y_voc_size, \n",
        "                                    output_dim = embedding_dims,\n",
        "                                    weights=[embedding_matrix_decoder],\n",
        "                                    input_length = max_len_summary, \n",
        "                                    trainable = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE5XTFGtr3CH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the model\n",
        "#ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,x_voc_size,embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = embedding_layer_encoder\n",
        "\n",
        "        #Bi-LSTM\n",
        "        #not like LSTM, Bi-LSTM has forward and backword hidden state and content state\n",
        "        #encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
        "        self.encoder_birnnlayer = Bidirectional(LSTM(rnn_units,return_state=True,return_sequences=True))\n",
        "        \n",
        "      \n",
        "#DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,y_voc_size, embedding_dims, rnn_units_decoder):\n",
        "        super().__init__()\n",
        "        self.decoder_embedding = embedding_layer_decoder\n",
        "        self.dense_layer = Dense(y_voc_size)\n",
        "        self.decoder_rnncell = LSTMCell(rnn_units_decoder)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        # Create attention mechanism with memory = None\n",
        "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell = self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
        "                                                output_layer=self.dense_layer)\n",
        "\n",
        "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "        #return tfa.seq2seq.LuongAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "    # wrap decode-rnn cell  \n",
        "    def build_rnn_cell(self, batch_size ):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
        "                                                attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
        "                                                                dtype = Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "        return decoder_initial_state\n",
        "\n",
        "\n",
        "\n",
        "encoderNetwork = EncoderNetwork(x_voc_size,embedding_dims, rnn_units)\n",
        "decoderNetwork = DecoderNetwork(y_voc_size,embedding_dims, rnn_units_decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiyPryBQsLvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optimizing\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlhB9MlZsSGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loss function\n",
        "def loss_function(y_pred, y):\n",
        "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                                                  reduction='none')\n",
        "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
        "    mask = tf.logical_not(tf.math.equal(y,0))  \n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss = mask* loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrlD2omyscHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73ea136a-c15a-4a55-ec0c-6a0273736016"
      },
      "source": [
        "decoderNetwork.attention_mechanism.memory_initialized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0DhCkIhsfIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#batch training & teacher-forcing\n",
        "def train_step(input_batch, output_batch, encoder_initial_cell_state):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "\n",
        "\n",
        "        #Outputs of BiLstm\n",
        "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
        "        encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoderNetwork.encoder_birnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state =encoder_initial_cell_state)\n",
        "        state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
        "        state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
        "       \n",
        "\n",
        "        \n",
        "        # Prepare correct Decoder input & output sequence data\n",
        "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "        #compare logits with timestepped +1 version of decoder_input\n",
        "        decoder_output = output_batch[:,1:] #ignore <start>\n",
        "\n",
        "\n",
        "        # Decoder Embeddings\n",
        "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
        "        decoderNetwork.attention_mechanism.setup_memory(encoder_outputs)\n",
        "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                           encoder_state=[state_h, state_c],\n",
        "                                                                           Dtype=tf.float32)\n",
        "        \n",
        "        #BasicDecoderOutput        \n",
        "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
        "                                               sequence_length=BATCH_SIZE*[Ty-1])\n",
        "\n",
        "        logits = outputs.rnn_output\n",
        "        #Calculate loss\n",
        "        loss = loss_function(logits, decoder_output)\n",
        "\n",
        "    #Returns the list of all layer variables / weights.\n",
        "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
        "    # differentiate loss wrt variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    #grads_and_vars – List of(gradient, variable) pairs.\n",
        "    grads_and_vars = zip(gradients,variables)\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gg8GpwbsveE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70a57fa2-f473-4614-fc8a-e9f99cbee46b"
      },
      "source": [
        "#training\n",
        "chkpoint_prefix = os.path.join(default_path, \"chkpoint\")\n",
        "if not os.path.exists(default_path):\n",
        "    os.mkdir(default_path)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer = optimizer, encoderNetwork = encoderNetwork, \n",
        "                                 decoderNetwork = decoderNetwork)\n",
        "\n",
        "try:\n",
        "    status = checkpoint.restore(tf.train.latest_checkpoint(default_path))\n",
        "    print(\"Checkpoint found at {}\".format(tf.train.latest_checkpoint(default_path)))\n",
        "except:\n",
        "    print(\"No checkpoint found at {}\".format(default_path))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint found at None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEKsbQ0HtXyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RNN LSTM hidden and memory state initializer\n",
        "def initialize_initial_state():\n",
        "    #for Bi-LSTM\n",
        "    return [tf.zeros((BATCH_SIZE, rnn_units)) for i in range(4)]\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uetAmVBStYcp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "84c75fb1-4928-4c3b-845f-c9820e7627e3"
      },
      "source": [
        "epochs = 10\n",
        "loss=[]\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "    #print(encoder_initial_cell_state)\n",
        "\n",
        "    \n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%100 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))\n",
        "            checkpoint.save(file_prefix = chkpoint_prefix)\n",
        "            loss.append(batch_loss.numpy())\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 6.512489318847656 epoch 1 batch 100 \n",
            "total loss: 5.871463298797607 epoch 1 batch 200 \n",
            "total loss: 5.497341156005859 epoch 1 batch 300 \n",
            "total loss: 5.354740619659424 epoch 1 batch 400 \n",
            "total loss: 5.078332901000977 epoch 2 batch 100 \n",
            "total loss: 5.158822059631348 epoch 2 batch 200 \n",
            "total loss: 4.890985012054443 epoch 2 batch 300 \n",
            "total loss: 4.782263278961182 epoch 2 batch 400 \n",
            "total loss: 4.291205883026123 epoch 3 batch 100 \n",
            "total loss: 4.528384208679199 epoch 3 batch 200 \n",
            "total loss: 4.6213059425354 epoch 3 batch 300 \n",
            "total loss: 4.513287544250488 epoch 3 batch 400 \n",
            "total loss: 4.343062877655029 epoch 4 batch 100 \n",
            "total loss: 4.258578777313232 epoch 4 batch 200 \n",
            "total loss: 4.290786266326904 epoch 4 batch 300 \n",
            "total loss: 4.2476396560668945 epoch 4 batch 400 \n",
            "total loss: 3.851248264312744 epoch 5 batch 100 \n",
            "total loss: 4.006019592285156 epoch 5 batch 200 \n",
            "total loss: 3.9940178394317627 epoch 5 batch 300 \n",
            "total loss: 3.997032403945923 epoch 5 batch 400 \n",
            "total loss: 3.918718099594116 epoch 6 batch 100 \n",
            "total loss: 3.8867175579071045 epoch 6 batch 200 \n",
            "total loss: 3.7773773670196533 epoch 6 batch 300 \n",
            "total loss: 3.930824041366577 epoch 6 batch 400 \n",
            "total loss: 3.5892229080200195 epoch 7 batch 100 \n",
            "total loss: 3.5558981895446777 epoch 7 batch 200 \n",
            "total loss: 3.754002809524536 epoch 7 batch 300 \n",
            "total loss: 3.7182047367095947 epoch 7 batch 400 \n",
            "total loss: 3.1914143562316895 epoch 8 batch 100 \n",
            "total loss: 3.3629791736602783 epoch 8 batch 200 \n",
            "total loss: 3.520423412322998 epoch 8 batch 300 \n",
            "total loss: 3.369615316390991 epoch 8 batch 400 \n",
            "total loss: 3.1363131999969482 epoch 9 batch 100 \n",
            "total loss: 3.233144760131836 epoch 9 batch 200 \n",
            "total loss: 3.2483277320861816 epoch 9 batch 300 \n",
            "total loss: 3.1291353702545166 epoch 9 batch 400 \n",
            "total loss: 2.8100786209106445 epoch 10 batch 100 \n",
            "total loss: 3.02341890335083 epoch 10 batch 200 \n",
            "total loss: 3.1002378463745117 epoch 10 batch 300 \n",
            "total loss: 3.3084716796875 epoch 10 batch 400 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9o9YVhGwgHx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "82e3b56e-30fd-4129-ed85-1db3e81b0cb6"
      },
      "source": [
        "#validation\n",
        "epochs = 10\n",
        "loss_val=[]\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "    #print(encoder_initial_cell_state)\n",
        "\n",
        "    \n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset_test.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%10 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))\n",
        "            checkpoint.save(file_prefix = chkpoint_prefix)\n",
        "            loss_val.append(batch_loss.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 4.26741886138916 epoch 1 batch 10 \n",
            "total loss: 4.256870746612549 epoch 1 batch 20 \n",
            "total loss: 4.3020124435424805 epoch 1 batch 30 \n",
            "total loss: 4.3555521965026855 epoch 1 batch 40 \n",
            "total loss: 3.5125231742858887 epoch 2 batch 10 \n",
            "total loss: 3.9475209712982178 epoch 2 batch 20 \n",
            "total loss: 3.713602066040039 epoch 2 batch 30 \n",
            "total loss: 3.6789162158966064 epoch 2 batch 40 \n",
            "total loss: 3.2059295177459717 epoch 3 batch 10 \n",
            "total loss: 3.0025386810302734 epoch 3 batch 20 \n",
            "total loss: 3.41898775100708 epoch 3 batch 30 \n",
            "total loss: 3.1254329681396484 epoch 3 batch 40 \n",
            "total loss: 2.697690010070801 epoch 4 batch 10 \n",
            "total loss: 2.7864065170288086 epoch 4 batch 20 \n",
            "total loss: 2.8105051517486572 epoch 4 batch 30 \n",
            "total loss: 2.9930570125579834 epoch 4 batch 40 \n",
            "total loss: 2.344799757003784 epoch 5 batch 10 \n",
            "total loss: 2.455707311630249 epoch 5 batch 20 \n",
            "total loss: 2.41428542137146 epoch 5 batch 30 \n",
            "total loss: 2.6164791584014893 epoch 5 batch 40 \n",
            "total loss: 2.2640960216522217 epoch 6 batch 10 \n",
            "total loss: 1.9752438068389893 epoch 6 batch 20 \n",
            "total loss: 2.157477617263794 epoch 6 batch 30 \n",
            "total loss: 2.272996187210083 epoch 6 batch 40 \n",
            "total loss: 1.9042987823486328 epoch 7 batch 10 \n",
            "total loss: 1.9127599000930786 epoch 7 batch 20 \n",
            "total loss: 1.9482543468475342 epoch 7 batch 30 \n",
            "total loss: 1.7766119241714478 epoch 7 batch 40 \n",
            "total loss: 1.6781599521636963 epoch 8 batch 10 \n",
            "total loss: 1.6273009777069092 epoch 8 batch 20 \n",
            "total loss: 1.819374918937683 epoch 8 batch 30 \n",
            "total loss: 1.8954441547393799 epoch 8 batch 40 \n",
            "total loss: 1.5422391891479492 epoch 9 batch 10 \n",
            "total loss: 1.4554206132888794 epoch 9 batch 20 \n",
            "total loss: 1.437380075454712 epoch 9 batch 30 \n",
            "total loss: 1.6301432847976685 epoch 9 batch 40 \n",
            "total loss: 1.3343746662139893 epoch 10 batch 10 \n",
            "total loss: 1.2401111125946045 epoch 10 batch 20 \n",
            "total loss: 1.3924931287765503 epoch 10 batch 30 \n",
            "total loss: 1.4000608921051025 epoch 10 batch 40 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW-IAuAetcnD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4477119b-8271-47f3-fd51-7638166940ca"
      },
      "source": [
        "#inference\n",
        "#if trained in same session else use checkpoint variable\n",
        "#decoder_embedding_matrix = tf.train.load_variable(checkpointdir, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
        "print(decoderNetwork.decoder_embedding.variables[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32987, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAdxjl3ntqqd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ef43bb40-3407-450a-f121-008101d7e446"
      },
      "source": [
        "[print(var) for var in tf.train.list_variables(\n",
        "    default_path) if re.match(r'.*decoder_embedding.*',var[0])]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE', [32987, 300])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8n1UlEFtuJx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ffac5b6-6ee7-4b78-c277-15b42294ff03"
      },
      "source": [
        "decoder_embedding_matrix = tf.train.load_variable(\n",
        "    default_path, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "print(decoder_embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32987, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpby4he_vSTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test=texts_tagged[15000:]\n",
        "x_test = x_tokenizer.texts_to_sequences(x_test)\n",
        "input_x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_len_text, padding='post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik6iBqYpMU4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "end_token = y_tokenizer.word_index['<end>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84TAvT78yu-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test (test_input):\n",
        "\n",
        "    beam_width = 8\n",
        "\n",
        "    inference_batch_size = test_input.shape[0]\n",
        "    inp = tf.convert_to_tensor(test_input)            \n",
        "    encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "\n",
        "    def initialize_initial_state_inference():\n",
        "        #for Bi-LSTM\n",
        "        return [tf.zeros((inference_batch_size, rnn_units)) for i in range(4)]\n",
        "\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoderNetwork.encoder_birnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state =initialize_initial_state_inference())\n",
        "    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
        "    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
        "\n",
        "    start_tokens = tf.fill([inference_batch_size],y_tokenizer.word_index['<start>'])\n",
        "    end_token = y_tokenizer.word_index['<end>']\n",
        "\n",
        "    decoder_input = tf.expand_dims([y_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
        "    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "    encoder_memory = tfa.seq2seq.tile_batch(encoder_outputs, beam_width)\n",
        "    decoderNetwork.attention_mechanism.setup_memory(encoder_memory)\n",
        "    print(\"beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] :\", encoder_memory.shape)\n",
        "    #set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
        "    decoder_initial_state = decoderNetwork.rnn_cell.get_initial_state(batch_size = inference_batch_size* beam_width,dtype = Dtype)\n",
        "    encoder_state = tfa.seq2seq.tile_batch([state_h, state_c], multiplier=beam_width)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "\n",
        "    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoderNetwork.rnn_cell,beam_width=beam_width,\n",
        "                                                 output_layer=decoderNetwork.dense_layer)\n",
        "\n",
        "\n",
        "    # Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "    # One heuristic is to decode up to two times the source sentence lengths.\n",
        "    maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "    #initialize inference decoder\n",
        "\n",
        "    (first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                                start_tokens = start_tokens,\n",
        "                                end_token=end_token,\n",
        "                                initial_state = decoder_initial_state)\n",
        "    #print( first_finished.shape)\n",
        "    #print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
        "\n",
        "    inputs = first_inputs\n",
        "    state = first_state  \n",
        "    predictions = np.empty((inference_batch_size, beam_width,0), dtype = np.int32)\n",
        "    beam_scores =  np.empty((inference_batch_size, beam_width,0), dtype = np.float32)                                                                            \n",
        "    for j in range(maximum_iterations):\n",
        "        beam_search_outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "        inputs = next_inputs\n",
        "        state = next_state\n",
        "        outputs = np.expand_dims(beam_search_outputs.predicted_ids,axis = -1)\n",
        "        scores = np.expand_dims(beam_search_outputs.scores,axis = -1)\n",
        "        predictions = np.append(predictions, outputs, axis = -1)\n",
        "        beam_scores = np.append(beam_scores, scores, axis = -1)\n",
        "    #print(predictions.shape) \n",
        "    #print(beam_scores.shape)\n",
        "    \n",
        "    return predictions, beam_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc8hjVUy39pA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "a3b2e5af-7635-41c4-f18f-9a0fca556278"
      },
      "source": [
        "#due to the limitattion of gpu memory, test is done in batchs\n",
        "test_batch = 32\n",
        "iters = len(x_test) // test_batch \n",
        "last_iter = iters+1\n",
        "all_predictions = []\n",
        "all_beam_scores = []\n",
        "for i in range(iters+1):\n",
        "    test_input = input_x_test[test_batch*i : test_batch*(i+1)]\n",
        "    predictions, beam_scores = test(test_input)\n",
        "    all_predictions.append(predictions)\n",
        "    all_beam_scores.append(beam_scores)\n",
        "    if i == last_iter:\n",
        "        test_input = input_x_test[test_batch*last_iter:]\n",
        "        predictions, beam_scores = test(test_input)\n",
        "        all_predictions.append(predictions)\n",
        "        all_beam_scores.append(beam_scores)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 300, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (136, 300, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdVEYjkf6yRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_sum = []\n",
        "for i in range (len(all_predictions)):\n",
        "    #select the predicted summary with highest beam score as the final result\n",
        "    predictions = all_predictions[i]\n",
        "    beam_scores = all_beam_scores[i]\n",
        "    for j in range(len(predictions)):\n",
        "        output_beams_per_sample = predictions[j,:,:]\n",
        "        score_beams_per_sample = beam_scores[j,:,:]\n",
        "        for beam, score in zip(output_beams_per_sample,score_beams_per_sample) :\n",
        "            seq = list(itertools.takewhile( lambda index: index !=end_token, beam))\n",
        "            score_indexes = np.arange(len(seq))\n",
        "            beam_score = score[score_indexes].sum()\n",
        "            predicted = sorted ({(\" \".join( [y_tokenizer.index_word[w] for w in seq])):beam_score}.keys(),reverse=True)[0]\n",
        "        predicted_sum.append(predicted)\n",
        "\n",
        "#store the results\n",
        "for i in range(len(predicted_sum)):\n",
        "    ref = open(default_path + \"ref/ref_{}.txt\".format(i), 'w')\n",
        "    ref.write(y_test[i])\n",
        "    ref.close()\n",
        "    summ = open(default_path + \"predicted/sum_{}.txt\".format(i), 'w')\n",
        "    summ.write(predicted_sum[i])\n",
        "    summ.close()\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xjRCCGasRrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "f734089c-51a0-4d12-a5db-f3d79467baed"
      },
      "source": [
        "#to see beam scores of a summary\n",
        "for i in range(len(predicted_sum[:5])):\n",
        "    print(\"-----------------\")\n",
        "    print(\"Original Summary:\")\n",
        "    print(y_test[i])\n",
        "    #print(\"-----------------\")\n",
        "    #print(\"\\nPredicted Summary:\")\n",
        "    #print(\"---------------------------------------------\")\n",
        "    output_beams_per_sample = predictions[i,:,:]\n",
        "    score_beams_per_sample = beam_scores[i,:,:]\n",
        "    for j,(beam, score) in enumerate(zip(output_beams_per_sample,score_beams_per_sample)) :\n",
        "        seq = list(itertools.takewhile( lambda index: index !=end_token, beam))\n",
        "        score_indexes = np.arange(len(seq))\n",
        "        beam_score = score[score_indexes].sum()\n",
        "        #print (\"version{}: \".format(j), (\" \".join( [y_tokenizer.index_word[w] for w in seq]), \" beam score: \", beam_score))\n",
        "        #print (\" \".join( [y_tokenizer.index_word[w] for w in seq]), \" beam score: \", beam_score)\n",
        "        predicted = sorted ({(\" \".join( [y_tokenizer.index_word[w] for w in seq])):beam_score}.keys(),reverse=True)[0]\n",
        "    print(\"---------------------------------------------\")\n",
        "    print(\"best summary: \" + predicted)\n",
        "    print('/n')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------\n",
            "Original Summary:\n",
            " despite the popularity of stochastic parsers , symbolic parsing still has some advantages , but is not practical without an effective mechanism for selecting among alternative analyses . this paper describes the symbolic preference system of a hybrid parser that combines a shallow parser with an overlay parser that builds on the chunks . the hybrid currently equals or exceeds most stochastic parsers in speed and is approaching them in accuracy . the preference system is novel in using a simple , threevalued scoring method for assigning preferences to constituents viewed in the context of their containing constituents . the approach addresses problems associated with earlier preference systems , and has considerably facilitated development . it is ultimately based on viewing preference scoring as an engineering mechanism , and only indirectly related to cognitive principles or corpusbased frequencies . \n",
            "---------------------------------------------\n",
            "best summary: our is a word based research model a for a . techniques word word is mandarin trigram set trigram show . a paper are the basic the word word accuracy recognition the system in recognition system , a with with with word a with system words .\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            " this paper addresses syntaxbased paraphrasing methods for recognizing textual entailment . in particular , we describe a dependencybased paraphrasing algorithm , using the dirt data set , and its application in the context of a straightforward rte system based on aligning dependency trees . we find a small positive effect of dependencybased paraphrasing on both the rte3 development and test sets , but the added value of this type of paraphrasing deserves further analysis . \n",
            "---------------------------------------------\n",
            "best summary: as discuss an a number to the baseline and and searching to similarity best . features evaluation , we variations in the a also the of our proposed our . show of show the comparison by evaluation between features proposed overall performance proposed our evaluation metrics .\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            " we present a phrasal inversion transduction grammar as an alternative to joint phrasal translation models . this syntactic model is similar to its flatstring phrasal predecessors , but admits polynomialtime algorithms for viterbi alignment and em training . we demonstrate that the consistency constraints that allow flat phrasal models to scale also help itg algorithms , producing an 80times faster insideoutside algorithm . we also show that the phrasal translation tables produced by the itg are superior to those of the flat joint phrasal model , producing up to a 2 . 5 point improvement in bleu score . finally , we explore , for the first time , the utility of a joint phrasal translation model as a word alignment method . \n",
            "---------------------------------------------\n",
            "best summary: as report proposes the application empirical of the evaluation two different different measures the them effectiveness a our of experiments the our agreement measures and this results . factors results number this results experiments of we results shows a a method are they results comparative framework .\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            " with the supply of 8 closely interpreted dialectometrical maps , this paper analyses the linguistic change of the geolinguistic deep structures in northern france between 1300 and 1900 . as a matter of fact , the result will show – with one exception – the great stability of these deep structures . \n",
            "---------------------------------------------\n",
            "best summary: it method a the technique method the automatic creation the the treebank corpus the a data of to incrementally are show two the this algorithm two by the experiments show is performance potential method is improve used improves the system system improve by .\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            " latent semantic analysis has only recently been applied to textual entailment recognition . however , these efforts have suffered from inadequate bag of words vector representations . our prototype implementation for the third recognising textual entailment challenge improves the approach by applying it to vector representations that contain semistructured representations of words . it uses variable size ngrams of word stems to model independently verbs , subjects and objects displayed in textual statements . the system performance shows positive results and provides insights about how to improve them further . \n",
            "---------------------------------------------\n",
            "best summary: determining experiment focuses we participation study the implementation variations participation two to finitestate a that a of and our participation of used by a achieved in an system based system language on submission the of is our shared task is our submission of achieved evaluated . .\n",
            "/n\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}