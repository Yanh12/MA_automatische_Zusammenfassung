{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " bilstm_tr_ori_sum",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQcImR+meZ3xQ2JUDH46vo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yanh12/MA_automatische_Zusammenfassung/blob/master/abstraktive_Phase/Model2/bilstm_tr_ori_sum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew5DrGdIo5Es",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7aee6a39-0667-46b4-9d92-06663e22a4b1"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "print(tf.__version__)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import re\n",
        "import zipfile\n",
        "import itertools\n",
        "#import dependencies\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional,LSTMCell\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMELLgYK2LDK",
        "colab_type": "text"
      },
      "source": [
        "This model is based on TF tuturial (https://www.tensorflow.org/tutorials/text/nmt_with_attention) for NMT using TF 2.0 with modifications.\n",
        "Specifically to say:\n",
        "1. modifying how texts are preprocessed; which to be tokenized which not;\n",
        "2. Using pretrained word2vec instead of the built-in keras embedding;\n",
        "3. Using BiLstm instead of LSTM for Encoder\n",
        "4. Other small modifications when needed (eg. some explanations)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY9klERzpBO6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "22e66efa-42ce-4894-8ee9-a8dc59c30ea3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIbgLQFypJZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/MA_colab')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSJaLWq7pM3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import text files\n",
        "default_path = '/content/drive/My Drive/MA_colab/bilstm_attn_beam/tr_ori_sum/'\n",
        "with open ('/content/drive/My Drive/MA_colab/abs_strV2.p', 'rb') as filehandle:\n",
        "  abs_selected = pickle.load(filehandle)\n",
        "abs_selected = [x.lower() for x in abs_selected]\n",
        "with open('/content/drive/My Drive/MA_colab/textRank_texts.p', 'rb') as filehandle:\n",
        "  texts = pickle.load(filehandle)\n",
        "texts = [' '.join(x) for x in texts]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oci98xug10UZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5c490bba-a760-4c9b-bf6a-4e36c8437608"
      },
      "source": [
        "for i in range(5):\n",
        "    print(texts[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after annotating instances based on simple queries and mlsupported ranked queries, we considered the natural next step to be searching automatically for the phenomenon in question utilizing machine learning, using arbitrary sentences from the corpus as input for the classifier instead of only the results returned by a query. our approach is different from active learning as our goal is not to improve the training efficiency of the classifier but rather to allow the user to interactively find and label as many true instances of a phenomenon as possible in a large corpus. consider the following typical scenario incorporating the ranking: a user constructs a query based on various features, such as pos tags or lemmata, which are used to search for matching sentences, e.g. the result is a list of sentences presented in a keywordsincontext view, along with an annotation field . we interpret this as the classifier being successful in helping the annotators after a brief training phase identifying additional occurrences of particular variants of a phenomenon as covered by the queries, but not easily generalizing to variants substantially different from those covered by the queries. to reduce the time and effort required, we present an extension of the annotationbyquery approach that introduces a ranking of the query results by means of machine learning; we order the results by confidence of the used classifier. we demonstrate our novel approach on the task of locating noncanonical constructions and conduct an intrinsic evaluation of the accuracy of the system augmented with machine learning output on the data annotated by expert linguists. the classifier producing the ranking can be invoked by the user at any time; it can be configured in certain characteristics, e.g. the annotations of which users should be used as training data, or how many of the selected users have to agree on an annotation for it to be included. in addition, webanno supports automatic annotation similar to our approach, also employing machine learning to build models from the data annotated by users. repeatedly annotating those highestranking results and reranking allows for quickly annotating instances of the phenomenon, while also improving the classifier accuracy at the same time. we conducted a cursory, “realworld” test regarding the speed of the ranking system.7 training the svm on differently sized subsets of the 449 sentences returned by a test query, we measured the time from clicking the rank results button until the process was complete and the gui had updated to reorder the sentences . to tackle this problem, we use the annotations already created by users on the search results to train a classifier. in this mode, the svm is first trained from all previously labeled instances for a given phenomenon, without taking the queries into account that were used to find those instances.\n",
            "however, these scores show that having only three labeled tokens per class is sufficient to capture most of the necessary distinctions for the hdpccg. in all cases, we assume that we can identify punctuation marks, which are moved to their own cluster and ignored for the purposes of tagging and parsing evaluation. we will evaluate three clustering approaches: brown clusters brown clusters assign each word to a single cluster using an aglomerative clustering that maximizes the probability of the corpus under a bigram class conditional model. this is likely due to morphology distinguishing otherwise ambiguous lemmas. that pos tagging metrics are not correlated with parsing performance. was the only participant competing in the pascal challenge that operated over raw text . , we train and test our systems on the english and chinese ccgbanks, and report directed labeled f1 and undirected unlabeled f1 over ccg dependencies . might outperform the systems we use here. note that the systems in st do not have access to any goldstandard pos tags, whereas our system has access to the gold tags for best shared task baseline. setting this value will prove important when moving between datasets of drastically different sizes. but we will see that three per cluster is sufficient to label most tokens correctly. we do the same here, use the dev set for choosing a hdpccg hyperparameter, and then present final results for comparison on the test section. any word can act as a modifier, i.e. have a category of the form x|x our results align with this research, leading us to believe that this paradigm of guided minimal supervision is a fruitful direction for future work. if it is adjacent to a word with category x or x|y, and any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the national science foundation, the national institutes of health, darpa or the u.s. government. we induce 36 tags for english and 37 for chinese to match the number of tags present in the treebanks . all of this confirms the observations of headden et al. to incorporate multiple types of features either at a token level or at a type level ). how to most effectively use seed knowledge and annotation is still an open question. more importantly, we see that, at least for english, despite clear differences in tagging performance, the parsing results are much more similar. of more immediate relevance to our task is the ability to accurately identify nouns and verbs: noun, verb, and other recall we measure the recall of our threeway labeling scheme of clusters as noun/verb/other against the universal pos tags of each token. et results table 1 presents the parsing and tagging development results on the two ccg corpora. however, their approach did not outperform the six baseline systems provided.\n",
            "as shown in figure 1, we construct a wordgraph using all the sentences assigned to a specific class by the classifier. the solution to the optimization problem decides the paths that are selected in the final section summary. as the classes might be unbalanced, we apply resampling on the training set. each article wj has several sections denoted as sjicji where sji and cji refer to the section title and content of the ith section in the jth article, respectively. categories on wikipedia group together pages on similar subjects. in our experiments, we set smax to 5 to generate short concise summaries in each section. table 1 shows the most frequent sections in each category. setting d to 0.80 in our experiments provided the best content selection results. naturally, we leverage knowledge existing in the categories to build our text classifier. we also compare wikikreator’s output with an existing wikipedia generation system of sauper and barzilay 11 that employs a perceptron learning framework to learn topic specific extractors. the rf classifier classifies the excerpts into one of the most frequent classes . web content from the search results obtained in the previous step requires cleaning to retain only the relevant information. instead, we use a classifier trained using topic distribution features to identify relevant content for the stub. the number of instances denotes the total number of the most frequent sections in each category. svmwv performs better in the category on diseases than the other two categories and the performance is comparable to . further, we avoid redundant sentences in the summary that carry similar information. the set of topics are t1, t2, t3,..., tm while pji refers to the probability of topic m of content cji. the results of our evaluation confirm the benefits of using abstractive summarization for content generation over approaches that do not use summarization. we populate the sections on wikipedia using the final summaries generated for each section along with the section title. however, these methods suffer from severe deficiencies. given a category, the api is capable of extracting all the articles under it. we retrieve relevant excerpts from the web by formulating queries. figure 1 shows a simple example of the wordgraph generation technique. global informativeness: in this work, we presented wikikreator that can generate content automatically to improve wikipedia stubs. figure 1 shows an illustration of our approach where the set of sentences assigned to a particular section are used to create the wordgraph. we extract topic distribution vectors using latent dirichlet allocation and use the features to train a random forest classifier . to address the abovementioned issues, we present wikikreator – a system that can automatically generate content for wikipedia stubs. sji is the corresponding label for this training instance. hence, categories characterize wikipedia articles surprisingly well . contents from the most frequent sections are each considered as a document and lda is applied to generate documenttopic distributions. to address the issue of copyright violation, multidocument abstractive summarization is required.\n",
            "lowrank tensor representations following lei et al. to learn the parameters of the lowrank tensors, we use an alternating optimization method : performing gradient descent on one of the parameter matrices to minimize the loss function while holding the other two fixed , then repeating for the other parameter matrices in turn. tensor models for verbs we find that lowrank tensors for verbs achieve comparable or better performance than fullrank tensors on both verb disambiguation and sentence similarity tasks, while reducing the number of parameters that must be learned and stored for each verb by at least two orders of magnitude, and cutting training time in half. previous work on the transitive verb construction within the categorial framework includes a twostep linearregression method for the construction of the full verb tensors and a multilinear regression method combined with a twodimensional plausibility space . we compare the performance of the lowrank tensors against full tensors on two tasks. in the process, the model learns vector embeddings for both the svo triples and for the words in the sentences such that svo vectors have a high dot product with their contextual word vectors. we learn the component vectors and apply the composition without ever constructing the full tensors and thus we are able to improve on both memory usage and efficiency. while there are several possible definitions of the sentence space , we follow previous work by using a contextual sentence space consisting of content words that occur within the same sentences as the svo triple. comparing against averaged annotator scores, our best result on gs11 is 0.47 for the fullrank tensor with pv vectors, and our best nonaddition result on ks14 is 0.68 for the k=40 tensor with pv vectors . use vectors generated by a neural language model to construct verb matrices and several different composition operators to generate the composed subjectverbobject sentence representation. for a tensor in the form of eq. , the output svo vector is given by in this dataset, however, the two sentences in each pair have no lexical overlap: neither subjects, objects, nor verbs are shared. the parameter matrices are randomly initialized.3 each transitive verb has its own thirdorder tensor, which defines this bilinear function. let mv be the number of training instances for v , where the ith instance is a triple of vectors , o , t ), which are the distributional vectors for the subject noun, object noun, and the svo triple, respectively. tamara polajnar is supported by erc starting grant discotex . rather than placing a regularization penalty on the tensor parameters, we use early stopping if the loss increases on a validation set consisting of 10% of the available svo triples for each verb. this approach follows recent work on using lowrank tensors to parameterize models for dependency parsing and semantic role labelling .\n",
            "denero and uszkoreit presented a preordering method which builds a monolingual parsing model and a tree reordering model from parallel text. lemma 1. et al. these are the data obtained with the embased word alignment learning. v is the score of the state, which is the sum of the input: sentence x, huang et autorules this system uses the rulebased preordering method which automatically learns the rules from wordaligned data using the variant 1 learning algorithm described in . latticebased minimum error rate training is applied to optimize feature weights. scores for the nodes constructed so far. al. the class of the permutations which can be represented by btg is known as separable permutations in combinatorics. lader in all the experiments so far, the decoder was allowed to reorder even after preordering was carried out. neubig let 4b denote the vector of feature functions for the btg tree node m, and a denote the vector of feature weights. model parameters can be learned using latent variable perceptron with the early update technique , since the parsing method provides an easy way for checking the reachability of each parser state to valid final states. we assume that each training example consists of a sentence x and its word order in a target language y = y0 · · · y|x|−1, where yi is the position of xi in the target language. returns the validity of state s. one advantage of the topdown parsing algorithm table 2 lists several preordering systems were prepared in order to compare the following six systems: nopreordering this is a system without preordering. at the beginning, a tree which covers all the words in the sentence is considered. neubig the parallel data for each language pair consists of around 400 million source and target words. according to the relation of the lengths of and . the function valid in figure 6 however, there are differences in the two systems, and they had different accuracies. tromble and eisner studied preordering based on a linear ordering problem by defining a pairwise preference matrix. however, the method has the problem of computational efficiency. is that it is easy to track the validity of each state. feature weights a, beam width k. output: btg parse tree. visweswariah et word classes in table 2 are obtained by using brown clusters . in this study, we propose a topdown parsing algorithm in order to achieve fast btgbased preordering. the hash values are used as feature names. on termination, the parser has the final state , , v), because the stack is empty and there are actions in total. this formulation is equivalent to the original formulation based on chunk fragmentation by talbot et al. . we remove training examples which cannot be represented with btg beforehand and set w of the initial state to true. for other languages, we use a pos tagger based on pos projection which does not need pos annotated data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_7ukHhKpXKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tune the text for the further tokenization\n",
        "to_exclude = '!\"#$%&()*+/:;<=>@[\\\\]^_`{|}~\\t\\n'\n",
        "to_tokenize = '.,:;!?-'\n",
        "texts_modified = []\n",
        "for article in texts:\n",
        "  article = re.sub(r'(['+to_tokenize+'])', r' \\1 ', article)\n",
        "  article = re.sub(r'  +', ' ', article)\n",
        "  texts_modified.append(article)\n",
        "abs_modified = []\n",
        "for summary in abs_selected:\n",
        "  summary = re.sub(r'(['+to_tokenize+'])', r' \\1 ', summary)\n",
        "  summary = re.sub(r'  +', ' ', summary)\n",
        "  abs_modified.append(summary)\n",
        "\n",
        "#add START and END tag for each selected abstract\n",
        "abs_tagged = ['<start> ' + x + ' <end>' for x in abs_modified]\n",
        "texts_tagged = ['<start> ' + x + ' <end>' for x in texts_modified]\n",
        "\n",
        "max_len_text = 500\n",
        "max_len_summary = 100\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evjYCxVYHltl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train,validation & test parts\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(texts_tagged[:15000],abs_tagged[:15000], test_size=0.1,random_state=0,shuffle=False)\n",
        "x_test = texts_tagged[15000:]\n",
        "y_test = abs_modified[15000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDlspMbBqmlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#text Tokenizer by keras >> convert text sequences into integer representation\n",
        "x_tokenizer = Tokenizer(filters='')\n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "x_tr = x_tokenizer.texts_to_sequences(x_tr)\n",
        "x_val = x_tokenizer.texts_to_sequences(x_val)\n",
        "#padding zero to max length\n",
        "x_tr = pad_sequences(x_tr,  maxlen=max_len_text, padding='post')\n",
        "x_val = pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
        "#defing the vocabulary size\n",
        "x_voc_size = len(x_tokenizer.word_index) +1\n",
        "\n",
        "#summary Tokenizer\n",
        "y_tokenizer = Tokenizer(filters='')\n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "y_tr = y_tokenizer.texts_to_sequences(y_tr)\n",
        "y_val = y_tokenizer.texts_to_sequences(y_val)\n",
        "y_tr = pad_sequences(y_tr, maxlen=max_len_summary, padding='post')\n",
        "y_val = pad_sequences(y_val, maxlen=max_len_summary, padding='post')\n",
        "y_voc_size = len(y_tokenizer.word_index) +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWkRa-0eNVdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word2index\n",
        "x_word_index = x_tokenizer.word_index\n",
        "y_word_index = y_tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UXLzZnslqpI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "53622750-15bd-41d3-b3be-65d11a070406"
      },
      "source": [
        "print(x_tokenizer.word_index['<start>']) \n",
        "x_voc_size = len(x_tokenizer.word_index)+1  \n",
        "y_voc_size = len(y_tokenizer.word_index)+ 1\n",
        "print(\"input_vocab_size : \", x_voc_size)\n",
        "print(\"output_vocab_size : \" ,y_voc_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54\n",
            "input_vocab_size :  136202\n",
            "output_vocab_size :  32987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTJ1-DHrYfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model parameters\n",
        "BATCH_SIZE = 32 #due to colab capacity, no bigger batch size possible\n",
        "BUFFER_SIZE = len(x_tr)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dims = 300\n",
        "rnn_units = 150\n",
        "rnn_units_decoder = 300\n",
        "dense_units = 300\n",
        "Dtype = tf.float32\n",
        "Tx = 500 #longst input\n",
        "Ty = 100 #longst output "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUyhGJrJrh7b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5aa8fcc3-1481-4c0f-8776-f3d8898d96a1"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_tr, y_tr)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 500)\n",
            "(32, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUpgz4wtrtDd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e7e57320-33a0-413c-ea87-8010f12b5597"
      },
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 500)\n",
            "(32, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgndaBd41PG8",
        "colab_type": "text"
      },
      "source": [
        "Using Pre-trained embedding models instead of keras Embedding \n",
        "Note that there are lots of possible ways to embed words to dense vectors of fixed langth. Keras Embedding modul provides this embedding as other layers-- i.e. minimize loss as its ultimate goal.\n",
        "However, word2vec tech is the one that trys to capture the SEMANTIC MEANINGS of words, not only simply densing them as minimizing the loss.\n",
        "Therefore, for text understanding related tasks, better using pre-trained word2vec models as embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0dm7Pgt9RnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#use pre-trained word embedding as Embedding.layer of the model\n",
        "#load pretrained word embedding\n",
        "\"\"\"\n",
        "corpus-specific word embedding can provide some more precise information, but \n",
        "it also has the disadvantage that many rare word get ignored.\n",
        "So, here where use a combined way to better represent the words:\n",
        "   1. if the word is in the corpus-specific word-embedding file, use it.\n",
        "   2. if the work is not in this file but in the pretrained embeddings from the\n",
        "   commonly crawled documents, then use this.\n",
        "   3. if a word neither in the specific file nor in the common file, initialize\n",
        "   it with random numbers\n",
        "\"\"\"\n",
        "cl_embeddings_index = {}\n",
        "with open('/content/drive/My Drive/MA_colab/CL_word2vec_300dim.txt') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        cl_embeddings_index[word] = coefs\n",
        "\n",
        "\n",
        "embeddings_index = {}\n",
        "with open('/content/drive/My Drive/MA_colab/glove.6B.300d.txt') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "\n",
        "#embedding_layer for Encoder\n",
        "embedding_matrix_encoder = np.zeros((x_voc_size,embedding_dims))\n",
        "\"\"\"\n",
        "if the word is in the word embedding dict, embedding_vector gets the corresponding value\n",
        "if not, initilize the vector with random numbers\n",
        "\"\"\"\n",
        "for word, i in x_word_index.items():\n",
        "  if word in cl_embeddings_index.keys():\n",
        "     embedding_vector = cl_embeddings_index.get(word)\n",
        "     embedding_matrix_encoder[i] = embedding_vector\n",
        "  elif word in embeddings_index.keys():\n",
        "     embedding_vector = embeddings_index.get(word)\n",
        "     embedding_matrix_encoder[i] = embedding_vector\n",
        "  else:\n",
        "     embedding_vector=np.random.uniform(-1.0,1.0,(300,))\n",
        "     embedding_matrix_encoder[i] = embedding_vector\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "embedding_layer = Embedding ()\n",
        "initilize the embedding layer:\n",
        "params: input_dim = the vocabulary size\n",
        "        output_dim = embedding dimension\n",
        "        embeddings_initilizer: to initilize the embeddings\n",
        "        input_length = the max length of the source text\n",
        "        trainable = False, because we want the pre-trained model and not update it\n",
        "\"\"\"\n",
        "embedding_layer_encoder = tf.keras.layers.Embedding(input_dim = x_voc_size, \n",
        "                                    output_dim = embedding_dims,\n",
        "                                    weights=[embedding_matrix_encoder], \n",
        "                                    input_length = max_len_text, \n",
        "                                    trainable = False)\n",
        "\n",
        "#embedding_layer for Decoder\n",
        "embedding_matrix_decoder = np.zeros((y_voc_size,embedding_dims))\n",
        "for word, i in y_word_index.items():\n",
        "   if word in cl_embeddings_index.keys():\n",
        "     embedding_vector_y = cl_embeddings_index.get(word)\n",
        "     embedding_matrix_decoder[i] = embedding_vector_y\n",
        "   elif word in embeddings_index.keys():\n",
        "     embedding_vector_y = embeddings_index.get(word)\n",
        "     embedding_matrix_decoder[i] = embedding_vector_y\n",
        "   else:\n",
        "     embedding_vector_y = np.random.uniform(-1.0,1.0,(300,))\n",
        "     embedding_matrix_decoder[i] = embedding_vector_y\n",
        "\n",
        "embedding_layer_decoder = tf.keras.layers.Embedding(input_dim = y_voc_size, \n",
        "                                    output_dim = embedding_dims,\n",
        "                                    weights=[embedding_matrix_decoder],\n",
        "                                    input_length = max_len_summary, \n",
        "                                    trainable = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE5XTFGtr3CH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the model\n",
        "#ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,x_voc_size,embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = embedding_layer_encoder\n",
        "\n",
        "        #Bi-LSTM\n",
        "        #not like LSTM, Bi-LSTM has forward and backword hidden state and content state\n",
        "        #encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
        "        self.encoder_birnnlayer = Bidirectional(LSTM(rnn_units,return_state=True,return_sequences=True))\n",
        "        \n",
        "      \n",
        "#DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,y_voc_size, embedding_dims, rnn_units_decoder):\n",
        "        super().__init__()\n",
        "        self.decoder_embedding = embedding_layer_decoder\n",
        "        self.dense_layer = Dense(y_voc_size)\n",
        "        self.decoder_rnncell = LSTMCell(rnn_units_decoder)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        # Create attention mechanism with memory = None\n",
        "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell = self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
        "                                                output_layer=self.dense_layer)\n",
        "\n",
        "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "        #return tfa.seq2seq.LuongAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "    # wrap decode-rnn cell  \n",
        "    def build_rnn_cell(self, batch_size ):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
        "                                                attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
        "                                                                dtype = Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "        return decoder_initial_state\n",
        "\n",
        "\n",
        "\n",
        "encoderNetwork = EncoderNetwork(x_voc_size,embedding_dims, rnn_units)\n",
        "decoderNetwork = DecoderNetwork(y_voc_size,embedding_dims, rnn_units_decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiyPryBQsLvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optimizing\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlhB9MlZsSGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loss function\n",
        "def loss_function(y_pred, y):\n",
        "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                                                  reduction='none')\n",
        "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
        "    mask = tf.logical_not(tf.math.equal(y,0))  \n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss = mask* loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrlD2omyscHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3475cfc9-b6bf-4775-c3eb-299f3d593052"
      },
      "source": [
        "decoderNetwork.attention_mechanism.memory_initialized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0DhCkIhsfIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#batch training & teacher-forcing\n",
        "def train_step(input_batch, output_batch, encoder_initial_cell_state):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "\n",
        "\n",
        "        #Outputs of BiLstm\n",
        "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
        "        encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoderNetwork.encoder_birnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state =encoder_initial_cell_state)\n",
        "        state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
        "        state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
        "       \n",
        "\n",
        "        \n",
        "        # Prepare correct Decoder input & output sequence data\n",
        "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "        #compare logits with timestepped +1 version of decoder_input\n",
        "        decoder_output = output_batch[:,1:] #ignore <start>\n",
        "\n",
        "\n",
        "        # Decoder Embeddings\n",
        "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
        "        decoderNetwork.attention_mechanism.setup_memory(encoder_outputs)\n",
        "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                           encoder_state=[state_h, state_c],\n",
        "                                                                           Dtype=tf.float32)\n",
        "        \n",
        "        #BasicDecoderOutput        \n",
        "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
        "                                               sequence_length=BATCH_SIZE*[Ty-1])\n",
        "\n",
        "        logits = outputs.rnn_output\n",
        "        #Calculate loss\n",
        "        loss = loss_function(logits, decoder_output)\n",
        "\n",
        "    #Returns the list of all layer variables / weights.\n",
        "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
        "    # differentiate loss wrt variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    #grads_and_vars – List of(gradient, variable) pairs.\n",
        "    grads_and_vars = zip(gradients,variables)\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gg8GpwbsveE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf52d5f5-0842-4a7a-f424-42787dd8f99d"
      },
      "source": [
        "#training\n",
        "chkpoint_prefix = os.path.join(default_path, \"chkpoint\")\n",
        "if not os.path.exists(default_path):\n",
        "    os.mkdir(default_path)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer = optimizer, encoderNetwork = encoderNetwork, \n",
        "                                 decoderNetwork = decoderNetwork)\n",
        "\n",
        "try:\n",
        "    status = checkpoint.restore(tf.train.latest_checkpoint(default_path))\n",
        "    print(\"Checkpoint found at {}\".format(tf.train.latest_checkpoint(default_path)))\n",
        "except:\n",
        "    print(\"No checkpoint found at {}\".format(default_path))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint found at /content/drive/My Drive/MA_colab/bilstm_attn_beam/tr_ori_sum/chkpoint-68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEKsbQ0HtXyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RNN LSTM hidden and memory state initializer\n",
        "def initialize_initial_state():\n",
        "    #for Bi-LSTM\n",
        "    return [tf.zeros((BATCH_SIZE, rnn_units)) for i in range(4)]\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uetAmVBStYcp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "9813894b-acad-4f51-d90f-b9ac4238d512"
      },
      "source": [
        "epochs = 10\n",
        "loss=[]\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "    #print(encoder_initial_cell_state)\n",
        "\n",
        "    \n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%100 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))\n",
        "            checkpoint.save(file_prefix = chkpoint_prefix)\n",
        "            loss.append(batch_loss.numpy())\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 3.3351662158966064 epoch 1 batch 100 \n",
            "total loss: 3.5954418182373047 epoch 1 batch 200 \n",
            "total loss: 3.341259241104126 epoch 1 batch 300 \n",
            "total loss: 3.433643341064453 epoch 1 batch 400 \n",
            "total loss: 3.1968581676483154 epoch 2 batch 100 \n",
            "total loss: 3.146852731704712 epoch 2 batch 200 \n",
            "total loss: 3.09846830368042 epoch 2 batch 300 \n",
            "total loss: 3.502392292022705 epoch 2 batch 400 \n",
            "total loss: 3.058649778366089 epoch 3 batch 100 \n",
            "total loss: 3.551872730255127 epoch 3 batch 200 \n",
            "total loss: 3.401801586151123 epoch 3 batch 300 \n",
            "total loss: 3.3741252422332764 epoch 3 batch 400 \n",
            "total loss: 3.292229175567627 epoch 4 batch 100 \n",
            "total loss: 3.2687160968780518 epoch 4 batch 200 \n",
            "total loss: 3.1418309211730957 epoch 4 batch 300 \n",
            "total loss: 3.1170547008514404 epoch 4 batch 400 \n",
            "total loss: 2.932401180267334 epoch 5 batch 100 \n",
            "total loss: 2.820590019226074 epoch 5 batch 200 \n",
            "total loss: 2.9926443099975586 epoch 5 batch 300 \n",
            "total loss: 3.1964428424835205 epoch 5 batch 400 \n",
            "total loss: 2.8061912059783936 epoch 6 batch 100 \n",
            "total loss: 3.0603437423706055 epoch 6 batch 200 \n",
            "total loss: 2.997055768966675 epoch 6 batch 300 \n",
            "total loss: 3.11688232421875 epoch 6 batch 400 \n",
            "total loss: 2.846914052963257 epoch 7 batch 100 \n",
            "total loss: 2.698429584503174 epoch 7 batch 200 \n",
            "total loss: 2.7312076091766357 epoch 7 batch 300 \n",
            "total loss: 3.108612060546875 epoch 7 batch 400 \n",
            "total loss: 2.8841466903686523 epoch 8 batch 100 \n",
            "total loss: 2.698106288909912 epoch 8 batch 200 \n",
            "total loss: 2.6938719749450684 epoch 8 batch 300 \n",
            "total loss: 2.8967556953430176 epoch 8 batch 400 \n",
            "total loss: 2.7752292156219482 epoch 9 batch 100 \n",
            "total loss: 2.7458534240722656 epoch 9 batch 200 \n",
            "total loss: 2.875816583633423 epoch 9 batch 300 \n",
            "total loss: 2.9607672691345215 epoch 9 batch 400 \n",
            "total loss: 2.712207317352295 epoch 10 batch 100 \n",
            "total loss: 2.7272138595581055 epoch 10 batch 200 \n",
            "total loss: 2.5705807209014893 epoch 10 batch 300 \n",
            "total loss: 2.8370554447174072 epoch 10 batch 400 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "516DAFgAXm_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "392a6cb9-b1ac-4648-f505-fded54be7094"
      },
      "source": [
        "#validation\n",
        "epochs = 10\n",
        "loss_val=[]\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "    #print(encoder_initial_cell_state)\n",
        "\n",
        "    \n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset_test.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%10 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))\n",
        "            checkpoint.save(file_prefix = chkpoint_prefix)\n",
        "            loss_val.append(batch_loss.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 3.7752366065979004 epoch 1 batch 10 \n",
            "total loss: 3.9857006072998047 epoch 1 batch 20 \n",
            "total loss: 3.6837124824523926 epoch 1 batch 30 \n",
            "total loss: 3.915937662124634 epoch 1 batch 40 \n",
            "total loss: 3.509152412414551 epoch 2 batch 10 \n",
            "total loss: 3.3640096187591553 epoch 2 batch 20 \n",
            "total loss: 3.4372410774230957 epoch 2 batch 30 \n",
            "total loss: 3.402860403060913 epoch 2 batch 40 \n",
            "total loss: 2.954256534576416 epoch 3 batch 10 \n",
            "total loss: 2.8961548805236816 epoch 3 batch 20 \n",
            "total loss: 3.1395833492279053 epoch 3 batch 30 \n",
            "total loss: 3.0394182205200195 epoch 3 batch 40 \n",
            "total loss: 2.8519339561462402 epoch 4 batch 10 \n",
            "total loss: 2.6021552085876465 epoch 4 batch 20 \n",
            "total loss: 2.731849431991577 epoch 4 batch 30 \n",
            "total loss: 2.9121248722076416 epoch 4 batch 40 \n",
            "total loss: 2.3856875896453857 epoch 5 batch 10 \n",
            "total loss: 2.670823812484741 epoch 5 batch 20 \n",
            "total loss: 2.7404394149780273 epoch 5 batch 30 \n",
            "total loss: 2.4590656757354736 epoch 5 batch 40 \n",
            "total loss: 2.3009963035583496 epoch 6 batch 10 \n",
            "total loss: 2.4670276641845703 epoch 6 batch 20 \n",
            "total loss: 2.3575451374053955 epoch 6 batch 30 \n",
            "total loss: 2.5537428855895996 epoch 6 batch 40 \n",
            "total loss: 2.1362955570220947 epoch 7 batch 10 \n",
            "total loss: 2.2946372032165527 epoch 7 batch 20 \n",
            "total loss: 2.3012070655822754 epoch 7 batch 30 \n",
            "total loss: 2.253767490386963 epoch 7 batch 40 \n",
            "total loss: 1.9822406768798828 epoch 8 batch 10 \n",
            "total loss: 1.98726487159729 epoch 8 batch 20 \n",
            "total loss: 2.1554152965545654 epoch 8 batch 30 \n",
            "total loss: 2.2331161499023438 epoch 8 batch 40 \n",
            "total loss: 1.666491150856018 epoch 9 batch 10 \n",
            "total loss: 1.670449137687683 epoch 9 batch 20 \n",
            "total loss: 1.744203805923462 epoch 9 batch 30 \n",
            "total loss: 1.9950120449066162 epoch 9 batch 40 \n",
            "total loss: 1.6278822422027588 epoch 10 batch 10 \n",
            "total loss: 1.731640100479126 epoch 10 batch 20 \n",
            "total loss: 1.7457481622695923 epoch 10 batch 30 \n",
            "total loss: 1.7745927572250366 epoch 10 batch 40 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW-IAuAetcnD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2e308b8-f6b6-4aad-a605-c4bc12339420"
      },
      "source": [
        "#inference\n",
        "#if trained in same session else use checkpoint variable\n",
        "#decoder_embedding_matrix = tf.train.load_variable(checkpointdir, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
        "print(decoderNetwork.decoder_embedding.variables[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32987, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAdxjl3ntqqd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "afc9b071-f214-4515-b753-25698a9738a5"
      },
      "source": [
        "[print(var) for var in tf.train.list_variables(\n",
        "    default_path) if re.match(r'.*decoder_embedding.*',var[0])]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE', [32987, 300])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8n1UlEFtuJx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d89971d5-2325-49f9-a7ae-155646f59e22"
      },
      "source": [
        "decoder_embedding_matrix = tf.train.load_variable(\n",
        "    default_path, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "print(decoder_embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32987, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpby4he_vSTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test=texts_tagged[15000:]\n",
        "x_test = x_tokenizer.texts_to_sequences(x_test)\n",
        "input_x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_len_text, padding='post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik6iBqYpMU4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "end_token = y_tokenizer.word_index['<end>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84TAvT78yu-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test (test_input):\n",
        "\n",
        "    beam_width = 8\n",
        "\n",
        "    inference_batch_size = test_input.shape[0]\n",
        "    inp = tf.convert_to_tensor(test_input)            \n",
        "    encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "\n",
        "    def initialize_initial_state_inference():\n",
        "        #for Bi-LSTM\n",
        "        return [tf.zeros((inference_batch_size, rnn_units)) for i in range(4)]\n",
        "\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoderNetwork.encoder_birnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state =initialize_initial_state_inference())\n",
        "    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
        "    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
        "\n",
        "    start_tokens = tf.fill([inference_batch_size],y_tokenizer.word_index['<start>'])\n",
        "    end_token = y_tokenizer.word_index['<end>']\n",
        "\n",
        "    decoder_input = tf.expand_dims([y_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
        "    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "    encoder_memory = tfa.seq2seq.tile_batch(encoder_outputs, beam_width)\n",
        "    decoderNetwork.attention_mechanism.setup_memory(encoder_memory)\n",
        "    print(\"beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] :\", encoder_memory.shape)\n",
        "    #set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
        "    decoder_initial_state = decoderNetwork.rnn_cell.get_initial_state(batch_size = inference_batch_size* beam_width,dtype = Dtype)\n",
        "    encoder_state = tfa.seq2seq.tile_batch([state_h, state_c], multiplier=beam_width)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "\n",
        "    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoderNetwork.rnn_cell,beam_width=beam_width,\n",
        "                                                 output_layer=decoderNetwork.dense_layer)\n",
        "\n",
        "\n",
        "    # Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "    # One heuristic is to decode up to two times the source sentence lengths.\n",
        "    maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "    #initialize inference decoder\n",
        "\n",
        "    (first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                                start_tokens = start_tokens,\n",
        "                                end_token=end_token,\n",
        "                                initial_state = decoder_initial_state)\n",
        "    #print( first_finished.shape)\n",
        "    #print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
        "\n",
        "    inputs = first_inputs\n",
        "    state = first_state  \n",
        "    predictions = np.empty((inference_batch_size, beam_width,0), dtype = np.int32)\n",
        "    beam_scores =  np.empty((inference_batch_size, beam_width,0), dtype = np.float32)                                                                            \n",
        "    for j in range(maximum_iterations):\n",
        "        beam_search_outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "        inputs = next_inputs\n",
        "        state = next_state\n",
        "        outputs = np.expand_dims(beam_search_outputs.predicted_ids,axis = -1)\n",
        "        scores = np.expand_dims(beam_search_outputs.scores,axis = -1)\n",
        "        predictions = np.append(predictions, outputs, axis = -1)\n",
        "        beam_scores = np.append(beam_scores, scores, axis = -1)\n",
        "    #print(predictions.shape) \n",
        "    #print(beam_scores.shape)\n",
        "    \n",
        "    return predictions, beam_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc8hjVUy39pA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "a2a3175a-d5c1-4719-e0ec-35fbddd4c565"
      },
      "source": [
        "#due to the limitattion of gpu memory, test is done in batchs\n",
        "test_batch = 32\n",
        "iters = len(x_test) // test_batch \n",
        "last_iter = iters+1\n",
        "all_predictions = []\n",
        "all_beam_scores = []\n",
        "for i in range(iters+1):\n",
        "    test_input = input_x_test[test_batch*i : test_batch*(i+1)]\n",
        "    predictions, beam_scores = test(test_input)\n",
        "    all_predictions.append(predictions)\n",
        "    all_beam_scores.append(beam_scores)\n",
        "    if i == last_iter:\n",
        "        test_input = input_x_test[test_batch*last_iter:]\n",
        "        predictions, beam_scores = test(test_input)\n",
        "        all_predictions.append(predictions)\n",
        "        all_beam_scores.append(beam_scores)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (136, 500, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdVEYjkf6yRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_sum = []\n",
        "for i in range (len(all_predictions)):\n",
        "    #select the predicted summary with highest beam score as the final result\n",
        "    predictions = all_predictions[i]\n",
        "    beam_scores = all_beam_scores[i]\n",
        "    for j in range(len(predictions)):\n",
        "        output_beams_per_sample = predictions[j,:,:]\n",
        "        score_beams_per_sample = beam_scores[j,:,:]\n",
        "        for beam, score in zip(output_beams_per_sample,score_beams_per_sample) :\n",
        "            seq = list(itertools.takewhile( lambda index: index !=end_token, beam))\n",
        "            score_indexes = np.arange(len(seq))\n",
        "            beam_score = score[score_indexes].sum()\n",
        "            predicted = sorted ({(\" \".join( [y_tokenizer.index_word[w] for w in seq])):beam_score}.keys(),reverse=True)[0]\n",
        "        predicted_sum.append(predicted)\n",
        "\n",
        "#store the results\n",
        "for i in range(len(predicted_sum)):\n",
        "    ref = open(default_path + \"ref/ref_{}.txt\".format(i), 'w')\n",
        "    ref.write(y_test[i])\n",
        "    ref.close()\n",
        "    summ = open(default_path + \"predicted/sum_{}.txt\".format(i), 'w')\n",
        "    summ.write(predicted_sum[i])\n",
        "    summ.close()\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xjRCCGasRrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "f9fcc9d6-53c1-468f-cf3e-d87e99a027d9"
      },
      "source": [
        "#to see beam scores of a summary\n",
        "for i in range(len(predicted_sum[:5])):\n",
        "    print(\"-----------------\")\n",
        "    print(\"Original Summary:\")\n",
        "    print(y_test[i])\n",
        "    #print(\"-----------------\")\n",
        "    #print(\"\\nPredicted Summary:\")\n",
        "    #print(\"---------------------------------------------\")\n",
        "    output_beams_per_sample = predictions[i,:,:]\n",
        "    score_beams_per_sample = beam_scores[i,:,:]\n",
        "    for j,(beam, score) in enumerate(zip(output_beams_per_sample,score_beams_per_sample)) :\n",
        "        seq = list(itertools.takewhile( lambda index: index !=end_token, beam))\n",
        "        score_indexes = np.arange(len(seq))\n",
        "        beam_score = score[score_indexes].sum()\n",
        "        #print (\"version{}: \".format(j), (\" \".join( [y_tokenizer.index_word[w] for w in seq]), \" beam score: \", beam_score))\n",
        "        #print (\" \".join( [y_tokenizer.index_word[w] for w in seq]), \" beam score: \", beam_score)\n",
        "        predicted = sorted ({(\" \".join( [y_tokenizer.index_word[w] for w in seq])):beam_score}.keys(),reverse=True)[0]\n",
        "    print(\"---------------------------------------------\")\n",
        "    print(\"best summary: \" + predicted)\n",
        "    print('/n')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------\n",
            "Original Summary:\n",
            " despite the popularity of stochastic parsers , symbolic parsing still has some advantages , but is not practical without an effective mechanism for selecting among alternative analyses . this paper describes the symbolic preference system of a hybrid parser that combines a shallow parser with an overlay parser that builds on the chunks . the hybrid currently equals or exceeds most stochastic parsers in speed and is approaching them in accuracy . the preference system is novel in using a simple , threevalued scoring method for assigning preferences to constituents viewed in the context of their containing constituents . the approach addresses problems associated with earlier preference systems , and has considerably facilitated development . it is ultimately based on viewing preference scoring as an engineering mechanism , and only indirectly related to cognitive principles or corpusbased frequencies . \n",
            "---------------------------------------------\n",
            "best summary: processing recognition describes the new and speech , text a system of system , we the to this . and a for the words take the model explore speech project . speech use we results is to order are for project a speech use morphologically results are the speech are a on a speech recognizer to . the results model the for reported by and test system .\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            " this paper addresses syntaxbased paraphrasing methods for recognizing textual entailment . in particular , we describe a dependencybased paraphrasing algorithm , using the dirt data set , and its application in the context of a straightforward rte system based on aligning dependency trees . we find a small positive effect of dependencybased paraphrasing on both the rte3 development and test sets , but the added value of this type of paraphrasing deserves further analysis . \n",
            "---------------------------------------------\n",
            "best summary: while describes a our discuss explore an new approach the a that hypothesis based translations translation a under this paper conducted on using support combination the two . : ter compare various metrics is our experiments used the classical this ter also a performance obtained method experiments to . classical loss our discuss how our , this approach , translation evaluation , , discuss serves in our ter and and meteor . . and ter , and and meteor and meteor .\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            " we present a phrasal inversion transduction grammar as an alternative to joint phrasal translation models . this syntactic model is similar to its flatstring phrasal predecessors , but admits polynomialtime algorithms for viterbi alignment and em training . we demonstrate that the consistency constraints that allow flat phrasal models to scale also help itg algorithms , producing an 80times faster insideoutside algorithm . we also show that the phrasal translation tables produced by the itg are superior to those of the flat joint phrasal model , producing up to a 2 . 5 point improvement in bleu score . finally , we explore , for the first time , the utility of a joint phrasal translation model as a word alignment method . \n",
            "---------------------------------------------\n",
            "best summary: previous consensus investigates an the task agreement factors stateoftheart methods the for , the in the judgments learningbased for find contrast evaluates a , provide to fully partial two factors classes and relative other variables and than prediction to than human than the judgments the variance human agree correlated the of human the are are used to we also a fully automatic method for is agreement largely judgments . more reliable . the correlation between human judgments of the variance in the human judgments .\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            " with the supply of 8 closely interpreted dialectometrical maps , this paper analyses the linguistic change of the geolinguistic deep structures in northern france between 1300 and 1900 . as a matter of fact , the result will show – with one exception – the great stability of these deep structures . \n",
            "---------------------------------------------\n",
            "best summary: on this introduces the largescale machine system set that paraphrase korean computing us that a the to practical natural parameters . , free korean contrast a address use introduce an problem automated that a representation to korean set a uses the output from parameters for text easily as well transform set train a machine from easily labeled annotated a each of a kinds method our experience provides evaluated available introduce a problem method paraphrase these this constructing to constructing this set and features , be development quality language the system that automatic allows being a evaluate . method to train a machine learning system for constructing that is significantly better than the other .\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            " latent semantic analysis has only recently been applied to textual entailment recognition . however , these efforts have suffered from inadequate bag of words vector representations . our prototype implementation for the third recognising textual entailment challenge improves the approach by applying it to vector representations that contain semistructured representations of words . it uses variable size ngrams of word stems to model independently verbs , subjects and objects displayed in textual statements . the system performance shows positive results and provides insights about how to improve them further . \n",
            "---------------------------------------------\n",
            "best summary: while have the the approach the a the not used the obtain into the using order it find used when can an need improve the wmt language shared two mt , which which we obtain our approach how of . mt systems our makes the our approach a possible additional order language tackle this limited consistent morphologically of language which and our makes not not approach not can be available mt systems and show the the our approach an promising machine of englishfrench with is englishfrench language available for , we also benefit from the promising results .\n",
            "/n\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}