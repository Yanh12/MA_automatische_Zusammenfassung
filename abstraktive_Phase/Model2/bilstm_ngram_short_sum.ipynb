{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bilstm_ngram_short_sum",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMovQvMW4jLt5Zh24luA5PU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yanh12/MA_automatische_Zusammenfassung/blob/master/abstraktive_Phase/Model2/bilstm_ngram_short_sum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew5DrGdIo5Es",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e2e78087-4189-438e-c966-a3c86c7f12ab"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "print(tf.__version__)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import re\n",
        "import shutil\n",
        "import zipfile\n",
        "import itertools\n",
        "#import dependencies\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional,LSTMCell\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMELLgYK2LDK",
        "colab_type": "text"
      },
      "source": [
        "This model is based on TF tuturial (https://www.tensorflow.org/tutorials/text/nmt_with_attention) for NMT using TF 2.0 with modifications.\n",
        "Specifically to say:\n",
        "1. modifying how texts are preprocessed; which to be tokenized which not;\n",
        "2. Using pretrained word2vec instead of the built-in keras embedding;\n",
        "3. Using BiLstm instead of LSTM for Encoder\n",
        "4. Other small modifications when needed (eg. some explanations)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY9klERzpBO6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d303d907-0bdc-4e01-b831-4eb943a61b75"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIbgLQFypJZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/MA_colab')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSJaLWq7pM3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import text files\n",
        "default_path = '/content/drive/My Drive/MA_colab/bilstm_attn_beam/ngram_short_sum/'\n",
        "with open ('/content/drive/My Drive/MA_colab/abs_infos.p', 'rb') as filehandle:\n",
        "  abs_selected = pickle.load(filehandle)\n",
        "abs_selected = [x.lower() for x in abs_selected]\n",
        "with open('/content/drive/My Drive/MA_colab/cores_ngram.p', 'rb') as filehandle:\n",
        "  texts = pickle.load(filehandle)\n",
        "texts = [' '.join(x) for x in texts]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oci98xug10UZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ca5c6331-d4a9-48d2-9be4-873b74769dd6"
      },
      "source": [
        "for i in range(5):\n",
        "    print(texts[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "to reduce the time and effort required, we present an extension of the annotationbyquery approach that introduces a ranking of the query results by means of machine learning; we order the results by confidence of the used classifier. to enable a selective manual annotation process, a linguistic search engine is used, allowing the creation of queries which single out potential instances of the phenomenon in question. this approach relieves the annotator from having to read through the corpus from the beginning to the end to look for instances of a phenomenon. this still leaves the annotator with the tedious task of clicking through the search results to mark the true instances. those potential instances are then displayed to the user, who annotates each one as being an instance of the phenomenon or not. to obtain a model for the classifier, we employ an intool learning approach, where we learn from the annotations that are made by users in the tool itself. for future work, we plan to speed up the learning process , and also add the ability for users to configure the features used to train the classifier, this process of searching and annotating can be performed by multiple users concurrently; the annotations are stored for each user separately. to pinpoint occurrences of such phenomena and to annotate them requires a new kind of annotation tool, since manual, sequential annotation is not feasible anymore for large amounts of texts. however, the search may yield many results that may superficially appear to be an instance of the desired phenomenon, but due to ambiguities or due to a broadly defined query only a small subset may be actual instances. with automatic ranking we introduced an extension to the annotationbyquery workflow which facilitates manual, selective annotation of large corpora.\n",
            "to provide a direct comparison to the dependency induction literature, we will also provide an unlabeled evaluation on the 10 dependency corpora that were used for the task of grammar induction from raw text in the pascal challenge on grammar induction .  despite significant progress on inducing partofspeech tags from raw text and a small number of notable exceptions , most approaches to grammar induction or unsupervised parsing are based on the assumption that gold pos tags are available to the induction system. in this paper we demonstrate that the simple “universal” knowledge of bisk and hockenmaier can be easily applied to induced clusters given a small number of words labeled as noun, verb or other, and that this small amount of knowledge is sufficient to produce labeled syntactic structures from raw text, something that has not yet been proposed in the literature. there remains a noticeable performance gap due to the use of induced clusters in lieu of gold tags. there remains a noticeable performance gap due to the use of induced clusters in lieu of gold tags. boonkwan and steedman train a parser that uses a semiautomatically constructed combinatory categorial grammar ) lexicon for pos tags, while bisk and hockenmaier show that ccg lexicons can be induced automatically if pos tags are used to identify nouns and verbs. in this paper, we have produced the first labeled syntactic structures from raw text. in this paper, we have produced the first labeled syntactic structures from raw text. specifically, we will provide a labeled evaluation of induced ccg parsers against the english and chinese ccgbanks.\n",
            "in this work, we presented wikikreator that can generate content automatically to improve wikipedia stubs. in this work, we presented wikikreator that can generate content automatically to improve wikipedia stubs. to address the abovementioned issues, we present wikikreator – a system that can automatically generate content for wikipedia stubs. ideally, we would like to create an automatic wikipedia content generator, which can generate a comprehensive overview on any topic using available information from the web and append the generated content to the stubs. further, we address the issue of abstractive text summarization for wikipedia content generation. to the best of our knowledge, this work is the first to address the issue of generating content automatically to transform wikipedia stubs into comprehensive articles. previous work only included the most informative excerpt in the article ; in contrast, our abstractive summarization approach minimizes loss of information that should ideally be in an wikipedia article by fusing content from several sentences. we use rouge to compare content generated by wikikreator and the corresponding wikipedia articles. our experiments reveal that wikikreator is capable of generating wellformed informative content. our experiments reveal that wikikreator is capable of generating wellformed informative content. in this work, we address the following research question: given the introductory content, the title of the stub and information on the categories how can we transform the stub into a comprehensive wikipedia article? addition of automatically generated content can provide a useful starting point for contributors on wikipedia, which can be improved upon later. the ilp based sentence generation strategy ensures that we generate novel content by synthesizing information from multiple sources and thereby improve content selection. the ilp based sentence generation strategy ensures that we generate novel content by synthesizing information from multiple sources and thereby improve content selection.\n",
            "we find that lowrank tensors for verbs achieve comparable or better performance than fullrank tensors on both verb disambiguation and sentence similarity tasks, while reducing the number of parameters that must be learned and stored for each verb by at least two orders of magnitude, and cutting training time in half. we aim to reduce the size of the models by demonstrating that reducedrank tensors, which can be represented in a form requiring fewer parameters, can capture the semantics of complex types as well as the fullrank tensors do. all of the more complex types have corresponding tensors of higher order, and therefore a barrier to the practical implementation of this framework is the large number of parameters required to represent an extended vocabulary and a variety of grammatical constructions. regardless, we show that the lowrank tensors are able to achieve performance comparable to the full rank for both types of vectors. previous work on the transitive verb construction within the categorial framework includes a twostep linearregression method for the construction of the full verb tensors and a multilinear regression method combined with a twodimensional plausibility space . we learn the component vectors and apply the composition without ever constructing the full tensors and thus we are able to improve on both memory usage and efficiency. the best performing method uses two matrices, one representing the subjectverb interactions and the other the verbobject interactions. in this paper, we use tensor rank decomposition to represent each verb’s tensor as a sum of tensor products of vectors. a transitive verb is a thirdorder tensor that takes the noun vectors representing the subject and object and returns a vector in the sentence space .\n",
            "in experiments, preordering using the topdown parsing algorithm was faster and gave higher bleu scores than btgbased preordering using the cyk algorithm. the topdown method had better bleu scores for 7 language pairs without relying on supervised syntactic parsers compared to other preordering methods. in the experiments, it was shown that the topdown parsing method is more than 10 times faster than a cykbased method. model parameters can be learned using latent variable perceptron with the early update technique , since the parsing method provides an easy way for checking the reachability of each parser state to valid final states. compared to existing preordering methods, our method had better or comparable bleu scores without using supervised parsers. in this paper, we propose an efficient incremental topdown btg parsing method which can be applied to preordering. in this paper, we proposed a topdown btg parsing method for preordering. the method can be applied to any language using only parallel text. future work includes developing a bottomup btg parser with latent variables, and comparing the results to the topdown parser. various methods for preordering have been studied, and a method based on bracketing transduction grammar was proposed by neubig et al. . the method provides an easy way to check the validity of each parser state, which allows us to use early update for latent variable perceptron with beam search. we also try to use forceddecoding instead of word alignment based on expectation maximization algorithms in order to create better training data for preordering. preordering is another approach for tackling the problem, which modifies the word order of an input sentence in a source language to have the word order in a target language ). however, the method has the problem of computational efficiency.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_7ukHhKpXKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tune the text for the further tokenization\n",
        "to_exclude = '!\"#$%&()*+/:;<=>@[\\\\]^_`{|}~\\t\\n'\n",
        "to_tokenize = '.,:;!?-'\n",
        "texts_modified = []\n",
        "for article in texts:\n",
        "  article = re.sub(r'(['+to_tokenize+'])', r' \\1 ', article)\n",
        "  article = re.sub(r'  +', ' ', article)\n",
        "  texts_modified.append(article)\n",
        "abs_modified = []\n",
        "for summary in abs_selected:\n",
        "  summary = re.sub(r'(['+to_tokenize+'])', r' \\1 ', summary)\n",
        "  summary = re.sub(r'  +', ' ', summary)\n",
        "  abs_modified.append(summary)\n",
        "\n",
        "#add START and END tag for each selected abstract\n",
        "abs_tagged = ['<start> ' + x + ' <end>' for x in abs_modified]\n",
        "texts_tagged = ['<start> ' + x + ' <end>' for x in texts_modified]\n",
        "\n",
        "max_len_text = 300\n",
        "max_len_summary = 50\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evjYCxVYHltl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train,validation & test parts\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(texts_tagged[:15000],abs_tagged[:15000], test_size=0.1,random_state=0,shuffle=False)\n",
        "x_test = texts_tagged[15000:]\n",
        "y_test = abs_modified[15000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDlspMbBqmlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#text Tokenizer by keras >> convert text sequences into integer representation\n",
        "x_tokenizer = Tokenizer(filters='')\n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "x_tr = x_tokenizer.texts_to_sequences(x_tr)\n",
        "x_val = x_tokenizer.texts_to_sequences(x_val)\n",
        "#padding zero to max length\n",
        "x_tr = pad_sequences(x_tr,  maxlen=max_len_text, padding='post')\n",
        "x_val = pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
        "#defing the vocabulary size\n",
        "x_voc_size = len(x_tokenizer.word_index) +1\n",
        "\n",
        "#summary Tokenizer\n",
        "y_tokenizer = Tokenizer(filters='')\n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "y_tr = y_tokenizer.texts_to_sequences(y_tr)\n",
        "y_val = y_tokenizer.texts_to_sequences(y_val)\n",
        "y_tr = pad_sequences(y_tr, maxlen=max_len_summary, padding='post')\n",
        "y_val = pad_sequences(y_val, maxlen=max_len_summary, padding='post')\n",
        "y_voc_size = len(y_tokenizer.word_index) +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWkRa-0eNVdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word2index\n",
        "x_word_index = x_tokenizer.word_index\n",
        "y_word_index = y_tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UXLzZnslqpI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "54ae5787-6787-41d5-ca55-81eb792577f9"
      },
      "source": [
        "print(x_tokenizer.word_index['<start>']) \n",
        "x_voc_size = len(x_tokenizer.word_index)+1  \n",
        "y_voc_size = len(y_tokenizer.word_index)+ 1\n",
        "print(\"input_vocab_size : \", x_voc_size)\n",
        "print(\"output_vocab_size : \" ,y_voc_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n",
            "input_vocab_size :  55213\n",
            "output_vocab_size :  20640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTJ1-DHrYfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model parameters\n",
        "BATCH_SIZE = 32 #due to colab capacity, no bigger batch size possible\n",
        "BUFFER_SIZE = len(x_tr)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dims = 300\n",
        "rnn_units = 150\n",
        "rnn_units_decoder = 300\n",
        "dense_units = 300\n",
        "Dtype = tf.float32\n",
        "Tx = 500 #longst input\n",
        "Ty = 50 #longst output "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUyhGJrJrh7b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "de17f287-4c0a-4a73-c8ad-4e732f11e57d"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_tr, y_tr)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 300)\n",
            "(32, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUpgz4wtrtDd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6214333a-e299-4e4e-e1d8-1c6e37ca1378"
      },
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 300)\n",
            "(32, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgndaBd41PG8",
        "colab_type": "text"
      },
      "source": [
        "Using Pre-trained embedding models instead of keras Embedding \n",
        "Note that there are lots of possible ways to embed words to dense vectors of fixed langth. Keras Embedding modul provides this embedding as other layers-- i.e. minimize loss as its ultimate goal.\n",
        "However, word2vec tech is the one that trys to capture the SEMANTIC MEANINGS of words, not only simply densing them as minimizing the loss.\n",
        "Therefore, for text understanding related tasks, better using pre-trained word2vec models as embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0dm7Pgt9RnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#use pre-trained word embedding as Embedding.layer of the model\n",
        "#load pretrained word embedding\n",
        "\"\"\"\n",
        "corpus-specific word embedding can provide some more precise information, but \n",
        "it also has the disadvantage that many rare word get ignored.\n",
        "So, here where use a combined way to better represent the words:\n",
        "   1. if the word is in the corpus-specific word-embedding file, use it.\n",
        "   2. if the work is not in this file but in the pretrained embeddings from the\n",
        "   commonly crawled documents, then use this.\n",
        "   3. if a word neither in the specific file nor in the common file, initialize\n",
        "   it with random numbers\n",
        "\"\"\"\n",
        "cl_embeddings_index = {}\n",
        "with open('/content/drive/My Drive/MA_colab/CL_word2vec_300dim.txt') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        cl_embeddings_index[word] = coefs\n",
        "\n",
        "\n",
        "embeddings_index = {}\n",
        "with open('/content/drive/My Drive/MA_colab/glove.6B.300d.txt') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "\n",
        "#embedding_layer for Encoder\n",
        "embedding_matrix_encoder = np.zeros((x_voc_size,embedding_dims))\n",
        "\"\"\"\n",
        "if the word is in the word embedding dict, embedding_vector gets the corresponding value\n",
        "if not, initilize the vector with random numbers\n",
        "\"\"\"\n",
        "for word, i in x_word_index.items():\n",
        "  if word in cl_embeddings_index.keys():\n",
        "     embedding_vector = cl_embeddings_index.get(word)\n",
        "     embedding_matrix_encoder[i] = embedding_vector\n",
        "  elif word in embeddings_index.keys():\n",
        "     embedding_vector = embeddings_index.get(word)\n",
        "     embedding_matrix_encoder[i] = embedding_vector\n",
        "  else:\n",
        "     embedding_vector=np.random.uniform(-1.0,1.0,(300,))\n",
        "     embedding_matrix_encoder[i] = embedding_vector\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "embedding_layer = Embedding ()\n",
        "initilize the embedding layer:\n",
        "params: input_dim = the vocabulary size\n",
        "        output_dim = embedding dimension\n",
        "        embeddings_initilizer: to initilize the embeddings\n",
        "        input_length = the max length of the source text\n",
        "        trainable = False, because we want the pre-trained model and not update it\n",
        "\"\"\"\n",
        "embedding_layer_encoder = tf.keras.layers.Embedding(input_dim = x_voc_size, \n",
        "                                    output_dim = embedding_dims,\n",
        "                                    weights=[embedding_matrix_encoder], \n",
        "                                    input_length = max_len_text, \n",
        "                                    trainable = False)\n",
        "\n",
        "#embedding_layer for Decoder\n",
        "embedding_matrix_decoder = np.zeros((y_voc_size,embedding_dims))\n",
        "for word, i in y_word_index.items():\n",
        "   if word in cl_embeddings_index.keys():\n",
        "     embedding_vector_y = cl_embeddings_index.get(word)\n",
        "     embedding_matrix_decoder[i] = embedding_vector_y\n",
        "   elif word in embeddings_index.keys():\n",
        "     embedding_vector_y = embeddings_index.get(word)\n",
        "     embedding_matrix_decoder[i] = embedding_vector_y\n",
        "   else:\n",
        "     embedding_vector_y = np.random.uniform(-1.0,1.0,(300,))\n",
        "     embedding_matrix_decoder[i] = embedding_vector_y\n",
        "\n",
        "embedding_layer_decoder = tf.keras.layers.Embedding(input_dim = y_voc_size, \n",
        "                                    output_dim = embedding_dims,\n",
        "                                    weights=[embedding_matrix_decoder],\n",
        "                                    input_length = max_len_summary, \n",
        "                                    trainable = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE5XTFGtr3CH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the model\n",
        "#ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,x_voc_size,embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = embedding_layer_encoder\n",
        "\n",
        "        #Bi-LSTM\n",
        "        #not like LSTM, Bi-LSTM has forward and backword hidden state and content state\n",
        "        #encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
        "        self.encoder_birnnlayer = Bidirectional(LSTM(rnn_units,return_state=True,return_sequences=True))\n",
        "        \n",
        "      \n",
        "#DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,y_voc_size, embedding_dims, rnn_units_decoder):\n",
        "        super().__init__()\n",
        "        self.decoder_embedding = embedding_layer_decoder\n",
        "        self.dense_layer = Dense(y_voc_size)\n",
        "        self.decoder_rnncell = LSTMCell(rnn_units_decoder)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        # Create attention mechanism with memory = None\n",
        "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell = self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
        "                                                output_layer=self.dense_layer)\n",
        "\n",
        "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "        #return tfa.seq2seq.LuongAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "    # wrap decode-rnn cell  \n",
        "    def build_rnn_cell(self, batch_size ):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
        "                                                attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
        "                                                                dtype = Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "        return decoder_initial_state\n",
        "\n",
        "\n",
        "\n",
        "encoderNetwork = EncoderNetwork(x_voc_size,embedding_dims, rnn_units)\n",
        "decoderNetwork = DecoderNetwork(y_voc_size,embedding_dims, rnn_units_decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiyPryBQsLvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optimizing\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlhB9MlZsSGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loss function\n",
        "def loss_function(y_pred, y):\n",
        "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                                                  reduction='none')\n",
        "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
        "    mask = tf.logical_not(tf.math.equal(y,0))  \n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss = mask* loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrlD2omyscHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "906f0ec2-e939-47d0-d04f-8b8b5814db33"
      },
      "source": [
        "decoderNetwork.attention_mechanism.memory_initialized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0DhCkIhsfIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#batch training & teacher-forcing\n",
        "def train_step(input_batch, output_batch, encoder_initial_cell_state):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "\n",
        "\n",
        "        #Outputs of BiLstm\n",
        "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
        "        encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoderNetwork.encoder_birnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state =encoder_initial_cell_state)\n",
        "        state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
        "        state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
        "       \n",
        "\n",
        "        \n",
        "        # Prepare correct Decoder input & output sequence data\n",
        "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "        #compare logits with timestepped +1 version of decoder_input\n",
        "        decoder_output = output_batch[:,1:] #ignore <start>\n",
        "\n",
        "\n",
        "        # Decoder Embeddings\n",
        "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
        "        decoderNetwork.attention_mechanism.setup_memory(encoder_outputs)\n",
        "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                           encoder_state=[state_h, state_c],\n",
        "                                                                           Dtype=tf.float32)\n",
        "        \n",
        "        #BasicDecoderOutput        \n",
        "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
        "                                               sequence_length=BATCH_SIZE*[Ty-1])\n",
        "\n",
        "        logits = outputs.rnn_output\n",
        "        #Calculate loss\n",
        "        loss = loss_function(logits, decoder_output)\n",
        "\n",
        "    #Returns the list of all layer variables / weights.\n",
        "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
        "    # differentiate loss wrt variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    #grads_and_vars – List of(gradient, variable) pairs.\n",
        "    grads_and_vars = zip(gradients,variables)\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gg8GpwbsveE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53c86da9-7e62-42c9-f090-724369ca7253"
      },
      "source": [
        "#training\n",
        "chkpoint_prefix = os.path.join(default_path, \"chkpoint\")\n",
        "if not os.path.exists(default_path):\n",
        "    os.mkdir(default_path)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer = optimizer, encoderNetwork = encoderNetwork, \n",
        "                                 decoderNetwork = decoderNetwork)\n",
        "\n",
        "try:\n",
        "    status = checkpoint.restore(tf.train.latest_checkpoint(default_path))\n",
        "    print(\"Checkpoint found at {}\".format(tf.train.latest_checkpoint(default_path)))\n",
        "except:\n",
        "    print(\"No checkpoint found at {}\".format(default_path))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint found at /content/drive/My Drive/MA_colab/bilstm_attn_beam/ngram_short_sum/chkpoint-43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEKsbQ0HtXyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RNN LSTM hidden and memory state initializer\n",
        "def initialize_initial_state():\n",
        "    #for Bi-LSTM\n",
        "    return [tf.zeros((BATCH_SIZE, rnn_units)) for i in range(4)]\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uetAmVBStYcp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "0fced17a-b625-4676-ed85-ab61861e6972"
      },
      "source": [
        "epochs = 10\n",
        "loss=[]\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "    #print(encoder_initial_cell_state)\n",
        "\n",
        "    \n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%100 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))\n",
        "            checkpoint.save(file_prefix = chkpoint_prefix)\n",
        "            loss.append(batch_loss.numpy())\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 3.6922731399536133 epoch 1 batch 100 \n",
            "total loss: 4.326529026031494 epoch 1 batch 200 \n",
            "total loss: 3.997694253921509 epoch 1 batch 300 \n",
            "total loss: 3.7185609340667725 epoch 1 batch 400 \n",
            "total loss: 3.6400656700134277 epoch 2 batch 100 \n",
            "total loss: 3.448594093322754 epoch 2 batch 200 \n",
            "total loss: 3.311030387878418 epoch 2 batch 300 \n",
            "total loss: 3.4657535552978516 epoch 2 batch 400 \n",
            "total loss: 3.7535054683685303 epoch 3 batch 100 \n",
            "total loss: 3.0258047580718994 epoch 3 batch 200 \n",
            "total loss: 3.4167237281799316 epoch 3 batch 300 \n",
            "total loss: 2.7622244358062744 epoch 3 batch 400 \n",
            "total loss: 3.0006115436553955 epoch 4 batch 100 \n",
            "total loss: 3.012178421020508 epoch 4 batch 200 \n",
            "total loss: 2.963334083557129 epoch 4 batch 300 \n",
            "total loss: 2.7703914642333984 epoch 4 batch 400 \n",
            "total loss: 2.5760176181793213 epoch 5 batch 100 \n",
            "total loss: 3.0105180740356445 epoch 5 batch 200 \n",
            "total loss: 2.6435394287109375 epoch 5 batch 300 \n",
            "total loss: 3.0229554176330566 epoch 5 batch 400 \n",
            "total loss: 2.450146198272705 epoch 6 batch 100 \n",
            "total loss: 2.733328104019165 epoch 6 batch 200 \n",
            "total loss: 2.523451089859009 epoch 6 batch 300 \n",
            "total loss: 2.6303231716156006 epoch 6 batch 400 \n",
            "total loss: 2.3282439708709717 epoch 7 batch 100 \n",
            "total loss: 2.264543294906616 epoch 7 batch 200 \n",
            "total loss: 2.6115939617156982 epoch 7 batch 300 \n",
            "total loss: 2.5069220066070557 epoch 7 batch 400 \n",
            "total loss: 2.3773868083953857 epoch 8 batch 100 \n",
            "total loss: 2.5742368698120117 epoch 8 batch 200 \n",
            "total loss: 2.321983575820923 epoch 8 batch 300 \n",
            "total loss: 2.179777145385742 epoch 8 batch 400 \n",
            "total loss: 1.7797478437423706 epoch 9 batch 100 \n",
            "total loss: 2.207096576690674 epoch 9 batch 200 \n",
            "total loss: 2.2622761726379395 epoch 9 batch 300 \n",
            "total loss: 2.1902596950531006 epoch 9 batch 400 \n",
            "total loss: 2.26790714263916 epoch 10 batch 100 \n",
            "total loss: 1.9641309976577759 epoch 10 batch 200 \n",
            "total loss: 2.0626907348632812 epoch 10 batch 300 \n",
            "total loss: 2.4514946937561035 epoch 10 batch 400 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9o9YVhGwgHx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "90f7ed7d-1697-4600-b042-e497b2794bde"
      },
      "source": [
        "#validation\n",
        "epochs = 10\n",
        "loss_val=[]\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "    #print(encoder_initial_cell_state)\n",
        "\n",
        "    \n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset_test.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%10 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))\n",
        "            checkpoint.save(file_prefix = chkpoint_prefix)\n",
        "            loss_val.append(batch_loss.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 3.4276235103607178 epoch 1 batch 10 \n",
            "total loss: 3.355577230453491 epoch 1 batch 20 \n",
            "total loss: 3.2937350273132324 epoch 1 batch 30 \n",
            "total loss: 3.185814619064331 epoch 1 batch 40 \n",
            "total loss: 2.5561001300811768 epoch 2 batch 10 \n",
            "total loss: 2.40260648727417 epoch 2 batch 20 \n",
            "total loss: 2.6181697845458984 epoch 2 batch 30 \n",
            "total loss: 2.2206218242645264 epoch 2 batch 40 \n",
            "total loss: 2.1219592094421387 epoch 3 batch 10 \n",
            "total loss: 1.9752225875854492 epoch 3 batch 20 \n",
            "total loss: 2.1356313228607178 epoch 3 batch 30 \n",
            "total loss: 1.830239176750183 epoch 3 batch 40 \n",
            "total loss: 1.8237940073013306 epoch 4 batch 10 \n",
            "total loss: 1.8988370895385742 epoch 4 batch 20 \n",
            "total loss: 1.6262074708938599 epoch 4 batch 30 \n",
            "total loss: 1.9913426637649536 epoch 4 batch 40 \n",
            "total loss: 1.5929601192474365 epoch 5 batch 10 \n",
            "total loss: 1.3150267601013184 epoch 5 batch 20 \n",
            "total loss: 1.3114452362060547 epoch 5 batch 30 \n",
            "total loss: 1.4930400848388672 epoch 5 batch 40 \n",
            "total loss: 1.35736083984375 epoch 6 batch 10 \n",
            "total loss: 1.3477662801742554 epoch 6 batch 20 \n",
            "total loss: 1.4226958751678467 epoch 6 batch 30 \n",
            "total loss: 1.5367341041564941 epoch 6 batch 40 \n",
            "total loss: 1.0459578037261963 epoch 7 batch 10 \n",
            "total loss: 1.1994417905807495 epoch 7 batch 20 \n",
            "total loss: 1.2427089214324951 epoch 7 batch 30 \n",
            "total loss: 1.0569707155227661 epoch 7 batch 40 \n",
            "total loss: 0.9750339984893799 epoch 8 batch 10 \n",
            "total loss: 0.8219929337501526 epoch 8 batch 20 \n",
            "total loss: 1.0376287698745728 epoch 8 batch 30 \n",
            "total loss: 1.0950260162353516 epoch 8 batch 40 \n",
            "total loss: 0.9182642698287964 epoch 9 batch 10 \n",
            "total loss: 0.9045939445495605 epoch 9 batch 20 \n",
            "total loss: 0.9269730448722839 epoch 9 batch 30 \n",
            "total loss: 0.882957935333252 epoch 9 batch 40 \n",
            "total loss: 0.7010276317596436 epoch 10 batch 10 \n",
            "total loss: 0.7947072982788086 epoch 10 batch 20 \n",
            "total loss: 0.627821147441864 epoch 10 batch 30 \n",
            "total loss: 0.667460560798645 epoch 10 batch 40 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW-IAuAetcnD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8643cc70-5c78-446a-8fcb-b4fd5c2306c8"
      },
      "source": [
        "#inference\n",
        "#if trained in same session else use checkpoint variable\n",
        "#decoder_embedding_matrix = tf.train.load_variable(checkpointdir, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
        "print(decoderNetwork.decoder_embedding.variables[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20640, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAdxjl3ntqqd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3f379ff8-4f30-4b3e-a2e2-781adbdc24c8"
      },
      "source": [
        "[print(var) for var in tf.train.list_variables(\n",
        "    default_path) if re.match(r'.*decoder_embedding.*',var[0])]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE', [20640, 300])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8n1UlEFtuJx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "afa6bea0-5f48-4741-ba26-5ae71f355f3a"
      },
      "source": [
        "decoder_embedding_matrix = tf.train.load_variable(\n",
        "    default_path, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "print(decoder_embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20640, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpby4he_vSTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test=texts_tagged[15000:]\n",
        "x_test = x_tokenizer.texts_to_sequences(x_test)\n",
        "input_x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_len_text, padding='post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik6iBqYpMU4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "end_token = y_tokenizer.word_index['<end>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84TAvT78yu-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test (test_input):\n",
        "\n",
        "    beam_width = 8\n",
        "\n",
        "    inference_batch_size = test_input.shape[0]\n",
        "    inp = tf.convert_to_tensor(test_input)            \n",
        "    encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "\n",
        "    def initialize_initial_state_inference():\n",
        "        #for Bi-LSTM\n",
        "        return [tf.zeros((inference_batch_size, rnn_units)) for i in range(4)]\n",
        "\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoderNetwork.encoder_birnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state =initialize_initial_state_inference())\n",
        "    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
        "    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
        "\n",
        "    start_tokens = tf.fill([inference_batch_size],y_tokenizer.word_index['<start>'])\n",
        "    end_token = y_tokenizer.word_index['<end>']\n",
        "\n",
        "    decoder_input = tf.expand_dims([y_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
        "    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "    encoder_memory = tfa.seq2seq.tile_batch(encoder_outputs, beam_width)\n",
        "    decoderNetwork.attention_mechanism.setup_memory(encoder_memory)\n",
        "    print(\"beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] :\", encoder_memory.shape)\n",
        "    #set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
        "    decoder_initial_state = decoderNetwork.rnn_cell.get_initial_state(batch_size = inference_batch_size* beam_width,dtype = Dtype)\n",
        "    encoder_state = tfa.seq2seq.tile_batch([state_h, state_c], multiplier=beam_width)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "\n",
        "    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoderNetwork.rnn_cell,beam_width=beam_width,\n",
        "                                                 output_layer=decoderNetwork.dense_layer)\n",
        "\n",
        "\n",
        "    # Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "    # One heuristic is to decode up to two times the source sentence lengths.\n",
        "    maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "    #initialize inference decoder\n",
        "\n",
        "    (first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                                start_tokens = start_tokens,\n",
        "                                end_token=end_token,\n",
        "                                initial_state = decoder_initial_state)\n",
        "    #print( first_finished.shape)\n",
        "    #print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
        "\n",
        "    inputs = first_inputs\n",
        "    state = first_state  \n",
        "    predictions = np.empty((inference_batch_size, beam_width,0), dtype = np.int32)\n",
        "    beam_scores =  np.empty((inference_batch_size, beam_width,0), dtype = np.float32)                                                                            \n",
        "    for j in range(maximum_iterations):\n",
        "        beam_search_outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "        inputs = next_inputs\n",
        "        state = next_state\n",
        "        outputs = np.expand_dims(beam_search_outputs.predicted_ids,axis = -1)\n",
        "        scores = np.expand_dims(beam_search_outputs.scores,axis = -1)\n",
        "        predictions = np.append(predictions, outputs, axis = -1)\n",
        "        beam_scores = np.append(beam_scores, scores, axis = -1)\n",
        "    #print(predictions.shape) \n",
        "    #print(beam_scores.shape)\n",
        "    \n",
        "    return predictions, beam_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc8hjVUy39pA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "21f28e2f-8da9-4b81-cd0f-af38a814f1ef"
      },
      "source": [
        "#due to the limitattion of gpu memory, test is done in batchs\n",
        "test_batch = 32\n",
        "iters = len(x_test) // test_batch \n",
        "last_iter = iters+1\n",
        "all_predictions = []\n",
        "all_beam_scores = []\n",
        "for i in range(iters+1):\n",
        "    test_input = input_x_test[test_batch*i : test_batch*(i+1)]\n",
        "    predictions, beam_scores = test(test_input)\n",
        "    all_predictions.append(predictions)\n",
        "    all_beam_scores.append(beam_scores)\n",
        "    if i == last_iter:\n",
        "        test_input = input_x_test[test_batch*last_iter:]\n",
        "        predictions, beam_scores = test(test_input)\n",
        "        all_predictions.append(predictions)\n",
        "        all_beam_scores.append(beam_scores)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (256, 500, 300)\n",
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (136, 500, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdVEYjkf6yRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_sum = []\n",
        "for i in range (len(all_predictions)):\n",
        "    #select the predicted summary with highest beam score as the final result\n",
        "    predictions = all_predictions[i]\n",
        "    beam_scores = all_beam_scores[i]\n",
        "    for j in range(len(predictions)):\n",
        "        output_beams_per_sample = predictions[j,:,:]\n",
        "        score_beams_per_sample = beam_scores[j,:,:]\n",
        "        for beam, score in zip(output_beams_per_sample,score_beams_per_sample) :\n",
        "            seq = list(itertools.takewhile( lambda index: index !=end_token, beam))\n",
        "            score_indexes = np.arange(len(seq))\n",
        "            beam_score = score[score_indexes].sum()\n",
        "            predicted = sorted ({(\" \".join( [y_tokenizer.index_word[w] for w in seq])):beam_score}.keys(),reverse=True)[0]\n",
        "        predicted_sum.append(predicted)\n",
        "\n",
        "#store the results\n",
        "for i in range(len(predicted_sum)):\n",
        "    ref = open(default_path + \"ref/ref_{}.txt\".format(i), 'w')\n",
        "    ref.write(y_test[i])\n",
        "    ref.close()\n",
        "    summ = open(default_path + \"predicted/sum_{}.txt\".format(i), 'w')\n",
        "    summ.write(predicted_sum[i])\n",
        "    summ.close()\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xjRCCGasRrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "ab164fe5-58d8-4590-b8d0-b013ae602b87"
      },
      "source": [
        "#to see beam scores of a summary\n",
        "for i in range(len(predicted_sum[:5])):\n",
        "    print(\"-----------------\")\n",
        "    print(\"Original Summary:\")\n",
        "    print(y_test[i])\n",
        "    #print(\"-----------------\")\n",
        "    #print(\"\\nPredicted Summary:\")\n",
        "    #print(\"---------------------------------------------\")\n",
        "    output_beams_per_sample = predictions[i,:,:]\n",
        "    score_beams_per_sample = beam_scores[i,:,:]\n",
        "    for j,(beam, score) in enumerate(zip(output_beams_per_sample,score_beams_per_sample)) :\n",
        "        seq = list(itertools.takewhile( lambda index: index !=end_token, beam))\n",
        "        score_indexes = np.arange(len(seq))\n",
        "        beam_score = score[score_indexes].sum()\n",
        "        #print (\"version{}: \".format(j), (\" \".join( [y_tokenizer.index_word[w] for w in seq]), \" beam score: \", beam_score))\n",
        "        #print (\" \".join( [y_tokenizer.index_word[w] for w in seq]), \" beam score: \", beam_score)\n",
        "        predicted = sorted ({(\" \".join( [y_tokenizer.index_word[w] for w in seq])):beam_score}.keys(),reverse=True)[0]\n",
        "    print(\"---------------------------------------------\")\n",
        "    print(\"best summary: \" + predicted)\n",
        "    print('/n')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------\n",
            "Original Summary:\n",
            "this article focuses on symbolic preference . it presents the symbolic preference system of a hybrid parser that combines a shallow parser with an overlay parser that builds on the chunks . novel in using a simple , threevalued scoring method for assigning preferences to constituents viewed in the context of their containing constituents . problems associated with earlier preference systems , and has considerably facilitated development . it uses simple scoring . \n",
            "---------------------------------------------\n",
            "best summary: that article focuses on fire vocabulary . vocabulary an a method on speech recognition research phone speech speech and . .\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            "this article focuses on recognizing textual entailment . it presents a dependencybased paraphrasing algorithm , using the dirt data set , and its application in the context of a straightforward rte system based on aligning dependency trees . \n",
            "---------------------------------------------\n",
            "best summary: it article focuses on lexical translations metric\n",
            " machine a mncd , passage for different . a incorporation incorporation\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            "this article focuses on joint phrasal translation modeling . it presents a phrasal inversion transduction grammar as an alternative to joint phrasal translation models . similar to its flatstring phrasal predecessors , but admits polynomialtime algorithms for viterbi alignment and em training . , for the first time , the utility of a joint phrasal translation model as a word alignment method . \n",
            "---------------------------------------------\n",
            "best summary: it . focuses on high judgements and and . it presents two two effectiveness between two between agreement agreement and of agreement and agreement . agreement between agreement of agreement .\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            "this article focuses on geolinguistic change . it presents with the supply of 8 closely interpreted dialectometrical maps , this paper analyses the linguistic change of the geolinguistic deep structures in northern france between 1300 and 1900 . . \n",
            "---------------------------------------------\n",
            "best summary: it article focuses on auto word . - presents presents automated tool method korean . on a a new of of the scoring learning .\n",
            "/n\n",
            "-----------------\n",
            "Original Summary:\n",
            "this article focuses on recognising textual . it presents latent semantic analysis has only recently been applied to textual entailment recognition . . \n",
            "---------------------------------------------\n",
            "best summary: englishspanish englishspanish focuses . tunable wmt and shared presents university for to participation for translation task shared hifst , the hifst configuration\n",
            "/n\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}