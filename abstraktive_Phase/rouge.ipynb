{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rouge.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0f4MSotOfHVL7g7HjmIH3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yanh12/MA_automatische_Zusammenfassung/blob/master/abstraktive_Phase/rouge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz1aumIhlJjV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2zRg5zTlKEK",
        "colab_type": "code",
        "outputId": "661382be-73a4-4802-b822-49dc6ab6f34f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.setrecursionlimit(15000)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGgaq1qPB-ez",
        "colab_type": "code",
        "outputId": "b2de5b64-6393-4d55-95b4-249b7ad3e5ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.12.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPkkYOkRIJeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "default_path  = \"/content/drive/My Drive/MA_colab/rouge/\"\n",
        "import glob,os,re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlOlllv-4w2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ref1 = []\n",
        "ref2 = []\n",
        "h1 = []\n",
        "h2 = []\n",
        "h3 = []\n",
        "h4 = []\n",
        "with open (default_path + \"m2_ngram_short_ref.txt\", 'r')as f:\n",
        "    for x in f:\n",
        "        ref1.append(x)\n",
        "  \n",
        "with open (default_path + \"m2_ngram_ori_ref.txt\", 'r')as f:\n",
        "    for x in f:\n",
        "        ref2.append(x)\n",
        "\n",
        "with open (default_path + 'm2_ngram_short_hyp.txt', 'r') as f:\n",
        "    for x in f:\n",
        "        h1.append(x)\n",
        "\n",
        "with open (default_path + \"m2_tr_short_hyp.txt\", 'r')as f:\n",
        "    for x in f:\n",
        "        h2.append(x)\n",
        "\n",
        "with open (default_path + \"m2_ngram_ori_hyp.txt\", 'r')as f:\n",
        "    for x in f:\n",
        "        h3.append(x)\n",
        "\n",
        "with open (default_path + \"m2_tr_ori_hyp.txt\", 'r')as f:\n",
        "    for x in f:\n",
        "        h4.append(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W3rrX746RZe",
        "colab_type": "code",
        "outputId": "c8e022e6-1798-4739-b46b-11981e158fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "print(ref1[1172])\n",
        "print(h1[1172])\n",
        "print(h2[1172])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this article focuses on modeling dialogue applications . it presents their complexity . on methods and tools which support this development . capable of representing parallel dialogue steps which is e . g . necessary for mixed— initiative dialogues . editors for different dialogue concepts , such as dialogue structures , grammars and parameters . able to check for the completeness and consistency of dialogue models . to provide specification models which are universal enough to be interpreted within different dialogue systems , i . e . different implementations of generic conversational systems . . possible . \n",
            "\n",
            "accuracy article driven on distributed . . and a a new general a that large , of dialogue and supports supports dialogue that the use two a knowledge of that dialogue dialogue . of the dialogue structure .\n",
            "\n",
            "independent evaluating problem spoken interaction modeling . it presents the this a for system for for , system a show is a attributed design train , it\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOCZDDgb9FHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rouge import Rouge \n",
        "from rouge import FilesRouge\n",
        "files_rouge = FilesRouge()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j10RZqKr7rcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combine_txts (path1, path2, name1, name2):\n",
        "   \n",
        "    path_hyp = path1\n",
        "    path_ref = path2\n",
        "   \n",
        "    hyps = []\n",
        "    refs = []\n",
        "\n",
        "    files_hyp = os.listdir(path_hyp)\n",
        "\n",
        "    for file in files_hyp:\n",
        "        f=open(os.path.join(path_hyp,file),'r')\n",
        "        hyps.append(f.read())\n",
        "        f.close()\n",
        "\n",
        "    files_ref = os.listdir(path_ref)\n",
        "    for file in files_ref:\n",
        "        f=open(os.path.join(path_ref,file),'r')\n",
        "        refs.append(f.read())\n",
        "        f.close()\n",
        "\n",
        "    with open(default_path + name1, 'w') as f:\n",
        "        for x in hyps:\n",
        "            x = re.sub(r' +', ' ', x)\n",
        "            f.write(\"%s\\n\" % x.replace('\\n', ''))\n",
        "\n",
        "    with open(default_path + name2, 'w') as f:\n",
        "        for x in refs:\n",
        "            x = re.sub(r' +', ' ', x)\n",
        "            f.write(\"%s\\n\" % x.replace('\\n', ''))\n",
        "\n",
        "    return hyps,refs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybvaTI5JeY7j",
        "colab_type": "text"
      },
      "source": [
        "Following for LSTM-Attention-Model (Model1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR_Y6wBZ9xz7",
        "colab_type": "code",
        "outputId": "eded1acb-9d88-460d-e3a2-c6dc07e40042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "#for m1 ngram-short\n",
        "hyps_m1_ngram_short, refs_m1_ngram_short = combine_txts('/content/drive/My Drive/MA_colab/attn_lstm/ngram_short_sum/predicted',\n",
        "                                                    '/content/drive/My Drive/MA_colab/attn_lstm/ngram_short_sum/ref',\n",
        "                                                    'm1_ngram_short_hyp.txt',\n",
        "                                                    \"m1_ngram_short_ref.txt\")\n",
        "score_m1_ngram_short = files_rouge.get_scores(default_path + \"m1_ngram_short_hyp.txt\", default_path + \"m1_ngram_short_ref.txt\", avg=True)\n",
        "print(score_m1_ngram_short)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-84bbc5d4c464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                     \u001b[0;34m'/content/drive/My Drive/MA_colab/attn_lstm/ngram_short_sum/ref'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                     \u001b[0;34m'm1_ngram_short_hyp.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                     \"m1_ngram_short_ref.txt\")\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mscore_m1_ngram_short\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles_rouge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"m1_ngram_short_hyp.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"m1_ngram_short_ref.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_m1_ngram_short\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-fee4f92abf2a>\u001b[0m in \u001b[0;36mcombine_txts\u001b[0;34m(path1, path2, name1, name2)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfiles_hyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_hyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_hyp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: '/content/drive/My Drive/MA_colab/attn_lstm/ngram_short_sum/predicted'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVqSo2I49StC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for m1 ngram-ori\n",
        "hyps_m1_ngram_ori, refs_m1_ngram_ori = combine_txts('/content/drive/My Drive/MA_colab/attn_lstm/ngram_ori_sum/predicted',\n",
        "                                                    '/content/drive/My Drive/MA_colab/attn_lstm/ngram_ori_sum/ref',\n",
        "                                                    'm1_ngram_ori_hyp.txt',\n",
        "                                                    \"m1_ngram_ori_ref.txt\")\n",
        "scores_m1_ngram_ori = files_rouge.get_scores(default_path + \"m1_ngram_ori_hyp.txt\", default_path + \"m1_ngram_ori_ref.txt\", avg=True,ignore_empty=True)\n",
        "print(scores_m1_ngram_ori)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocQaQ2PV9TRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for m1 textrank-short\n",
        "hyps_m1_tr_short, refs_m1_tr_short = combine_txts('/content/drive/My Drive/MA_colab/attn_lstm/tr_short_sum/predicted',\n",
        "                                                    '/content/drive/My Drive/MA_colab/attn_lstm/tr_short_sum/ref',\n",
        "                                                    'm1_tr_short_hyp.txt',\n",
        "                                                    \"m1_tr_short_ref.txt\")\n",
        "scores_m1_tr_short = files_rouge.get_scores(default_path + \"m1_tr_short_hyp.txt\", default_path + \"m1_tr_short_ref.txt\", avg=True, ignore_empty=True)\n",
        "print(scores_m1_tr_short)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRJR02-q01m1",
        "colab_type": "code",
        "outputId": "ef135914-2e24-4b2d-896e-fbabf4be9f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#for m1 textrank-ori\n",
        "hyps_m1_tr_ori, refs_m1_tr_ori = combine_txts('/content/drive/My Drive/MA_colab/attn_lstm/tr_ori_sum/predicted',\n",
        "                                              '/content/drive/My Drive/MA_colab/attn_lstm/tr_ori_sum/ref',\n",
        "                                               'm1_tr_ori_hyp.txt',\n",
        "                                               \"m1_tr_ori_ref.txt\")\n",
        "scores_m1_tr_ori = files_rouge.get_scores(default_path + \"m1_tr_ori_hyp.txt\", default_path + \"m1_tr_ori_ref.txt\", avg=True,ignore_empty=True)\n",
        "print(scores_m1_tr_ori)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'f': 0.2672972972482211, 'p': 0.3328416815612318, 'r': 0.22949347278843724}, 'rouge-2': {'f': 0.04204985867339327, 'p': 0.05226476916845628, 'r': 0.03623691654897323}, 'rouge-l': {'f': 0.23392280489136927, 'p': 0.3518397680570589, 'r': 0.17924062339429234}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8H0diHmeHAm",
        "colab_type": "text"
      },
      "source": [
        "# Follwing for BiLSTM-Attention-BeamSearch-Model (Model2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzkB8TML35MO",
        "colab_type": "code",
        "outputId": "a87346d3-773e-4eeb-c922-88f39ff6585a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#for m2 ngram-short\n",
        "hyps_m2_ngram_short, refs_m2_ngram_short = combine_txts('/content/drive/My Drive/MA_colab/bilstm_attn_beam/ngram_short_sum/predicted',\n",
        "                                                        '/content/drive/My Drive/MA_colab/bilstm_attn_beam/ngram_short_sum/ref',\n",
        "                                                        'm2_ngram_short_hyp.txt',\n",
        "                                                        \"m2_ngram_short_ref.txt\")\n",
        "scores_m2_ngram_short = files_rouge.get_scores(default_path + \"m2_ngram_short_hyp.txt\", default_path + \"m2_ngram_short_ref.txt\", avg=True, ignore_empty=True)\n",
        "print(scores_m2_ngram_short)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'f': 0.27170364801438207, 'p': 0.38788975089913874, 'r': 0.23049962081919417}, 'rouge-2': {'f': 0.0458582469247518, 'p': 0.06510018618869894, 'r': 0.03938565567910326}, 'rouge-l': {'f': 0.26859216494963645, 'p': 0.39941792403339527, 'r': 0.21506000851622942}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIQ0hd419h1z",
        "colab_type": "code",
        "outputId": "d6705b26-359f-4fdd-ead9-7b7975462f08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "#for m2 ngram-ori\n",
        "hyps_m2_ngram_ori, refs_m2_ngram_ori = combine_txts('/content/drive/My Drive/MA_colab/bilstm_attn_beam/ngram_ori_sum/predicted',\n",
        "                                                    '/content/drive/My Drive/MA_colab/bilstm_attn_beam/ngram_ori_sum/ref',\n",
        "                                                    'm2_ngram_ori_hyp.txt',\n",
        "                                                    \"m2_ngram_ori_ref.txt\")\n",
        "scores_m2_ngram_ori = files_rouge.get_scores(default_path + \"m2_ngram_ori_hyp.txt\", default_path + \"m2_ngram_ori_ref.txt\", avg=True, ignore_empty=True)\n",
        "print(scores_m2_ngram_ori)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'f': 0.235445289002414, 'p': 0.4255253660116271, 'r': 0.171243177389026}, 'rouge-2': {'f': 0.014659265955685521, 'p': 0.02719094012777289, 'r': 0.010599657815452293}, 'rouge-l': {'f': 0.19226577233396108, 'p': 0.3524879956178261, 'r': 0.13702162566935092}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X5Qx6Bb9jwO",
        "colab_type": "code",
        "outputId": "d49e108b-0ea9-4757-ce43-b5f78092a35a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#for m2 textrank-short\n",
        "hyps_m2_tr_short, refs_m2_tr_short = combine_txts('/content/drive/My Drive/MA_colab/bilstm_attn_beam/tr_short_sum/predicted',\n",
        "                                                    '/content/drive/My Drive/MA_colab/bilstm_attn_beam/tr_short_sum/ref',\n",
        "                                                    'm2_tr_short_hyp.txt',\n",
        "                                                    \"m2_tr_short_ref.txt\")\n",
        "scores_m2_tr_short = files_rouge.get_scores(default_path + \"m2_tr_short_hyp.txt\", default_path + \"m2_tr_short_ref.txt\", avg=True,ignore_empty=True)\n",
        "print(scores_m2_tr_short)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'f': 0.21621872892317678, 'p': 0.2837723237773137, 'r': 0.19541144522884443}, 'rouge-2': {'f': 0.021809715392128854, 'p': 0.02800847297115889, 'r': 0.020123659537627646}, 'rouge-l': {'f': 0.21116723073847946, 'p': 0.2841500834727915, 'r': 0.18142580732502575}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7e-_5Cjd7Rw",
        "colab_type": "text"
      },
      "source": [
        "## Following for Pointer-Generator-Model (Model 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHmjYbEA8rN6",
        "colab_type": "code",
        "outputId": "97cf05c6-2e50-4bad-a470-f428dc859d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#for m3 textrank-ori\n",
        "\n",
        "hyps_m3_tr, refs_m3_tr = combine_txts('/content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode_test_500maxenc_8beam_75mindec_100maxdec_ckpt-11945test/decoded',\n",
        "                                            '/content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode_test_500maxenc_8beam_75mindec_100maxdec_ckpt-11945test/reference',\n",
        "                                            'm3_tr_hyp.txt',\n",
        "                                            \"m3_tr_ref.txt\")\n",
        "scores_m3_tr = files_rouge.get_scores(default_path + \"m3_tr_hyp.txt\", default_path + \"m3_tr_ref.txt\", avg=True,ignore_empty=True)\n",
        "print(scores_m3_tr)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'f': 0.24481077198362883, 'p': 0.26119001267561254, 'r': 0.2482061046119939}, 'rouge-2': {'f': 0.051827515361860366, 'p': 0.05540202034458516, 'r': 0.05265622900994997}, 'rouge-l': {'f': 0.19663296570657562, 'p': 0.39784429448521497, 'r': 0.13681121057605233}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlndmbLZFWUm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6f9a2157-902c-4be3-8a8d-bd6c8d24ca89"
      },
      "source": [
        "#for m3 textrank-short\n",
        "\n",
        "hyps_m3_2, refs_m3_2 = combine_txts('/content/drive/My Drive/MA_colab/PG_TR_short/logs/myexperiment/decode_test_500maxenc_8beam_25mindec_50maxdec_ckpt-16337test/decoded',\n",
        "                                            '/content/drive/My Drive/MA_colab/PG_TR_short/logs/myexperiment/decode_test_500maxenc_8beam_25mindec_50maxdec_ckpt-16337test/reference',\n",
        "                                            'm3_tr_short_hyp.txt',\n",
        "                                            \"m3_tr_short_ref.txt\")\n",
        "scores_m3_2 = files_rouge.get_scores(default_path + \"m3_tr_short_hyp.txt\", default_path + \"m3_tr_short_ref.txt\", avg=True,ignore_empty=True)\n",
        "print(scores_m3_2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'f': 0.32647975398759277, 'p': 0.3983260892472081, 'r': 0.304027635407075}, 'rouge-2': {'f': 0.14612929727754548, 'p': 0.17513286926664012, 'r': 0.1394390910461702}, 'rouge-l': {'f': 0.3352356789591373, 'p': 0.4476380040769328, 'r': 0.28883648951224866}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1lgY2phGUIs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "edefa1df-c766-4f45-cec3-d8689ca9b5ef"
      },
      "source": [
        "#for m3 ngran-ori\n",
        "\n",
        "hyps_m3_3, refs_m3_3 = combine_txts('/content/drive/My Drive/MA_colab/PG_ngram/logs/myexperiment/decode_test_300maxenc_8beam_75mindec_100maxdec_ckpt-13989test/decoded',\n",
        "                                            '/content/drive/My Drive/MA_colab/PG_ngram/logs/myexperiment/decode_test_300maxenc_8beam_75mindec_100maxdec_ckpt-13989test/reference',\n",
        "                                            'm3_ngram_hyp.txt',\n",
        "                                            \"m3_ngram_ref.txt\")\n",
        "scores_m3_3 = files_rouge.get_scores(default_path + \"m3_ngram_hyp.txt\", default_path + \"m3_ngram_ref.txt\", avg=True,ignore_empty=True)\n",
        "print(scores_m3_3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'f': 0.35494829485999135, 'p': 0.3833896319652876, 'r': 0.35310570144912934}, 'rouge-2': {'f': 0.12693405777367064, 'p': 0.13677649736593384, 'r': 0.1267068614115404}, 'rouge-l': {'f': 0.28924822240396275, 'p': 0.4468554791962442, 'r': 0.22616881187887794}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPR3VHP_EpQW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ee4fa6d7-d3b4-4a15-ad8e-694811e6bd74"
      },
      "source": [
        "#for m3 ngran-short\n",
        "\n",
        "hyps_m3_4, refs_m3_4 = combine_txts('/content/drive/My Drive/MA_colab/PG_Ngram_short/logs/myexperiment/decode_test_300maxenc_8beam_25mindec_50maxdec_ckpt-50508test/decoded',\n",
        "                                            '/content/drive/My Drive/MA_colab/PG_Ngram_short/logs/myexperiment/decode_test_300maxenc_8beam_25mindec_50maxdec_ckpt-50508test /reference',\n",
        "                                            'm3_ngram_short_hyp.txt',\n",
        "                                            \"m3_ngram_short_ref.txt\")\n",
        "scores_m3_4 = files_rouge.get_scores(default_path + \"m3_ngram_short_hyp.txt\", default_path + \"m3_ngram_short_ref.txt\", avg=True,ignore_empty=True)\n",
        "print(scores_m3_4)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'f': 0.41861802355185257, 'p': 0.4809551317043826, 'r': 0.40639601760633676}, 'rouge-2': {'f': 0.22681007802993244, 'p': 0.25526162865954577, 'r': 0.22470154989021865}, 'rouge-l': {'f': 0.4042628392749752, 'p': 0.4842944637304879, 'r': 0.3731680052588847}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWPz7BEC119G",
        "colab_type": "text"
      },
      "source": [
        "#####following only for testing an example#######"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkVdZ0AZCCOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for model1 Ngram-short-sum version\n",
        "path_hyp = '/content/drive/My Drive/MA_colab/attn_lstm/ngram_short_sum/predicted'\n",
        "path_ref = '/content/drive/My Drive/MA_colab/attn_lstm/ngram_short_sum/ref'\n",
        "refs = []\n",
        "hyps = []\n",
        "\n",
        "files = os.listdir(path_hyp)\n",
        "\n",
        "for file in files:\n",
        "    f=open(os.path.join(path_hyp,file),'r')\n",
        "    hyps.append(f.read())\n",
        "    f.close()\n",
        "\n",
        "files = os.listdir(path_ref)\n",
        "for file in files:\n",
        "    f=open(os.path.join(path_ref,file),'r')\n",
        "    refs.append(f.read())\n",
        "    f.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJNwrtFiHScC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ori_ref_path =  '/content/drive/My Drive/MA_colab/attn_lstm/ngram_ori_sum/ref'\n",
        "ori_refs = []\n",
        "files = os.listdir(ori_ref_path)\n",
        "for file in files:\n",
        "    f=open(os.path.join(ori_ref_path,file),'r')\n",
        "    ori_refs.append(f.read())\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHlWdBBcHpnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(default_path + \"ori_ref.txt\", 'w') as f:\n",
        "    for x in ori_refs:\n",
        "        f.write(\"%s\\n\" % x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpiH5D1IQa3T",
        "colab_type": "code",
        "outputId": "9caba38d-10e8-41f9-d360-057da8b9964e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(hyps[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " this article focuses on semantic similarity . it presents a novel approach to semantic similarity based on a large scale distributional similarity measure . it uses distributional semantic models .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt2hQvHlHDp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(default_path + \"m1_ngram_short_hyp.txt\", 'w') as f:\n",
        "    for x in hyps:\n",
        "        f.write(\"%s\\n\" % x)\n",
        "\n",
        "with open(default_path + \"m1_ngram_short_ref.txt\", 'w') as f:\n",
        "    for x in refs:\n",
        "        f.write(\"%s\\n\" % x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL0gMkXDKmTs",
        "colab_type": "code",
        "outputId": "f416370e-98f9-4d92-c416-adcae24e790d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(refs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ibT7x9LFFIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rouge import Rouge \n",
        "from rouge import FilesRouge\n",
        "files_rouge = FilesRouge()\n",
        "scores = files_rouge.get_scores(default_path + \"m1_ngram_short_hyp.txt\", default_path + \"m1_ngram_short_ref.txt\", avg=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B27qR2TxPXaD",
        "colab_type": "code",
        "outputId": "4ef3a6d5-896c-4bc0-8c17-334ea5d65b1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'f': 0.27194531452744963, 'p': 0.34532158507673716, 'r': 0.2392973765722533}, 'rouge-2': {'f': 0.12456279211099185, 'p': 0.1535861823678082, 'r': 0.11328600589933698}, 'rouge-l': {'f': 0.29827859854467403, 'p': 0.3910419745808104, 'r': 0.25243893759816144}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1UUKRt7yhBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}