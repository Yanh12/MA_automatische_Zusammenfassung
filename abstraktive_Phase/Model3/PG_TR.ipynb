{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " PG-TR(final).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSZ2Ya6jnkwci+GRs7mvUJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yanh12/MA_automatische_Zusammenfassung/blob/master/abstraktive_Phase/Model3/PG_TR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryUPLlBjSzVC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0949ca9-4e4d-4466-b526-168ab2ba4287"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/MA_colab')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpQGb70ktcIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "default_path = '/content/drive/My Drive/MA_colab/PG_TR/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0oyjotkO1K4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "eec91f17-4762-4310-a9c7-197f7e3c57c5"
      },
      "source": [
        "#make it compatible to run tf v1 codes\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ6oFW223SE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DATA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTjtQR3b3p3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"This file contains code to read the train/eval/test data from file and process it, and read the vocab data from file and process it\"\"\"\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import struct\n",
        "import csv\n",
        "from tensorflow.core.example import example_pb2\n",
        "\n",
        "# <s> and </s> are used in the data files to segment the abstracts into sentences. They don't receive vocab ids.\n",
        "SENTENCE_START = '<s>'\n",
        "SENTENCE_END = '</s>'\n",
        "\n",
        "PAD_TOKEN = '[PAD]' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
        "UNKNOWN_TOKEN = '[UNK]' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
        "START_DECODING = '[START]' # This has a vocab id, which is used at the start of every decoder input sequence\n",
        "STOP_DECODING = '[STOP]' # This has a vocab id, which is used at the end of untruncated target sequences\n",
        "\n",
        "# Note: none of <s>, </s>, [PAD], [UNK], [START], [STOP] should appear in the vocab file.\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "  \"\"\"Vocabulary class for mapping between words and ids (integers)\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_file, max_size):\n",
        "    \"\"\"Creates a vocab of up to max_size words, reading from the vocab_file. If max_size is 0, reads the entire vocab file.\n",
        "    Args:\n",
        "      vocab_file: path to the vocab file, which is assumed to contain \"<word> <frequency>\" on each line, sorted with most frequent word first. This code doesn't actually use the frequencies, though.\n",
        "      max_size: integer. The maximum size of the resulting Vocabulary.\"\"\"\n",
        "    self._word_to_id = {}\n",
        "    self._id_to_word = {}\n",
        "    self._count = 0 # keeps track of total number of words in the Vocab\n",
        "\n",
        "    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
        "    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
        "      self._word_to_id[w] = self._count\n",
        "      self._id_to_word[self._count] = w\n",
        "      self._count += 1\n",
        "\n",
        "    # Read the vocab file and add words up to max_size\n",
        "    with open(vocab_file, 'r') as vocab_f:\n",
        "      for line in vocab_f:\n",
        "        pieces = line.split()\n",
        "        if len(pieces) != 2:\n",
        "          print ('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n",
        "          continue\n",
        "        w = pieces[0]\n",
        "        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
        "          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
        "        if w in self._word_to_id:\n",
        "          raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
        "        self._word_to_id[w] = self._count\n",
        "        self._id_to_word[self._count] = w\n",
        "        self._count += 1\n",
        "        if max_size != 0 and self._count >= max_size:\n",
        "          print (\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
        "          break\n",
        "\n",
        "    print (\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
        "\n",
        "  def word2id(self, word):\n",
        "    \"\"\"Returns the id (integer) of a word (string). Returns [UNK] id if word is OOV.\"\"\"\n",
        "    if word not in self._word_to_id:\n",
        "      return self._word_to_id[UNKNOWN_TOKEN]\n",
        "    return self._word_to_id[word]\n",
        "\n",
        "  def id2word(self, word_id):\n",
        "    \"\"\"Returns the word (string) corresponding to an id (integer).\"\"\"\n",
        "    if word_id not in self._id_to_word:\n",
        "      raise ValueError('Id not found in vocab: %d' % word_id)\n",
        "    return self._id_to_word[word_id]\n",
        "\n",
        "  def size(self):\n",
        "    \"\"\"Returns the total size of the vocabulary\"\"\"\n",
        "    return self._count\n",
        "\n",
        "  def write_metadata(self, fpath):\n",
        "    \"\"\"Writes metadata file for Tensorboard word embedding visualizer as described here:\n",
        "      https://www.tensorflow.org/get_started/embedding_viz\n",
        "    Args:\n",
        "      fpath: place to write the metadata file\n",
        "    \"\"\"\n",
        "    print (\"Writing word embedding metadata file to %s...\" % (fpath))\n",
        "    with open(fpath, \"w\") as f:\n",
        "      fieldnames = ['word']\n",
        "      writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
        "      for i in range(self.size()):\n",
        "        writer.writerow({\"word\": self._id_to_word[i]})\n",
        "\n",
        "\n",
        "def example_generator(data_path, single_pass):\n",
        "  \"\"\"Generates tf.Examples from data files.\n",
        "    Binary data format: <length><blob>. <length> represents the byte size\n",
        "    of <blob>. <blob> is serialized tf.Example proto. The tf.Example contains\n",
        "    the tokenized article text and summary.\n",
        "  Args:\n",
        "    data_path:\n",
        "      Path to tf.Example data files. Can include wildcards, e.g. if you have several training data chunk files train_001.bin, train_002.bin, etc, then pass data_path=train_* to access them all.\n",
        "    single_pass:\n",
        "      Boolean. If True, go through the dataset exactly once, generating examples in the order they appear, then return. Otherwise, generate random examples indefinitely.\n",
        "  Yields:\n",
        "    Deserialized tf.Example.\n",
        "  \"\"\"\n",
        "  while True:\n",
        "    filelist = glob.glob(data_path) # get the list of datafiles\n",
        "    assert filelist, ('Error: Empty filelist at %s' % data_path) # check filelist isn't empty\n",
        "    if single_pass:\n",
        "      filelist = sorted(filelist)\n",
        "    else:\n",
        "      random.shuffle(filelist)\n",
        "    for f in filelist:\n",
        "      reader = open(f, 'rb')\n",
        "      while True:\n",
        "        len_bytes = reader.read(8)\n",
        "        if not len_bytes: break # finished reading this file\n",
        "        str_len = struct.unpack('q', len_bytes)[0]\n",
        "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
        "        yield example_pb2.Example.FromString(example_str)\n",
        "    if single_pass:\n",
        "      print (\"example_generator completed reading all datafiles. No more data.\")\n",
        "      break\n",
        "\n",
        "\n",
        "def article2ids(article_words, vocab):\n",
        "  \"\"\"Map the article words to their ids. Also return a list of OOVs in the article.\n",
        "  Args:\n",
        "    article_words: list of words (strings)\n",
        "    vocab: Vocabulary object\n",
        "  Returns:\n",
        "    ids:\n",
        "      A list of word ids (integers); OOVs are represented by their temporary article OOV number. If the vocabulary size is 50k and the article has 3 OOVs, then these temporary OOV numbers will be 50000, 50001, 50002.\n",
        "    oovs:\n",
        "      A list of the OOV words in the article (strings), in the order corresponding to their temporary article OOV numbers.\"\"\"\n",
        "  ids = []\n",
        "  oovs = []\n",
        "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
        "  for w in article_words:\n",
        "    i = vocab.word2id(w)\n",
        "    if i == unk_id: # If w is OOV\n",
        "      if w not in oovs: # Add to list of OOVs\n",
        "        oovs.append(w)\n",
        "      oov_num = oovs.index(w) # This is 0 for the first article OOV, 1 for the second article OOV...\n",
        "      ids.append(vocab.size() + oov_num) # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
        "    else:\n",
        "      ids.append(i)\n",
        "  return ids, oovs\n",
        "\n",
        "\n",
        "def abstract2ids(abstract_words, vocab, article_oovs):\n",
        "  \"\"\"Map the abstract words to their ids. In-article OOVs are mapped to their temporary OOV numbers.\n",
        "  Args:\n",
        "    abstract_words: list of words (strings)\n",
        "    vocab: Vocabulary object\n",
        "    article_oovs: list of in-article OOV words (strings), in the order corresponding to their temporary article OOV numbers\n",
        "  Returns:\n",
        "    ids: List of ids (integers). In-article OOV words are mapped to their temporary OOV numbers. Out-of-article OOV words are mapped to the UNK token id.\"\"\"\n",
        "  ids = []\n",
        "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
        "  for w in abstract_words:\n",
        "    i = vocab.word2id(w)\n",
        "    if i == unk_id: # If w is an OOV word\n",
        "      if w in article_oovs: # If w is an in-article OOV\n",
        "        vocab_idx = vocab.size() + article_oovs.index(w) # Map to its temporary article OOV number\n",
        "        ids.append(vocab_idx)\n",
        "      else: # If w is an out-of-article OOV\n",
        "        ids.append(unk_id) # Map to the UNK token id\n",
        "    else:\n",
        "      ids.append(i)\n",
        "  return ids\n",
        "\n",
        "\n",
        "def outputids2words(id_list, vocab, article_oovs):\n",
        "  \"\"\"Maps output ids to words, including mapping in-article OOVs from their temporary ids to the original OOV string (applicable in pointer-generator mode).\n",
        "  Args:\n",
        "    id_list: list of ids (integers)\n",
        "    vocab: Vocabulary object\n",
        "    article_oovs: list of OOV words (strings) in the order corresponding to their temporary article OOV ids (that have been assigned in pointer-generator mode), or None (in baseline mode)\n",
        "  Returns:\n",
        "    words: list of words (strings)\n",
        "  \"\"\"\n",
        "  words = []\n",
        "  for i in id_list:\n",
        "    try:\n",
        "      w = vocab.id2word(i) # might be [UNK]\n",
        "    except ValueError as e: # w is OOV\n",
        "      assert article_oovs is not None, \"Error: model produced a word ID that isn't in the vocabulary. This should not happen in baseline (no pointer-generator) mode\"\n",
        "      article_oov_idx = i - vocab.size()\n",
        "      try:\n",
        "        w = article_oovs[article_oov_idx]\n",
        "      except ValueError as e: # i doesn't correspond to an article oov\n",
        "        raise ValueError('Error: model produced word ID %i which corresponds to article OOV %i but this example only has %i article OOVs' % (i, article_oov_idx, len(article_oovs)))\n",
        "    words.append(w)\n",
        "  return words\n",
        "\n",
        "\n",
        "def abstract2sents(abstract):\n",
        "  \"\"\"Splits abstract text from datafile into list of sentences.\n",
        "  Args:\n",
        "    abstract: string containing <s> and </s> tags for starts and ends of sentences\n",
        "  Returns:\n",
        "    sents: List of sentence strings (no tags)\"\"\"\n",
        "  cur = 0\n",
        "  sents = []\n",
        "  while True:\n",
        "    try:\n",
        "      start_p = abstract.index(SENTENCE_START, cur)\n",
        "      end_p = abstract.index(SENTENCE_END, start_p + 1)\n",
        "      cur = end_p + len(SENTENCE_END)\n",
        "      sents.append(abstract[start_p+len(SENTENCE_START):end_p])\n",
        "    except ValueError as e: # no more sentences\n",
        "      return sents\n",
        "\n",
        "\n",
        "def show_art_oovs(article, vocab):\n",
        "  \"\"\"Returns the article string, highlighting the OOVs by placing __underscores__ around them\"\"\"\n",
        "  unk_token = vocab.word2id(UNKNOWN_TOKEN)\n",
        "  words = article.split(' ')\n",
        "  words = [(\"__%s__\" % w) if vocab.word2id(w)==unk_token else w for w in words]\n",
        "  out_str = ' '.join(words)\n",
        "  return out_str\n",
        "\n",
        "\n",
        "def show_abs_oovs(abstract, vocab, article_oovs):\n",
        "  \"\"\"Returns the abstract string, highlighting the article OOVs with __underscores__.\n",
        "  If a list of article_oovs is provided, non-article OOVs are differentiated like !!__this__!!.\n",
        "  Args:\n",
        "    abstract: string\n",
        "    vocab: Vocabulary object\n",
        "    article_oovs: list of words (strings), or None (in baseline mode)\n",
        "  \"\"\"\n",
        "  unk_token = vocab.word2id(UNKNOWN_TOKEN)\n",
        "  words = abstract.split(' ')\n",
        "  new_words = []\n",
        "  for w in words:\n",
        "    if vocab.word2id(w) == unk_token: # w is oov\n",
        "      if article_oovs is None: # baseline mode\n",
        "        new_words.append(\"__%s__\" % w)\n",
        "      else: # pointer-generator mode\n",
        "        if w in article_oovs:\n",
        "          new_words.append(\"__%s__\" % w)\n",
        "        else:\n",
        "          new_words.append(\"!!__%s__!!\" % w)\n",
        "    else: # w is in-vocab word\n",
        "      new_words.append(w)\n",
        "  out_str = ' '.join(new_words)\n",
        "  return out_str\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov6XVvGS3xCI",
        "colab_type": "text"
      },
      "source": [
        "#BATCH\n",
        "\"\"\"This file contains code to process data into batches\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_huyNBN3vt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import queue as Queue\n",
        "from random import shuffle\n",
        "from threading import Thread\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class Example(object):\n",
        "  \"\"\"Class representing a train/val/test example for text summarization.\"\"\"\n",
        "\n",
        "  def __init__(self, article, abstract_sentences, vocab, hps):\n",
        "    \"\"\"Initializes the Example, performing tokenization and truncation to produce the encoder, decoder and target sequences, which are stored in self.\n",
        "    Args:\n",
        "      article: source text; a string. each token is separated by a single space.\n",
        "      abstract_sentences: list of strings, one per abstract sentence. In each sentence, each token is separated by a single space.\n",
        "      vocab: Vocabulary object\n",
        "      hps: hyperparameters\n",
        "    \"\"\"\n",
        "    self.hps = hps\n",
        "\n",
        "    # Get ids of special tokens\n",
        "    start_decoding = vocab.word2id(START_DECODING)\n",
        "    stop_decoding = vocab.word2id(STOP_DECODING)\n",
        "\n",
        "    # Process the article\n",
        "    article_words = article.split()\n",
        "    if len(article_words) > hps.max_enc_steps:\n",
        "      article_words = article_words[:hps.max_enc_steps]\n",
        "    self.enc_len = len(article_words) # store the length after truncation but before padding\n",
        "    self.enc_input = [vocab.word2id(w) for w in article_words] # list of word ids; OOVs are represented by the id for UNK token\n",
        "\n",
        "    # Process the abstract\n",
        "    abstract = ' '.join(abstract_sentences) # string\n",
        "    abstract_words = abstract.split() # list of strings\n",
        "    abs_ids = [vocab.word2id(w) for w in abstract_words] # list of word ids; OOVs are represented by the id for UNK token\n",
        "\n",
        "    # Get the decoder input sequence and target sequence\n",
        "    self.dec_input, self.target = self.get_dec_inp_targ_seqs(abs_ids, hps.max_dec_steps, start_decoding, stop_decoding)\n",
        "    self.dec_len = len(self.dec_input)\n",
        "\n",
        "    # If using pointer-generator mode, we need to store some extra info\n",
        "    if hps.pointer_gen:\n",
        "      # Store a version of the enc_input where in-article OOVs are represented by their temporary OOV id; also store the in-article OOVs words themselves\n",
        "      self.enc_input_extend_vocab, self.article_oovs = article2ids(article_words, vocab)\n",
        "\n",
        "      # Get a verison of the reference summary where in-article OOVs are represented by their temporary article OOV id\n",
        "      abs_ids_extend_vocab = abstract2ids(abstract_words, vocab, self.article_oovs)\n",
        "\n",
        "      # Overwrite decoder target sequence so it uses the temp article OOV ids\n",
        "      _, self.target = self.get_dec_inp_targ_seqs(abs_ids_extend_vocab, hps.max_dec_steps, start_decoding, stop_decoding)\n",
        "\n",
        "    # Store the original strings\n",
        "    self.original_article = article\n",
        "    self.original_abstract = abstract\n",
        "    self.original_abstract_sents = abstract_sentences\n",
        "\n",
        "\n",
        "  def get_dec_inp_targ_seqs(self, sequence, max_len, start_id, stop_id):\n",
        "    \"\"\"Given the reference summary as a sequence of tokens, return the input sequence for the decoder, and the target sequence which we will use to calculate loss. The sequence will be truncated if it is longer than max_len. The input sequence must start with the start_id and the target sequence must end with the stop_id (but not if it's been truncated).\n",
        "    Args:\n",
        "      sequence: List of ids (integers)\n",
        "      max_len: integer\n",
        "      start_id: integer\n",
        "      stop_id: integer\n",
        "    Returns:\n",
        "      inp: sequence length <=max_len starting with start_id\n",
        "      target: sequence same length as input, ending with stop_id only if there was no truncation\n",
        "    \"\"\"\n",
        "    inp = [start_id] + sequence[:]\n",
        "    target = sequence[:]\n",
        "    if len(inp) > max_len: # truncate\n",
        "      inp = inp[:max_len]\n",
        "      target = target[:max_len] # no end_token\n",
        "    else: # no truncation\n",
        "      target.append(stop_id) # end token\n",
        "    assert len(inp) == len(target)\n",
        "    return inp, target\n",
        "\n",
        "\n",
        "  def pad_decoder_inp_targ(self, max_len, pad_id):\n",
        "    \"\"\"Pad decoder input and target sequences with pad_id up to max_len.\"\"\"\n",
        "    while len(self.dec_input) < max_len:\n",
        "      self.dec_input.append(pad_id)\n",
        "    while len(self.target) < max_len:\n",
        "      self.target.append(pad_id)\n",
        "\n",
        "\n",
        "  def pad_encoder_input(self, max_len, pad_id):\n",
        "    \"\"\"Pad the encoder input sequence with pad_id up to max_len.\"\"\"\n",
        "    while len(self.enc_input) < max_len:\n",
        "      self.enc_input.append(pad_id)\n",
        "    if self.hps.pointer_gen:\n",
        "      while len(self.enc_input_extend_vocab) < max_len:\n",
        "        self.enc_input_extend_vocab.append(pad_id)\n",
        "\n",
        "\n",
        "class Batch(object):\n",
        "  \"\"\"Class representing a minibatch of train/val/test examples for text summarization.\"\"\"\n",
        "\n",
        "  def __init__(self, example_list, hps, vocab):\n",
        "    \"\"\"Turns the example_list into a Batch object.\n",
        "    Args:\n",
        "       example_list: List of Example objects\n",
        "       hps: hyperparameters\n",
        "       vocab: Vocabulary object\n",
        "    \"\"\"\n",
        "    self.pad_id = vocab.word2id(PAD_TOKEN) # id of the PAD token used to pad sequences\n",
        "    self.init_encoder_seq(example_list, hps) # initialize the input to the encoder\n",
        "    self.init_decoder_seq(example_list, hps) # initialize the input and targets for the decoder\n",
        "    self.store_orig_strings(example_list) # store the original strings\n",
        "\n",
        "  def init_encoder_seq(self, example_list, hps):\n",
        "    \"\"\"Initializes the following:\n",
        "        self.enc_batch:\n",
        "          numpy array of shape (batch_size, <=max_enc_steps) containing integer ids (all OOVs represented by UNK id), padded to length of longest sequence in the batch\n",
        "        self.enc_lens:\n",
        "          numpy array of shape (batch_size) containing integers. The (truncated) length of each encoder input sequence (pre-padding).\n",
        "        self.enc_padding_mask:\n",
        "          numpy array of shape (batch_size, <=max_enc_steps), containing 1s and 0s. 1s correspond to real tokens in enc_batch and target_batch; 0s correspond to padding.\n",
        "      If hps.pointer_gen, additionally initializes the following:\n",
        "        self.max_art_oovs:\n",
        "          maximum number of in-article OOVs in the batch\n",
        "        self.art_oovs:\n",
        "          list of list of in-article OOVs (strings), for each example in the batch\n",
        "        self.enc_batch_extend_vocab:\n",
        "          Same as self.enc_batch, but in-article OOVs are represented by their temporary article OOV number.\n",
        "    \"\"\"\n",
        "    # Determine the maximum length of the encoder input sequence in this batch\n",
        "    max_enc_seq_len = max([ex.enc_len for ex in example_list])\n",
        "\n",
        "    # Pad the encoder input sequences up to the length of the longest sequence\n",
        "    for ex in example_list:\n",
        "      ex.pad_encoder_input(max_enc_seq_len, self.pad_id)\n",
        "\n",
        "    # Initialize the numpy arrays\n",
        "    # Note: our enc_batch can have different length (second dimension) for each batch because we use dynamic_rnn for the encoder.\n",
        "    self.enc_batch = np.zeros((hps.batch_size, max_enc_seq_len), dtype=np.int32)\n",
        "    self.enc_lens = np.zeros((hps.batch_size), dtype=np.int32)\n",
        "    self.enc_padding_mask = np.zeros((hps.batch_size, max_enc_seq_len), dtype=np.float32)\n",
        "\n",
        "    # Fill in the numpy arrays\n",
        "    for i, ex in enumerate(example_list):\n",
        "      self.enc_batch[i, :] = ex.enc_input[:]\n",
        "      self.enc_lens[i] = ex.enc_len\n",
        "      for j in range(ex.enc_len):\n",
        "        self.enc_padding_mask[i][j] = 1\n",
        "\n",
        "    # For pointer-generator mode, need to store some extra info\n",
        "    if hps.pointer_gen:\n",
        "      # Determine the max number of in-article OOVs in this batch\n",
        "      self.max_art_oovs = max([len(ex.article_oovs) for ex in example_list])\n",
        "      # Store the in-article OOVs themselves\n",
        "      self.art_oovs = [ex.article_oovs for ex in example_list]\n",
        "      # Store the version of the enc_batch that uses the article OOV ids\n",
        "      self.enc_batch_extend_vocab = np.zeros((hps.batch_size, max_enc_seq_len), dtype=np.int32)\n",
        "      for i, ex in enumerate(example_list):\n",
        "        self.enc_batch_extend_vocab[i, :] = ex.enc_input_extend_vocab[:]\n",
        "\n",
        "  def init_decoder_seq(self, example_list, hps):\n",
        "    \"\"\"Initializes the following:\n",
        "        self.dec_batch:\n",
        "          numpy array of shape (batch_size, max_dec_steps), containing integer ids as input for the decoder, padded to max_dec_steps length.\n",
        "        self.target_batch:\n",
        "          numpy array of shape (batch_size, max_dec_steps), containing integer ids for the target sequence, padded to max_dec_steps length.\n",
        "        self.dec_padding_mask:\n",
        "          numpy array of shape (batch_size, max_dec_steps), containing 1s and 0s. 1s correspond to real tokens in dec_batch and target_batch; 0s correspond to padding.\n",
        "        \"\"\"\n",
        "    # Pad the inputs and targets\n",
        "    for ex in example_list:\n",
        "      ex.pad_decoder_inp_targ(hps.max_dec_steps, self.pad_id)\n",
        "\n",
        "    # Initialize the numpy arrays.\n",
        "    # Note: our decoder inputs and targets must be the same length for each batch (second dimension = max_dec_steps) because we do not use a dynamic_rnn for decoding. However I believe this is possible, or will soon be possible, with Tensorflow 1.0, in which case it may be best to upgrade to that.\n",
        "    self.dec_batch = np.zeros((hps.batch_size, hps.max_dec_steps), dtype=np.int32)\n",
        "    self.target_batch = np.zeros((hps.batch_size, hps.max_dec_steps), dtype=np.int32)\n",
        "    self.dec_padding_mask = np.zeros((hps.batch_size, hps.max_dec_steps), dtype=np.float32)\n",
        "\n",
        "    # Fill in the numpy arrays\n",
        "    for i, ex in enumerate(example_list):\n",
        "      self.dec_batch[i, :] = ex.dec_input[:]\n",
        "      self.target_batch[i, :] = ex.target[:]\n",
        "      for j in range(ex.dec_len):\n",
        "        self.dec_padding_mask[i][j] = 1\n",
        "\n",
        "  def store_orig_strings(self, example_list):\n",
        "    \"\"\"Store the original article and abstract strings in the Batch object\"\"\"\n",
        "    self.original_articles = [ex.original_article for ex in example_list] # list of lists\n",
        "    self.original_abstracts = [ex.original_abstract for ex in example_list] # list of lists\n",
        "    self.original_abstracts_sents = [ex.original_abstract_sents for ex in example_list] # list of list of lists\n",
        "\n",
        "\n",
        "class Batcher(object):\n",
        "  \"\"\"A class to generate minibatches of data. Buckets examples together based on length of the encoder sequence.\"\"\"\n",
        "\n",
        "  BATCH_QUEUE_MAX = 100 # max number of batches the batch_queue can hold\n",
        "\n",
        "  def __init__(self, data_path, vocab, hps, single_pass):\n",
        "    \"\"\"Initialize the batcher. Start threads that process the data into batches.\n",
        "    Args:\n",
        "      data_path: tf.Example filepattern.\n",
        "      vocab: Vocabulary object\n",
        "      hps: hyperparameters\n",
        "      single_pass: If True, run through the dataset exactly once (useful for when you want to run evaluation on the dev or test set). Otherwise generate random batches indefinitely (useful for training).\n",
        "    \"\"\"\n",
        "    self._data_path = data_path\n",
        "    self._vocab = vocab\n",
        "    self._hps = hps\n",
        "    self._single_pass = single_pass\n",
        "\n",
        "    # Initialize a queue of Batches waiting to be used, and a queue of Examples waiting to be batched\n",
        "    self._batch_queue = Queue.Queue(self.BATCH_QUEUE_MAX)\n",
        "    self._example_queue = Queue.Queue(self.BATCH_QUEUE_MAX * self._hps.batch_size)\n",
        "\n",
        "    # Different settings depending on whether we're in single_pass mode or not\n",
        "    if single_pass:\n",
        "      self._num_example_q_threads = 1 # just one thread, so we read through the dataset just once\n",
        "      self._num_batch_q_threads = 1  # just one thread to batch examples\n",
        "      self._bucketing_cache_size = 1 # only load one batch's worth of examples before bucketing; this essentially means no bucketing\n",
        "      self._finished_reading = False # this will tell us when we're finished reading the dataset\n",
        "    else:\n",
        "      self._num_example_q_threads = 16 # num threads to fill example queue\n",
        "      self._num_batch_q_threads = 4  # num threads to fill batch queue\n",
        "      self._bucketing_cache_size = 100 # how many batches-worth of examples to load into cache before bucketing\n",
        "\n",
        "    # Start the threads that load the queues\n",
        "    self._example_q_threads = []\n",
        "    for _ in range(self._num_example_q_threads):\n",
        "      self._example_q_threads.append(Thread(target=self.fill_example_queue))\n",
        "      self._example_q_threads[-1].daemon = True\n",
        "      self._example_q_threads[-1].start()\n",
        "    self._batch_q_threads = []\n",
        "    for _ in range(self._num_batch_q_threads):\n",
        "      self._batch_q_threads.append(Thread(target=self.fill_batch_queue))\n",
        "      self._batch_q_threads[-1].daemon = True\n",
        "      self._batch_q_threads[-1].start()\n",
        "\n",
        "    # Start a thread that watches the other threads and restarts them if they're dead\n",
        "    if not single_pass: # We don't want a watcher in single_pass mode because the threads shouldn't run forever\n",
        "      self._watch_thread = Thread(target=self.watch_threads)\n",
        "      self._watch_thread.daemon = True\n",
        "      self._watch_thread.start()\n",
        "\n",
        "\n",
        "  def next_batch(self):\n",
        "    \"\"\"Return a Batch from the batch queue.\n",
        "    If mode='decode' then each batch contains a single example repeated beam_size-many times; this is necessary for beam search.\n",
        "    Returns:\n",
        "      batch: a Batch object, or None if we're in single_pass mode and we've exhausted the dataset.\n",
        "    \"\"\"\n",
        "    # If the batch queue is empty, print a warning\n",
        "    if self._batch_queue.qsize() == 0:\n",
        "      tf.compat.v1.logging.warning('Bucket input queue is empty when calling next_batch. Bucket queue size: %i, Input queue size: %i', self._batch_queue.qsize(), self._example_queue.qsize())\n",
        "      if self._single_pass and self._finished_reading:\n",
        "        tf.compat.v1.logging.info(\"Finished reading dataset in single_pass mode.\")\n",
        "        return None\n",
        "\n",
        "    batch = self._batch_queue.get() # get the next Batch\n",
        "    return batch\n",
        "\n",
        "  def fill_example_queue(self):\n",
        "    \"\"\"Reads data from file and processes into Examples which are then placed into the example queue.\"\"\"\n",
        "\n",
        "    input_gen = self.text_generator(example_generator(self._data_path, self._single_pass))\n",
        "\n",
        "    while True:\n",
        "      try:\n",
        "        (article, abstract) = next(input_gen) # read the next example from file. article and abstract are both strings.\n",
        "        article = article.decode(\"utf-8\")\n",
        "        abstract = abstract.decode(\"utf-8\")\n",
        "      except StopIteration: # if there are no more examples:\n",
        "        tf.compat.v1.logging.info(\"The example generator for this example queue filling thread has exhausted data.\")\n",
        "        if self._single_pass:\n",
        "          tf.compat.v1.logging.info(\"single_pass mode is on, so we've finished reading dataset. This thread is stopping.\")\n",
        "          self._finished_reading = True\n",
        "          break\n",
        "        else:\n",
        "          raise Exception(\"single_pass mode is off but the example generator is out of data; error.\")\n",
        "\n",
        "      abstract_sentences = [sent.strip() for sent in abstract2sents(abstract)] # Use the <s> and </s> tags in abstract to get a list of sentences.\n",
        "      example = Example(article, abstract_sentences, self._vocab, self._hps) # Process into an Example.\n",
        "      self._example_queue.put(example) # place the Example in the example queue.\n",
        "\n",
        "\n",
        "  def fill_batch_queue(self):\n",
        "    \"\"\"Takes Examples out of example queue, sorts them by encoder sequence length, processes into Batches and places them in the batch queue.\n",
        "    In decode mode, makes batches that each contain a single example repeated.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "      if self._hps.mode != 'decode':\n",
        "        # Get bucketing_cache_size-many batches of Examples into a list, then sort\n",
        "        inputs = []\n",
        "        for _ in range(self._hps.batch_size * self._bucketing_cache_size):\n",
        "          inputs.append(self._example_queue.get())\n",
        "        inputs = sorted(inputs, key=lambda inp: inp.enc_len) # sort by length of encoder sequence\n",
        "\n",
        "        # Group the sorted Examples into batches, optionally shuffle the batches, and place in the batch queue.\n",
        "        batches = []\n",
        "        for i in range(0, len(inputs), self._hps.batch_size):\n",
        "          batches.append(inputs[i:i + self._hps.batch_size])\n",
        "        if not self._single_pass:\n",
        "          shuffle(batches)\n",
        "        for b in batches:  # each b is a list of Example objects\n",
        "          self._batch_queue.put(Batch(b, self._hps, self._vocab))\n",
        "\n",
        "      else: # beam search decode mode\n",
        "        ex = self._example_queue.get()\n",
        "        b = [ex for _ in range(self._hps.batch_size)]\n",
        "        self._batch_queue.put(Batch(b, self._hps, self._vocab))\n",
        "\n",
        "\n",
        "  def watch_threads(self):\n",
        "    \"\"\"Watch example queue and batch queue threads and restart if dead.\"\"\"\n",
        "    while True:\n",
        "      time.sleep(60)\n",
        "      for idx,t in enumerate(self._example_q_threads):\n",
        "        if not t.is_alive(): # if the thread is dead\n",
        "          tf.compat.v1.logging.error('Found example queue thread dead. Restarting.')\n",
        "          new_t = Thread(target=self.fill_example_queue)\n",
        "          self._example_q_threads[idx] = new_t\n",
        "          new_t.daemon = True\n",
        "          new_t.start()\n",
        "      for idx,t in enumerate(self._batch_q_threads):\n",
        "        if not t.is_alive(): # if the thread is dead\n",
        "          tf.compat.v1.logging.error('Found batch queue thread dead. Restarting.')\n",
        "          new_t = Thread(target=self.fill_batch_queue)\n",
        "          self._batch_q_threads[idx] = new_t\n",
        "          new_t.daemon = True\n",
        "          new_t.start()\n",
        "\n",
        "\n",
        "  def text_generator(self, example_generator):\n",
        "    \"\"\"Generates article and abstract text from tf.Example.\n",
        "    Args:\n",
        "      example_generator: a generator of tf.Examples from file. See data.example_generator\"\"\"\n",
        "    while True:\n",
        "      e = next(example_generator) # e is a tf.Example\n",
        "      try:\n",
        "        article_text = e.features.feature['article'].bytes_list.value[0] # the article text was saved under the key 'article' in the data files\n",
        "        abstract_text = e.features.feature['abstract'].bytes_list.value[0] # the abstract text was saved under the key 'abstract' in the data files\n",
        "      except ValueError:\n",
        "        tf.compat.v1.logging.error('Failed to get article or abstract from example')\n",
        "        continue\n",
        "      if len(article_text)==0: # See https://github.com/abisee/pointer-generator/issues/1\n",
        "        tf.compat.v1.logging.warning('Found an example with empty article text. Skipping it.')\n",
        "      else:\n",
        "        yield (article_text, abstract_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7GuKa7Ggpze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWX7Hoq437qH",
        "colab_type": "text"
      },
      "source": [
        "#Attention Decoder#\n",
        "\"\"\"This file defines the decoder\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhkYvbsr3_RX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import variable_scope\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import nn_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "# Note: this function is based on tf.contrib.legacy_seq2seq_attention_decoder, which is now outdated.\n",
        "# In the future, it would make more sense to write variants on the attention mechanism using the new seq2seq library for tensorflow 1.0: https://www.tensorflow.org/api_guides/python/contrib.seq2seq#Attention\n",
        "def attention_decoder(decoder_inputs, initial_state, encoder_states, enc_padding_mask, cell, initial_state_attention=False, pointer_gen=True, use_coverage=False, prev_coverage=None):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n",
        "    initial_state: 2D Tensor [batch_size x cell.state_size].\n",
        "    encoder_states: 3D Tensor [batch_size x attn_length x attn_size].\n",
        "    enc_padding_mask: 2D Tensor [batch_size x attn_length] containing 1s and 0s; indicates which of the encoder locations are padding (0) or a real token (1).\n",
        "    cell: rnn_cell.RNNCell defining the cell function and size.\n",
        "    initial_state_attention:\n",
        "      Note that this attention decoder passes each decoder input through a linear layer with the previous step's context vector to get a modified version of the input. If initial_state_attention is False, on the first decoder step the \"previous context vector\" is just a zero vector. If initial_state_attention is True, we use initial_state to (re)calculate the previous step's context vector. We set this to False for train/eval mode (because we call attention_decoder once for all decoder steps) and True for decode mode (because we call attention_decoder once for each decoder step).\n",
        "    pointer_gen: boolean. If True, calculate the generation probability p_gen for each decoder step.\n",
        "    use_coverage: boolean. If True, use coverage mechanism.\n",
        "    prev_coverage:\n",
        "      If not None, a tensor with shape (batch_size, attn_length). The previous step's coverage vector. This is only not None in decode mode when using coverage.\n",
        "  Returns:\n",
        "    outputs: A list of the same length as decoder_inputs of 2D Tensors of\n",
        "      shape [batch_size x cell.output_size]. The output vectors.\n",
        "    state: The final state of the decoder. A tensor shape [batch_size x cell.state_size].\n",
        "    attn_dists: A list containing tensors of shape (batch_size,attn_length).\n",
        "      The attention distributions for each decoder step.\n",
        "    p_gens: List of length input_size, containing tensors of shape [batch_size, 1]. The values of p_gen for each decoder step. Empty list if pointer_gen=False.\n",
        "    coverage: Coverage vector on the last step computed. None if use_coverage=False.\n",
        "  \"\"\"\n",
        "  with variable_scope.variable_scope(\"attention_decoder\") as scope:\n",
        "    batch_size = encoder_states.get_shape()[0].value # if this line fails, it's because the batch size isn't defined\n",
        "    attn_size = encoder_states.get_shape()[2].value # if this line fails, it's because the attention length isn't defined\n",
        "\n",
        "    # Reshape encoder_states (need to insert a dim)\n",
        "    encoder_states = tf.expand_dims(encoder_states, axis=2) # now is shape (batch_size, attn_len, 1, attn_size)\n",
        "\n",
        "    # To calculate attention, we calculate\n",
        "    #   v^T tanh(W_h h_i + W_s s_t + b_attn)\n",
        "    # where h_i is an encoder state, and s_t a decoder state.\n",
        "    # attn_vec_size is the length of the vectors v, b_attn, (W_h h_i) and (W_s s_t).\n",
        "    # We set it to be equal to the size of the encoder states.\n",
        "    attention_vec_size = attn_size\n",
        "\n",
        "    # Get the weight matrix W_h and apply it to each encoder state to get (W_h h_i), the encoder features\n",
        "    W_h = variable_scope.get_variable(\"W_h\", [1, 1, attn_size, attention_vec_size])\n",
        "    encoder_features = nn_ops.conv2d(encoder_states, W_h, [1, 1, 1, 1], \"SAME\") # shape (batch_size,attn_length,1,attention_vec_size)\n",
        "\n",
        "    # Get the weight vectors v and w_c (w_c is for coverage)\n",
        "    v = variable_scope.get_variable(\"v\", [attention_vec_size])\n",
        "    if use_coverage:\n",
        "      with variable_scope.variable_scope(\"coverage\"):\n",
        "        w_c = variable_scope.get_variable(\"w_c\", [1, 1, 1, attention_vec_size])\n",
        "\n",
        "    if prev_coverage is not None: # for beam search mode with coverage\n",
        "      # reshape from (batch_size, attn_length) to (batch_size, attn_len, 1, 1)\n",
        "      prev_coverage = tf.expand_dims(tf.expand_dims(prev_coverage,2),3)\n",
        "\n",
        "    def attention(decoder_state, coverage=None):\n",
        "      \"\"\"Calculate the context vector and attention distribution from the decoder state.\n",
        "      Args:\n",
        "        decoder_state: state of the decoder\n",
        "        coverage: Optional. Previous timestep's coverage vector, shape (batch_size, attn_len, 1, 1).\n",
        "      Returns:\n",
        "        context_vector: weighted sum of encoder_states\n",
        "        attn_dist: attention distribution\n",
        "        coverage: new coverage vector. shape (batch_size, attn_len, 1, 1)\n",
        "      \"\"\"\n",
        "      with variable_scope.variable_scope(\"Attention\"):\n",
        "        # Pass the decoder state through a linear layer (this is W_s s_t + b_attn in the paper)\n",
        "        decoder_features = linear(decoder_state, attention_vec_size, True) # shape (batch_size, attention_vec_size)\n",
        "        decoder_features = tf.expand_dims(tf.expand_dims(decoder_features, 1), 1) # reshape to (batch_size, 1, 1, attention_vec_size)\n",
        "\n",
        "        def masked_attention(e):\n",
        "          \"\"\"Take softmax of e then apply enc_padding_mask and re-normalize\"\"\"\n",
        "          attn_dist = nn_ops.softmax(e) # take softmax. shape (batch_size, attn_length)\n",
        "          attn_dist *= enc_padding_mask # apply mask\n",
        "          masked_sums = tf.reduce_sum(attn_dist, axis=1) # shape (batch_size)\n",
        "          return attn_dist / tf.reshape(masked_sums, [-1, 1]) # re-normalize\n",
        "\n",
        "        if use_coverage and coverage is not None: # non-first step of coverage\n",
        "          # Multiply coverage vector by w_c to get coverage_features.\n",
        "          coverage_features = nn_ops.conv2d(coverage, w_c, [1, 1, 1, 1], \"SAME\") # c has shape (batch_size, attn_length, 1, attention_vec_size)\n",
        "\n",
        "          # Calculate v^T tanh(W_h h_i + W_s s_t + w_c c_i^t + b_attn)\n",
        "          e = math_ops.reduce_sum(v * math_ops.tanh(encoder_features + decoder_features + coverage_features), [2, 3])  # shape (batch_size,attn_length)\n",
        "\n",
        "          # Calculate attention distribution\n",
        "          attn_dist = masked_attention(e)\n",
        "\n",
        "          # Update coverage vector\n",
        "          coverage += array_ops.reshape(attn_dist, [batch_size, -1, 1, 1])\n",
        "        else:\n",
        "          # Calculate v^T tanh(W_h h_i + W_s s_t + b_attn)\n",
        "          e = math_ops.reduce_sum(v * math_ops.tanh(encoder_features + decoder_features), [2, 3]) # calculate e\n",
        "\n",
        "          # Calculate attention distribution\n",
        "          attn_dist = masked_attention(e)\n",
        "\n",
        "          if use_coverage: # first step of training\n",
        "            coverage = tf.expand_dims(tf.expand_dims(attn_dist,2),2) # initialize coverage\n",
        "\n",
        "        # Calculate the context vector from attn_dist and encoder_states\n",
        "        context_vector = math_ops.reduce_sum(array_ops.reshape(attn_dist, [batch_size, -1, 1, 1]) * encoder_states, [1, 2]) # shape (batch_size, attn_size).\n",
        "        context_vector = array_ops.reshape(context_vector, [-1, attn_size])\n",
        "\n",
        "      return context_vector, attn_dist, coverage\n",
        "\n",
        "    outputs = []\n",
        "    attn_dists = []\n",
        "    p_gens = []\n",
        "    state = initial_state\n",
        "    coverage = prev_coverage # initialize coverage to None or whatever was passed in\n",
        "    context_vector = array_ops.zeros([batch_size, attn_size])\n",
        "    context_vector.set_shape([None, attn_size])  # Ensure the second shape of attention vectors is set.\n",
        "    if initial_state_attention: # true in decode mode\n",
        "      # Re-calculate the context vector from the previous step so that we can pass it through a linear layer with this step's input to get a modified version of the input\n",
        "      context_vector, _, coverage = attention(initial_state, coverage) # in decode mode, this is what updates the coverage vector\n",
        "    for i, inp in enumerate(decoder_inputs):\n",
        "      tf.compat.v1.logging.info(\"Adding attention_decoder timestep %i of %i\", i, len(decoder_inputs))\n",
        "      if i > 0:\n",
        "        variable_scope.get_variable_scope().reuse_variables()\n",
        "\n",
        "      # Merge input and previous attentions into one vector x of the same size as inp\n",
        "      input_size = inp.get_shape().with_rank(2)[1]\n",
        "      if input_size.value is None:\n",
        "        raise ValueError(\"Could not infer input size from input: %s\" % inp.name)\n",
        "      x = linear([inp] + [context_vector], input_size, True)\n",
        "\n",
        "      # Run the decoder RNN cell. cell_output = decoder state\n",
        "      cell_output, state = cell(x, state)\n",
        "\n",
        "      # Run the attention mechanism.\n",
        "      if i == 0 and initial_state_attention:  # always true in decode mode\n",
        "        with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=True): # you need this because you've already run the initial attention(...) call\n",
        "          context_vector, attn_dist, _ = attention(state, coverage) # don't allow coverage to update\n",
        "      else:\n",
        "        context_vector, attn_dist, coverage = attention(state, coverage)\n",
        "      attn_dists.append(attn_dist)\n",
        "\n",
        "      # Calculate p_gen\n",
        "      if pointer_gen:\n",
        "        with tf.compat.v1.variable_scope('calculate_pgen'):\n",
        "          p_gen = linear([context_vector, state.c, state.h, x], 1, True) # Tensor shape (batch_size, 1)\n",
        "          p_gen = tf.sigmoid(p_gen)\n",
        "          p_gens.append(p_gen)\n",
        "\n",
        "      # Concatenate the cell_output (= decoder state) and the context vector, and pass them through a linear layer\n",
        "      # This is V[s_t, h*_t] + b in the paper\n",
        "      with variable_scope.variable_scope(\"AttnOutputProjection\"):\n",
        "        output = linear([cell_output] + [context_vector], cell.output_size, True)\n",
        "      outputs.append(output)\n",
        "\n",
        "    # If using coverage, reshape it\n",
        "    if coverage is not None:\n",
        "      coverage = array_ops.reshape(coverage, [batch_size, -1])\n",
        "\n",
        "    return outputs, state, attn_dists, p_gens, coverage\n",
        "\n",
        "\n",
        "\n",
        "def linear(args, output_size, bias, bias_start=0.0, scope=None):\n",
        "  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n",
        "  Args:\n",
        "    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n",
        "    output_size: int, second dimension of W[i].\n",
        "    bias: boolean, whether to add a bias term or not.\n",
        "    bias_start: starting value to initialize the bias; 0 by default.\n",
        "    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
        "  Returns:\n",
        "    A 2D Tensor with shape [batch x output_size] equal to\n",
        "    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
        "  Raises:\n",
        "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
        "  \"\"\"\n",
        "  if args is None or (isinstance(args, (list, tuple)) and not args):\n",
        "    raise ValueError(\"`args` must be specified\")\n",
        "  if not isinstance(args, (list, tuple)):\n",
        "    args = [args]\n",
        "\n",
        "  # Calculate the total size of arguments on dimension 1.\n",
        "  total_arg_size = 0\n",
        "  shapes = [a.get_shape().as_list() for a in args]\n",
        "  for shape in shapes:\n",
        "    if len(shape) != 2:\n",
        "      raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n",
        "    if not shape[1]:\n",
        "      raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n",
        "    else:\n",
        "      total_arg_size += shape[1]\n",
        "\n",
        "  # Now the computation.\n",
        "  with tf.compat.v1.variable_scope(scope or \"Linear\"):\n",
        "    matrix = tf.compat.v1.get_variable(\"Matrix\", [total_arg_size, output_size])\n",
        "    if len(args) == 1:\n",
        "      res = tf.matmul(args[0], matrix)\n",
        "    else:\n",
        "      res = tf.matmul(tf.concat(axis=1, values=args), matrix)\n",
        "    if not bias:\n",
        "      return res\n",
        "    bias_term = tf.compat.v1.get_variable(\n",
        "        \"Bias\", [output_size], initializer=tf.constant_initializer(bias_start))\n",
        "  return res + bias_term\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taK21fvD4Eqr",
        "colab_type": "text"
      },
      "source": [
        "#MODEL#\n",
        "\"\"\"This file contains code to build and run the tensorflow graph for the sequence-to-sequence model\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYxhjeE34DuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorboard.plugins import projector\n",
        "\n",
        "FLAGS = tf.compat.v1.flags.Flag\n",
        "\n",
        "class SummarizationModel(object):\n",
        "  \"\"\"A class to represent a sequence-to-sequence model for text summarization. Supports both baseline mode, pointer-generator mode, and coverage\"\"\"\n",
        "\n",
        "  def __init__(self, hps, vocab):\n",
        "    self._hps = hps\n",
        "    self._vocab = vocab\n",
        "\n",
        "  def _add_placeholders(self):\n",
        "    \"\"\"Add placeholders to the graph. These are entry points for any input data.\"\"\"\n",
        "    hps = self._hps\n",
        "\n",
        "    # encoder part\n",
        "    self._enc_batch = tf.compat.v1.placeholder(tf.int32, [hps.batch_size, None], name='enc_batch')\n",
        "    self._enc_lens = tf.compat.v1.placeholder(tf.int32, [hps.batch_size], name='enc_lens')\n",
        "    self._enc_padding_mask = tf.compat.v1.placeholder(tf.float32, [hps.batch_size, None], name='enc_padding_mask')\n",
        "    if FLAGS.pointer_gen:\n",
        "      self._enc_batch_extend_vocab = tf.compat.v1.placeholder(tf.int32, [hps.batch_size, None], name='enc_batch_extend_vocab')\n",
        "      self._max_art_oovs = tf.compat.v1.placeholder(tf.int32, [], name='max_art_oovs')\n",
        "\n",
        "    # decoder part\n",
        "    self._dec_batch = tf.compat.v1.placeholder(tf.int32, [hps.batch_size, hps.max_dec_steps], name='dec_batch')\n",
        "    self._target_batch = tf.compat.v1.placeholder(tf.int32, [hps.batch_size, hps.max_dec_steps], name='target_batch')\n",
        "    self._dec_padding_mask = tf.compat.v1.placeholder(tf.float32, [hps.batch_size, hps.max_dec_steps], name='dec_padding_mask')\n",
        "\n",
        "    if hps.mode==\"decode\" and hps.coverage:\n",
        "      self.prev_coverage = tf.compat.v1.placeholder(tf.float32, [hps.batch_size, None], name='prev_coverage')\n",
        "\n",
        "\n",
        "  def _make_feed_dict(self, batch, just_enc=False):\n",
        "    \"\"\"Make a feed dictionary mapping parts of the batch to the appropriate placeholders.\n",
        "    Args:\n",
        "      batch: Batch object\n",
        "      just_enc: Boolean. If True, only feed the parts needed for the encoder.\n",
        "    \"\"\"\n",
        "    feed_dict = {}\n",
        "    feed_dict[self._enc_batch] = batch.enc_batch\n",
        "    feed_dict[self._enc_lens] = batch.enc_lens\n",
        "    feed_dict[self._enc_padding_mask] = batch.enc_padding_mask\n",
        "    if FLAGS.pointer_gen:\n",
        "      feed_dict[self._enc_batch_extend_vocab] = batch.enc_batch_extend_vocab\n",
        "      feed_dict[self._max_art_oovs] = batch.max_art_oovs\n",
        "    if not just_enc:\n",
        "      feed_dict[self._dec_batch] = batch.dec_batch\n",
        "      feed_dict[self._target_batch] = batch.target_batch\n",
        "      feed_dict[self._dec_padding_mask] = batch.dec_padding_mask\n",
        "    return feed_dict\n",
        "\n",
        "  def _add_encoder(self, encoder_inputs, seq_len):\n",
        "    \"\"\"Add a single-layer bidirectional LSTM encoder to the graph.\n",
        "    Args:\n",
        "      encoder_inputs: A tensor of shape [batch_size, <=max_enc_steps, emb_size].\n",
        "      seq_len: Lengths of encoder_inputs (before padding). A tensor of shape [batch_size].\n",
        "    Returns:\n",
        "      encoder_outputs:\n",
        "        A tensor of shape [batch_size, <=max_enc_steps, 2*hidden_dim]. It's 2*hidden_dim because it's the concatenation of the forwards and backwards states.\n",
        "      fw_state, bw_state:\n",
        "        Each are LSTMStateTuples of shape ([batch_size,hidden_dim],[batch_size,hidden_dim])\n",
        "    \"\"\"\n",
        "    with tf.compat.v1.variable_scope('encoder'):\n",
        "      cell_fw = tf.compat.v1.nn.rnn_cell.LSTMCell(self._hps.hidden_dim, initializer=self.rand_unif_init, state_is_tuple=True)\n",
        "      cell_bw = tf.compat.v1.nn.rnn_cell.LSTMCell(self._hps.hidden_dim, initializer=self.rand_unif_init, state_is_tuple=True)\n",
        "      (encoder_outputs, (fw_st, bw_st)) = tf.compat.v1.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, encoder_inputs, dtype=tf.float32, sequence_length=seq_len, swap_memory=True)\n",
        "      encoder_outputs = tf.concat(axis=2, values=encoder_outputs) # concatenate the forwards and backwards states\n",
        "    return encoder_outputs, fw_st, bw_st\n",
        "\n",
        "\n",
        "  def _reduce_states(self, fw_st, bw_st):\n",
        "    \"\"\"Add to the graph a linear layer to reduce the encoder's final FW and BW state into a single initial state for the decoder. This is needed because the encoder is bidirectional but the decoder is not.\n",
        "    Args:\n",
        "      fw_st: LSTMStateTuple with hidden_dim units.\n",
        "      bw_st: LSTMStateTuple with hidden_dim units.\n",
        "    Returns:\n",
        "      state: LSTMStateTuple with hidden_dim units.\n",
        "    \"\"\"\n",
        "    hidden_dim = self._hps.hidden_dim\n",
        "    with tf.compat.v1.variable_scope('reduce_final_st'):\n",
        "\n",
        "      # Define weights and biases to reduce the cell and reduce the state\n",
        "      w_reduce_c = tf.compat.v1.get_variable('w_reduce_c', [hidden_dim * 2, hidden_dim], dtype=tf.float32, initializer=self.trunc_norm_init)\n",
        "      w_reduce_h = tf.compat.v1.get_variable('w_reduce_h', [hidden_dim * 2, hidden_dim], dtype=tf.float32, initializer=self.trunc_norm_init)\n",
        "      bias_reduce_c = tf.compat.v1.get_variable('bias_reduce_c', [hidden_dim], dtype=tf.float32, initializer=self.trunc_norm_init)\n",
        "      bias_reduce_h = tf.compat.v1.get_variable('bias_reduce_h', [hidden_dim], dtype=tf.float32, initializer=self.trunc_norm_init)\n",
        "\n",
        "      # Apply linear layer\n",
        "      old_c = tf.concat(axis=1, values=[fw_st.c, bw_st.c]) # Concatenation of fw and bw cell\n",
        "      old_h = tf.concat(axis=1, values=[fw_st.h, bw_st.h]) # Concatenation of fw and bw state\n",
        "      new_c = tf.nn.relu(tf.matmul(old_c, w_reduce_c) + bias_reduce_c) # Get new cell from old cell\n",
        "      new_h = tf.nn.relu(tf.matmul(old_h, w_reduce_h) + bias_reduce_h) # Get new state from old state\n",
        "      return tf.compat.v1.nn.rnn_cell.LSTMStateTuple(new_c, new_h) # Return new cell and state\n",
        "\n",
        "\n",
        "  def _add_decoder(self, inputs):\n",
        "    \"\"\"Add attention decoder to the graph. In train or eval mode, you call this once to get output on ALL steps. In decode (beam search) mode, you call this once for EACH decoder step.\n",
        "    Args:\n",
        "      inputs: inputs to the decoder (word embeddings). A list of tensors shape (batch_size, emb_dim)\n",
        "    Returns:\n",
        "      outputs: List of tensors; the outputs of the decoder\n",
        "      out_state: The final state of the decoder\n",
        "      attn_dists: A list of tensors; the attention distributions\n",
        "      p_gens: A list of tensors shape (batch_size, 1); the generation probabilities\n",
        "      coverage: A tensor, the current coverage vector\n",
        "    \"\"\"\n",
        "    hps = self._hps\n",
        "    cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hps.hidden_dim, state_is_tuple=True, initializer=self.rand_unif_init)\n",
        "\n",
        "    prev_coverage = self.prev_coverage if hps.mode==\"decode\" and hps.coverage else None # In decode mode, we run attention_decoder one step at a time and so need to pass in the previous step's coverage vector each time\n",
        "\n",
        "    outputs, out_state, attn_dists, p_gens, coverage = attention_decoder(inputs, self._dec_in_state, self._enc_states, self._enc_padding_mask, cell, initial_state_attention=(hps.mode==\"decode\"), pointer_gen=hps.pointer_gen, use_coverage=hps.coverage, prev_coverage=prev_coverage)\n",
        "\n",
        "    return outputs, out_state, attn_dists, p_gens, coverage\n",
        "\n",
        "  def _calc_final_dist(self, vocab_dists, attn_dists):\n",
        "    \"\"\"Calculate the final distribution, for the pointer-generator model\n",
        "    Args:\n",
        "      vocab_dists: The vocabulary distributions. List length max_dec_steps of (batch_size, vsize) arrays. The words are in the order they appear in the vocabulary file.\n",
        "      attn_dists: The attention distributions. List length max_dec_steps of (batch_size, attn_len) arrays\n",
        "    Returns:\n",
        "      final_dists: The final distributions. List length max_dec_steps of (batch_size, extended_vsize) arrays.\n",
        "    \"\"\"\n",
        "    with tf.compat.v1.variable_scope('final_distribution'):\n",
        "      # Multiply vocab dists by p_gen and attention dists by (1-p_gen)\n",
        "      vocab_dists = [p_gen * dist for (p_gen,dist) in zip(self.p_gens, vocab_dists)]\n",
        "      attn_dists = [(1-p_gen) * dist for (p_gen,dist) in zip(self.p_gens, attn_dists)]\n",
        "\n",
        "      # Concatenate some zeros to each vocabulary dist, to hold the probabilities for in-article OOV words\n",
        "      extended_vsize = self._vocab.size() + self._max_art_oovs # the maximum (over the batch) size of the extended vocabulary\n",
        "      extra_zeros = tf.zeros((self._hps.batch_size, self._max_art_oovs))\n",
        "      vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists] # list length max_dec_steps of shape (batch_size, extended_vsize)\n",
        "\n",
        "      # Project the values in the attention distributions onto the appropriate entries in the final distributions\n",
        "      # This means that if a_i = 0.1 and the ith encoder word is w, and w has index 500 in the vocabulary, then we add 0.1 onto the 500th entry of the final distribution\n",
        "      # This is done for each decoder timestep.\n",
        "      # This is fiddly; we use tf.scatter_nd to do the projection\n",
        "      batch_nums = tf.range(0, limit=self._hps.batch_size) # shape (batch_size)\n",
        "      batch_nums = tf.expand_dims(batch_nums, 1) # shape (batch_size, 1)\n",
        "      attn_len = tf.shape(self._enc_batch_extend_vocab)[1] # number of states we attend over\n",
        "      batch_nums = tf.tile(batch_nums, [1, attn_len]) # shape (batch_size, attn_len)\n",
        "      indices = tf.stack( (batch_nums, self._enc_batch_extend_vocab), axis=2) # shape (batch_size, enc_t, 2)\n",
        "      shape = [self._hps.batch_size, extended_vsize]\n",
        "      attn_dists_projected = [tf.scatter_nd(indices, copy_dist, shape) for copy_dist in attn_dists] # list length max_dec_steps (batch_size, extended_vsize)\n",
        "\n",
        "      # Add the vocab distributions and the copy distributions together to get the final distributions\n",
        "      # final_dists is a list length max_dec_steps; each entry is a tensor shape (batch_size, extended_vsize) giving the final distribution for that decoder timestep\n",
        "      # Note that for decoder timesteps and examples corresponding to a [PAD] token, this is junk - ignore.\n",
        "      final_dists = [vocab_dist + copy_dist for (vocab_dist,copy_dist) in zip(vocab_dists_extended, attn_dists_projected)]\n",
        "\n",
        "      return final_dists\n",
        "\n",
        "  def _add_emb_vis(self, embedding_var):\n",
        "    \"\"\"Do setup so that we can view word embedding visualization in Tensorboard, as described here:\n",
        "    https://www.tensorflow.org/get_started/embedding_viz\n",
        "    Make the vocab metadata file, then make the projector config file pointing to it.\"\"\"\n",
        "    train_dir = os.path.join(FLAGS.log_root, \"train\")\n",
        "    vocab_metadata_path = os.path.join(train_dir, \"vocab_metadata.tsv\")\n",
        "    self._vocab.write_metadata(vocab_metadata_path) # write metadata file\n",
        "    summary_writer = tf.compat.v1.summary.FileWriter(train_dir)\n",
        "    config = projector.ProjectorConfig()\n",
        "    embedding = config.embeddings.add()\n",
        "    embedding.tensor_name = embedding_var.name\n",
        "    embedding.metadata_path = vocab_metadata_path\n",
        "    projector.visualize_embeddings(summary_writer, config)\n",
        "\n",
        "  def _add_seq2seq(self):\n",
        "    \"\"\"Add the whole sequence-to-sequence model to the graph.\"\"\"\n",
        "    hps = self._hps\n",
        "    vsize = self._vocab.size() # size of the vocabulary\n",
        "\n",
        "    with tf.compat.v1.variable_scope('seq2seq'):\n",
        "      # Some initializers\n",
        "      self.rand_unif_init = tf.random_uniform_initializer(-hps.rand_unif_init_mag, hps.rand_unif_init_mag, seed=123)\n",
        "      self.trunc_norm_init = tf.compat.v1.truncated_normal_initializer(stddev=hps.trunc_norm_init_std)\n",
        "\n",
        "      # Add embedding matrix (shared by the encoder and decoder inputs)\n",
        "      with tf.compat.v1.variable_scope('embedding' , reuse = tf.compat.v1.AUTO_REUSE):\n",
        "        embedding = tf.compat.v1.get_variable('embedding', [vsize, hps.emb_dim], dtype=tf.float32, initializer=self.trunc_norm_init)\n",
        "        if hps.mode==\"train\": self._add_emb_vis(embedding) # add to tensorboard\n",
        "        emb_enc_inputs = tf.nn.embedding_lookup(embedding, self._enc_batch) # tensor with shape (batch_size, max_enc_steps, emb_size)\n",
        "        emb_dec_inputs = [tf.nn.embedding_lookup(embedding, x) for x in tf.unstack(self._dec_batch, axis=1)] # list length max_dec_steps containing shape (batch_size, emb_size)\n",
        "\n",
        "      # Add the encoder.\n",
        "      enc_outputs, fw_st, bw_st = self._add_encoder(emb_enc_inputs, self._enc_lens)\n",
        "      self._enc_states = enc_outputs\n",
        "\n",
        "      # Our encoder is bidirectional and our decoder is unidirectional so we need to reduce the final encoder hidden state to the right size to be the initial decoder hidden state\n",
        "      self._dec_in_state = self._reduce_states(fw_st, bw_st)\n",
        "\n",
        "      # Add the decoder.\n",
        "      with tf.compat.v1.variable_scope('decoder'):\n",
        "        decoder_outputs, self._dec_out_state, self.attn_dists, self.p_gens, self.coverage = self._add_decoder(emb_dec_inputs)\n",
        "\n",
        "      # Add the output projection to obtain the vocabulary distribution\n",
        "      with tf.compat.v1.variable_scope('output_projection'):\n",
        "        w = tf.compat.v1.get_variable('w', [hps.hidden_dim, vsize], dtype=tf.float32, initializer=self.trunc_norm_init)\n",
        "        w_t = tf.transpose(w)\n",
        "        v = tf.compat.v1.get_variable('v', [vsize], dtype=tf.float32, initializer=self.trunc_norm_init)\n",
        "        vocab_scores = [] # vocab_scores is the vocabulary distribution before applying softmax. Each entry on the list corresponds to one decoder step\n",
        "        for i,output in enumerate(decoder_outputs):\n",
        "          if i > 0:\n",
        "            tf.compat.v1.get_variable_scope().reuse_variables()\n",
        "          vocab_scores.append(tf.compat.v1.nn.xw_plus_b(output, w, v)) # apply the linear layer\n",
        "\n",
        "        vocab_dists = [tf.nn.softmax(s) for s in vocab_scores] # The vocabulary distributions. List length max_dec_steps of (batch_size, vsize) arrays. The words are in the order they appear in the vocabulary file.\n",
        "\n",
        "\n",
        "      # For pointer-generator model, calc final distribution from copy distribution and vocabulary distribution\n",
        "      if FLAGS.pointer_gen:\n",
        "        final_dists = self._calc_final_dist(vocab_dists, self.attn_dists)\n",
        "      else: # final distribution is just vocabulary distribution\n",
        "        final_dists = vocab_dists\n",
        "\n",
        "\n",
        "\n",
        "      if hps.mode in ['train', 'eval']:\n",
        "        # Calculate the loss\n",
        "        with tf.compat.v1.variable_scope('loss'):\n",
        "          if FLAGS.pointer_gen:\n",
        "            # Calculate the loss per step\n",
        "            # This is fiddly; we use tf.gather_nd to pick out the probabilities of the gold target words\n",
        "            loss_per_step = [] # will be list length max_dec_steps containing shape (batch_size)\n",
        "            batch_nums = tf.range(0, limit=hps.batch_size) # shape (batch_size)\n",
        "            for dec_step, dist in enumerate(final_dists):\n",
        "              targets = self._target_batch[:,dec_step] # The indices of the target words. shape (batch_size)\n",
        "              indices = tf.stack( (batch_nums, targets), axis=1) # shape (batch_size, 2)\n",
        "              gold_probs = tf.gather_nd(dist, indices) # shape (batch_size). prob of correct words on this step\n",
        "              losses = -tf.compat.v1.log(gold_probs)\n",
        "              loss_per_step.append(losses)\n",
        "\n",
        "            # Apply dec_padding_mask and get loss\n",
        "            self._loss = _mask_and_avg(loss_per_step, self._dec_padding_mask)\n",
        "\n",
        "          else: # baseline model\n",
        "            self._loss = tfa.seq2seq.sequence_loss(tf.stack(vocab_scores, axis=1), self._target_batch, self._dec_padding_mask) # this applies softmax internally\n",
        "\n",
        "          tf.summary.scalar('loss', self._loss)\n",
        "\n",
        "          # Calculate coverage loss from the attention distributions\n",
        "          if hps.coverage:\n",
        "            with tf.compat.v1.variable_scope('coverage_loss'):\n",
        "              self._coverage_loss = _coverage_loss(self.attn_dists, self._dec_padding_mask)\n",
        "              tf.summary.scalar('coverage_loss', self._coverage_loss)\n",
        "            self._total_loss = self._loss + hps.cov_loss_wt * self._coverage_loss\n",
        "            tf.summary.scalar('total_loss', self._total_loss)\n",
        "\n",
        "    if hps.mode == \"decode\":\n",
        "      # We run decode beam search mode one decoder step at a time\n",
        "      assert len(final_dists)==1 # final_dists is a singleton list containing shape (batch_size, extended_vsize)\n",
        "      final_dists = final_dists[0]\n",
        "      topk_probs, self._topk_ids = tf.nn.top_k(final_dists, hps.batch_size*2) # take the k largest probs. note batch_size=beam_size in decode mode\n",
        "      self._topk_log_probs = tf.compat.v1.log(topk_probs)\n",
        "\n",
        "\n",
        "  def _add_train_op(self):\n",
        "    \"\"\"Sets self._train_op, the op to run for training.\"\"\"\n",
        "    # Take gradients of the trainable variables w.r.t. the loss function to minimize\n",
        "    loss_to_minimize = self._total_loss if self._hps.coverage else self._loss\n",
        "    tvars = tf.compat.v1.trainable_variables()\n",
        "    gradients = tf.gradients(loss_to_minimize, tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
        "\n",
        "    # Clip the gradients\n",
        "    with tf.device(\"/gpu:0\"):\n",
        "      grads, global_norm = tf.clip_by_global_norm(gradients, self._hps.max_grad_norm)\n",
        "\n",
        "    # Add a summary\n",
        "    tf.summary.scalar('global_norm', global_norm)\n",
        "\n",
        "    # Apply adagrad optimizer\n",
        "    optimizer = tf.compat.v1.train.AdagradOptimizer(self._hps.lr, initial_accumulator_value=self._hps.adagrad_init_acc)\n",
        "    with tf.device(\"/gpu:0\"):\n",
        "      self._train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step, name='train_step')\n",
        "\n",
        "\n",
        "  def build_graph(self):\n",
        "    \"\"\"Add the placeholders, model, global step, train_op and summaries to the graph\"\"\"\n",
        "    tf.compat.v1.logging.info('Building graph...')\n",
        "    t0 = time.time()\n",
        "    self._add_placeholders()\n",
        "    with tf.device(\"/gpu:0\"):\n",
        "      self._add_seq2seq()\n",
        "    self.global_step = tf.compat.v1.Variable(0, name='global_step', trainable=False)\n",
        "    if self._hps.mode == 'train':\n",
        "      self._add_train_op()\n",
        "    self._summaries = tf.compat.v1.summary.merge_all()\n",
        "    t1 = time.time()\n",
        "    tf.compat.v1.logging.info('Time to build graph: %i seconds', t1 - t0)\n",
        "\n",
        "  def run_train_step(self, sess, batch):\n",
        "    \"\"\"Runs one training iteration. Returns a dictionary containing train op, summaries, loss, global_step and (optionally) coverage loss.\"\"\"\n",
        "    feed_dict = self._make_feed_dict(batch)\n",
        "    to_return = {\n",
        "        'train_op': self._train_op,\n",
        "        'summaries': self._summaries,\n",
        "        'loss': self._loss,\n",
        "        'global_step': self.global_step,\n",
        "    }\n",
        "    if self._hps.coverage:\n",
        "      to_return['coverage_loss'] = self._coverage_loss\n",
        "    return sess.run(to_return, feed_dict)\n",
        "\n",
        "  def run_eval_step(self, sess, batch):\n",
        "    \"\"\"Runs one evaluation iteration. Returns a dictionary containing summaries, loss, global_step and (optionally) coverage loss.\"\"\"\n",
        "    feed_dict = self._make_feed_dict(batch)\n",
        "    to_return = {\n",
        "        'summaries': self._summaries,\n",
        "        'loss': self._loss,\n",
        "        'global_step': self.global_step,\n",
        "    }\n",
        "    if self._hps.coverage:\n",
        "      to_return['coverage_loss'] = self._coverage_loss\n",
        "    return sess.run(to_return, feed_dict)\n",
        "\n",
        "  def run_encoder(self, sess, batch):\n",
        "    \"\"\"For beam search decoding. Run the encoder on the batch and return the encoder states and decoder initial state.\n",
        "    Args:\n",
        "      sess: Tensorflow session.\n",
        "      batch: Batch object that is the same example repeated across the batch (for beam search)\n",
        "    Returns:\n",
        "      enc_states: The encoder states. A tensor of shape [batch_size, <=max_enc_steps, 2*hidden_dim].\n",
        "      dec_in_state: A LSTMStateTuple of shape ([1,hidden_dim],[1,hidden_dim])\n",
        "    \"\"\"\n",
        "    feed_dict = self._make_feed_dict(batch, just_enc=True) # feed the batch into the placeholders\n",
        "    (enc_states, dec_in_state, global_step) = sess.run([self._enc_states, self._dec_in_state, self.global_step], feed_dict) # run the encoder\n",
        "\n",
        "    # dec_in_state is LSTMStateTuple shape ([batch_size,hidden_dim],[batch_size,hidden_dim])\n",
        "    # Given that the batch is a single example repeated, dec_in_state is identical across the batch so we just take the top row.\n",
        "    dec_in_state = tf.compat.v1.nn.rnn_cell.LSTMStateTuple(dec_in_state.c[0], dec_in_state.h[0])\n",
        "    return enc_states, dec_in_state\n",
        "\n",
        "\n",
        "  def decode_onestep(self, sess, batch, latest_tokens, enc_states, dec_init_states, prev_coverage):\n",
        "    \"\"\"For beam search decoding. Run the decoder for one step.\n",
        "    Args:\n",
        "      sess: Tensorflow session.\n",
        "      batch: Batch object containing single example repeated across the batch\n",
        "      latest_tokens: Tokens to be fed as input into the decoder for this timestep\n",
        "      enc_states: The encoder states.\n",
        "      dec_init_states: List of beam_size LSTMStateTuples; the decoder states from the previous timestep\n",
        "      prev_coverage: List of np arrays. The coverage vectors from the previous timestep. List of None if not using coverage.\n",
        "    Returns:\n",
        "      ids: top 2k ids. shape [beam_size, 2*beam_size]\n",
        "      probs: top 2k log probabilities. shape [beam_size, 2*beam_size]\n",
        "      new_states: new states of the decoder. a list length beam_size containing\n",
        "        LSTMStateTuples each of shape ([hidden_dim,],[hidden_dim,])\n",
        "      attn_dists: List length beam_size containing lists length attn_length.\n",
        "      p_gens: Generation probabilities for this step. A list length beam_size. List of None if in baseline mode.\n",
        "      new_coverage: Coverage vectors for this step. A list of arrays. List of None if coverage is not turned on.\n",
        "    \"\"\"\n",
        "\n",
        "    beam_size = len(dec_init_states)\n",
        "\n",
        "    # Turn dec_init_states (a list of LSTMStateTuples) into a single LSTMStateTuple for the batch\n",
        "    cells = [np.expand_dims(state.c, axis=0) for state in dec_init_states]\n",
        "    hiddens = [np.expand_dims(state.h, axis=0) for state in dec_init_states]\n",
        "    new_c = np.concatenate(cells, axis=0)  # shape [batch_size,hidden_dim]\n",
        "    new_h = np.concatenate(hiddens, axis=0)  # shape [batch_size,hidden_dim]\n",
        "    new_dec_in_state = tf.compat.v1.nn.rnn_cell.LSTMStateTuple(new_c, new_h)\n",
        "\n",
        "    feed = {\n",
        "        self._enc_states: enc_states,\n",
        "        self._enc_padding_mask: batch.enc_padding_mask,\n",
        "        self._dec_in_state: new_dec_in_state,\n",
        "        self._dec_batch: np.transpose(np.array([latest_tokens])),\n",
        "    }\n",
        "\n",
        "    to_return = {\n",
        "      \"ids\": self._topk_ids,\n",
        "      \"probs\": self._topk_log_probs,\n",
        "      \"states\": self._dec_out_state,\n",
        "      \"attn_dists\": self.attn_dists\n",
        "    }\n",
        "\n",
        "    if FLAGS.pointer_gen:\n",
        "      feed[self._enc_batch_extend_vocab] = batch.enc_batch_extend_vocab\n",
        "      feed[self._max_art_oovs] = batch.max_art_oovs\n",
        "      to_return['p_gens'] = self.p_gens\n",
        "\n",
        "    if self._hps.coverage:\n",
        "      feed[self.prev_coverage] = np.stack(prev_coverage, axis=0)\n",
        "      to_return['coverage'] = self.coverage\n",
        "\n",
        "    results = sess.run(to_return, feed_dict=feed) # run the decoder step\n",
        "\n",
        "    # Convert results['states'] (a single LSTMStateTuple) into a list of LSTMStateTuple -- one for each hypothesis\n",
        "    new_states = [tf.compat.v1.nn.rnn_cell.LSTMStateTuple(results['states'].c[i, :], results['states'].h[i, :]) for i in range(beam_size)]\n",
        "\n",
        "    # Convert singleton list containing a tensor to a list of k arrays\n",
        "    assert len(results['attn_dists'])==1\n",
        "    attn_dists = results['attn_dists'][0].tolist()\n",
        "\n",
        "    if FLAGS.pointer_gen:\n",
        "      # Convert singleton list containing a tensor to a list of k arrays\n",
        "      assert len(results['p_gens'])==1\n",
        "      p_gens = results['p_gens'][0].tolist()\n",
        "    else:\n",
        "      p_gens = [None for _ in range(beam_size)]\n",
        "\n",
        "    # Convert the coverage tensor to a list length k containing the coverage vector for each hypothesis\n",
        "    if FLAGS.coverage:\n",
        "      new_coverage = results['coverage'].tolist()\n",
        "      assert len(new_coverage) == beam_size\n",
        "    else:\n",
        "      new_coverage = [None for _ in range(beam_size)]\n",
        "\n",
        "    return results['ids'], results['probs'], new_states, attn_dists, p_gens, new_coverage\n",
        "\n",
        "\n",
        "def _mask_and_avg(values, padding_mask):\n",
        "  \"\"\"Applies mask to values then returns overall average (a scalar)\n",
        "  Args:\n",
        "    values: a list length max_dec_steps containing arrays shape (batch_size).\n",
        "    padding_mask: tensor shape (batch_size, max_dec_steps) containing 1s and 0s.\n",
        "  Returns:\n",
        "    a scalar\n",
        "  \"\"\"\n",
        "\n",
        "  dec_lens = tf.reduce_sum(padding_mask, axis=1) # shape batch_size. float32\n",
        "  values_per_step = [v * padding_mask[:,dec_step] for dec_step,v in enumerate(values)]\n",
        "  values_per_ex = sum(values_per_step)/dec_lens # shape (batch_size); normalized value for each batch member\n",
        "  return tf.reduce_mean(values_per_ex) # overall average\n",
        "\n",
        "\n",
        "def _coverage_loss(attn_dists, padding_mask):\n",
        "  \"\"\"Calculates the coverage loss from the attention distributions.\n",
        "  Args:\n",
        "    attn_dists: The attention distributions for each decoder timestep. A list length max_dec_steps containing shape (batch_size, attn_length)\n",
        "    padding_mask: shape (batch_size, max_dec_steps).\n",
        "  Returns:\n",
        "    coverage_loss: scalar\n",
        "  \"\"\"\n",
        "  coverage = tf.zeros_like(attn_dists[0]) # shape (batch_size, attn_length). Initial coverage is zero.\n",
        "  covlosses = [] # Coverage loss per decoder timestep. Will be list length max_dec_steps containing shape (batch_size).\n",
        "  for a in attn_dists:\n",
        "    covloss = tf.reduce_sum(tf.minimum(a, coverage), [1]) # calculate the coverage loss for this step\n",
        "    covlosses.append(covloss)\n",
        "    coverage += a # update the coverage vector\n",
        "  coverage_loss = _mask_and_avg(covlosses, padding_mask)\n",
        "  return coverage_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xiTKD1O4N_A",
        "colab_type": "text"
      },
      "source": [
        "#BEAM SEARCH#\n",
        "\"\"\"This file contains code to run beam search decoding\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2cHi-sW4Se1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "FLAGS = tf.compat.v1.flags.Flag\n",
        "\n",
        "class Hypothesis(object):\n",
        "  \"\"\"Class to represent a hypothesis during beam search. Holds all the information needed for the hypothesis.\"\"\"\n",
        "\n",
        "  def __init__(self, tokens, log_probs, state, attn_dists, p_gens, coverage):\n",
        "    \"\"\"Hypothesis constructor.\n",
        "    Args:\n",
        "      tokens: List of integers. The ids of the tokens that form the summary so far.\n",
        "      log_probs: List, same length as tokens, of floats, giving the log probabilities of the tokens so far.\n",
        "      state: Current state of the decoder, a LSTMStateTuple.\n",
        "      attn_dists: List, same length as tokens, of numpy arrays with shape (attn_length). These are the attention distributions so far.\n",
        "      p_gens: List, same length as tokens, of floats, or None if not using pointer-generator model. The values of the generation probability so far.\n",
        "      coverage: Numpy array of shape (attn_length), or None if not using coverage. The current coverage vector.\n",
        "    \"\"\"\n",
        "    self.tokens = tokens\n",
        "    self.log_probs = log_probs\n",
        "    self.state = state\n",
        "    self.attn_dists = attn_dists\n",
        "    self.p_gens = p_gens\n",
        "    self.coverage = coverage\n",
        "\n",
        "  def extend(self, token, log_prob, state, attn_dist, p_gen, coverage):\n",
        "    \"\"\"Return a NEW hypothesis, extended with the information from the latest step of beam search.\n",
        "    Args:\n",
        "      token: Integer. Latest token produced by beam search.\n",
        "      log_prob: Float. Log prob of the latest token.\n",
        "      state: Current decoder state, a LSTMStateTuple.\n",
        "      attn_dist: Attention distribution from latest step. Numpy array shape (attn_length).\n",
        "      p_gen: Generation probability on latest step. Float.\n",
        "      coverage: Latest coverage vector. Numpy array shape (attn_length), or None if not using coverage.\n",
        "    Returns:\n",
        "      New Hypothesis for next step.\n",
        "    \"\"\"\n",
        "    return Hypothesis(tokens = self.tokens + [token],\n",
        "                      log_probs = self.log_probs + [log_prob],\n",
        "                      state = state,\n",
        "                      attn_dists = self.attn_dists + [attn_dist],\n",
        "                      p_gens = self.p_gens + [p_gen],\n",
        "                      coverage = coverage)\n",
        "\n",
        "  @property\n",
        "  def latest_token(self):\n",
        "    return self.tokens[-1]\n",
        "\n",
        "  @property\n",
        "  def log_prob(self):\n",
        "    # the log probability of the hypothesis so far is the sum of the log probabilities of the tokens so far\n",
        "    return sum(self.log_probs)\n",
        "\n",
        "  @property\n",
        "  def avg_log_prob(self):\n",
        "    # normalize log probability by number of tokens (otherwise longer sequences always have lower probability)\n",
        "    return self.log_prob / len(self.tokens)\n",
        "\n",
        "\n",
        "def run_beam_search(sess, model, vocab, batch):\n",
        "  \"\"\"Performs beam search decoding on the given example.\n",
        "  Args:\n",
        "    sess: a tf.Session\n",
        "    model: a seq2seq model\n",
        "    vocab: Vocabulary object\n",
        "    batch: Batch object that is the same example repeated across the batch\n",
        "  Returns:\n",
        "    best_hyp: Hypothesis object; the best hypothesis found by beam search.\n",
        "  \"\"\"\n",
        "  # Run the encoder to get the encoder hidden states and decoder initial state\n",
        "  enc_states, dec_in_state = model.run_encoder(sess, batch)\n",
        "  # dec_in_state is a LSTMStateTuple\n",
        "  # enc_states has shape [batch_size, <=max_enc_steps, 2*hidden_dim].\n",
        "\n",
        "  # Initialize beam_size-many hyptheses\n",
        "  hyps = [Hypothesis(tokens=[vocab.word2id(START_DECODING)],\n",
        "                     log_probs=[0.0],\n",
        "                     state=dec_in_state,\n",
        "                     attn_dists=[],\n",
        "                     p_gens=[],\n",
        "                     coverage=np.zeros([batch.enc_batch.shape[1]]) # zero vector of length attention_length\n",
        "                     ) for _ in range(FLAGS.beam_size)]\n",
        "  results = [] # this will contain finished hypotheses (those that have emitted the [STOP] token)\n",
        "\n",
        "  steps = 0\n",
        "  while steps < FLAGS.max_dec_steps and len(results) < FLAGS.beam_size:\n",
        "    latest_tokens = [h.latest_token for h in hyps] # latest token produced by each hypothesis\n",
        "    latest_tokens = [t if t in range(vocab.size()) else vocab.word2id(UNKNOWN_TOKEN) for t in latest_tokens] # change any in-article temporary OOV ids to [UNK] id, so that we can lookup word embeddings\n",
        "    states = [h.state for h in hyps] # list of current decoder states of the hypotheses\n",
        "    prev_coverage = [h.coverage for h in hyps] # list of coverage vectors (or None)\n",
        "\n",
        "    # Run one step of the decoder to get the new info\n",
        "    (topk_ids, topk_log_probs, new_states, attn_dists, p_gens, new_coverage) = model.decode_onestep(sess=sess,\n",
        "                        batch=batch,\n",
        "                        latest_tokens=latest_tokens,\n",
        "                        enc_states=enc_states,\n",
        "                        dec_init_states=states,\n",
        "                        prev_coverage=prev_coverage)\n",
        "\n",
        "    # Extend each hypothesis and collect them all in all_hyps\n",
        "    all_hyps = []\n",
        "    num_orig_hyps = 1 if steps == 0 else len(hyps) # On the first step, we only had one original hypothesis (the initial hypothesis). On subsequent steps, all original hypotheses are distinct.\n",
        "    for i in range(num_orig_hyps):\n",
        "      h, new_state, attn_dist, p_gen, new_coverage_i = hyps[i], new_states[i], attn_dists[i], p_gens[i], new_coverage[i]  # take the ith hypothesis and new decoder state info\n",
        "      for j in range(FLAGS.beam_size * 2):  # for each of the top 2*beam_size hyps:\n",
        "        # Extend the ith hypothesis with the jth option\n",
        "        new_hyp = h.extend(token=topk_ids[i, j],\n",
        "                           log_prob=topk_log_probs[i, j],\n",
        "                           state=new_state,\n",
        "                           attn_dist=attn_dist,\n",
        "                           p_gen=p_gen,\n",
        "                           coverage=new_coverage_i)\n",
        "        all_hyps.append(new_hyp)\n",
        "\n",
        "    # Filter and collect any hypotheses that have produced the end token.\n",
        "    hyps = [] # will contain hypotheses for the next step\n",
        "    for h in sort_hyps(all_hyps): # in order of most likely h\n",
        "      if h.latest_token == vocab.word2id(STOP_DECODING): # if stop token is reached...\n",
        "        # If this hypothesis is sufficiently long, put in results. Otherwise discard.\n",
        "        if steps >= FLAGS.min_dec_steps:\n",
        "          results.append(h)\n",
        "      else: # hasn't reached stop token, so continue to extend this hypothesis\n",
        "        hyps.append(h)\n",
        "      if len(hyps) == FLAGS.beam_size or len(results) == FLAGS.beam_size:\n",
        "        # Once we've collected beam_size-many hypotheses for the next step, or beam_size-many complete hypotheses, stop.\n",
        "        break\n",
        "\n",
        "    steps += 1\n",
        "\n",
        "  # At this point, either we've got beam_size results, or we've reached maximum decoder steps\n",
        "\n",
        "  if len(results)==0: # if we don't have any complete results, add all current hypotheses (incomplete summaries) to results\n",
        "    results = hyps\n",
        "\n",
        "  # Sort hypotheses by average log probability\n",
        "  hyps_sorted = sort_hyps(results)\n",
        "\n",
        "  # Return the hypothesis with highest average log prob\n",
        "  return hyps_sorted[0]\n",
        "\n",
        "def sort_hyps(hyps):\n",
        "  \"\"\"Return a list of Hypothesis objects, sorted by descending average log probability\"\"\"\n",
        "  return sorted(hyps, key=lambda h: h.avg_log_prob, reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPN1L8CT4V3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "FLAGS = tf.compat.v1.flags.Flag\n",
        "\n",
        "def get_config():\n",
        "  \"\"\"Returns config for tf.session\"\"\"\n",
        "  config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
        "  config.gpu_options.allow_growth=True\n",
        "  return config\n",
        "\n",
        "def load_ckpt(saver, sess, ckpt_dir=\"train\"):\n",
        "  \"\"\"Load checkpoint from the ckpt_dir (if unspecified, this is train dir) and restore it to saver and sess, waiting 10 secs in the case of failure. Also returns checkpoint name.\"\"\"\n",
        "  while True:\n",
        "    try:\n",
        "      latest_filename = \"checkpoint_best\" if ckpt_dir==\"eval\" else None\n",
        "      ckpt_dir = os.path.join(FLAGS.log_root, ckpt_dir)\n",
        "      ckpt_state = tf.train.get_checkpoint_state(ckpt_dir, latest_filename=latest_filename)\n",
        "      tf.compat.v1.logging.info('Loading checkpoint %s', ckpt_state.model_checkpoint_path)\n",
        "      saver.restore(sess, ckpt_state.model_checkpoint_path)\n",
        "      return ckpt_state.model_checkpoint_path\n",
        "    except:\n",
        "      tf.compat.v1.logging.info(\"Failed to load checkpoint from %s. Sleeping for %i secs...\", ckpt_dir, 10)\n",
        "      time.sleep(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "929neZps4Ya2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://pymotw.com/2/xml/etree/ElementTree/create.html\n",
        "\n",
        "from xml.etree import ElementTree\n",
        "from xml.dom import minidom\n",
        "from functools import reduce\n",
        "\n",
        "def prettify(elem):\n",
        "    \"\"\"Return a pretty-printed XML string for the Element.\n",
        "    \"\"\"\n",
        "    rough_string = ElementTree.tostring(elem, 'utf-8')\n",
        "    reparsed = minidom.parseString(rough_string)\n",
        "    return reparsed.toprettyxml(indent=\"  \")\n",
        "  \n",
        "from xml.etree.ElementTree import Element, SubElement, Comment\n",
        "\n",
        "\n",
        "def write_sum(article , reference , summary_array , default_path):\n",
        "  top = Element('WriteSum')\n",
        "\n",
        "  comment = Comment('Write summarizes')\n",
        "  top.append(comment)\n",
        "\n",
        "  i=0\n",
        "  for summ in summary_array:\n",
        "    example = SubElement(top, 'example')\n",
        "    article_element   = SubElement(example, 'article')\n",
        "    article_element.text = article[i]\n",
        "\n",
        "    reference_element = SubElement(example, 'reference')\n",
        "    reference_element.text = reference[i]\n",
        "\n",
        "    summary_element   = SubElement(example, 'summary')\n",
        "    summary_element.text = summ\n",
        "    i+=1\n",
        "    \n",
        "  with open(default_path + \"result_pointer_2.xml\", \"w+\") as f:\n",
        "    f.write(prettify(top))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM2WQbZo4gav",
        "colab_type": "text"
      },
      "source": [
        "#DECODER#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrVu194DJmr_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6173f8f4-734f-413f-f19b-657a792e7a08"
      },
      "source": [
        "pip install pyrouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyrouge in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2q38HV54fD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"This file contains code to run beam search decoding, including running ROUGE evaluation and producing JSON datafiles for the in-browser attention visualizer, which can be found here https://github.com/abisee/attn_vis\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "#import beam_search\n",
        "#import data\n",
        "import json\n",
        "import pyrouge\n",
        "#import util\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "FLAGS = tf.compat.v1.flags.Flag\n",
        "\n",
        "SECS_UNTIL_NEW_CKPT = 60  # max number of seconds before loading new checkpoint\n",
        "\n",
        "article_withunks_list = []\n",
        "abstract_withunks_list=[]\n",
        "decoded_output_list = []\n",
        "    \n",
        "class BeamSearchDecoder(object):\n",
        "  \"\"\"Beam search decoder.\"\"\"\n",
        "\n",
        "  def __init__(self, model, batcher, vocab):\n",
        "    \"\"\"Initialize decoder.\n",
        "    Args:\n",
        "      model: a Seq2SeqAttentionModel object.\n",
        "      batcher: a Batcher object.\n",
        "      vocab: Vocabulary object\n",
        "    \"\"\"\n",
        "    self._model = model\n",
        "    self._model.build_graph()\n",
        "    self._batcher = batcher\n",
        "    self._vocab = vocab\n",
        "    self._saver = tf.compat.v1.train.Saver() # we use this to load checkpoints for decoding\n",
        "    self._sess = tf.compat.v1.Session(config = get_config())\n",
        "\n",
        "    # Load an initial checkpoint to use for decoding\n",
        "    ckpt_path = load_ckpt(self._saver, self._sess)\n",
        "\n",
        "    if FLAGS.single_pass:\n",
        "      # Make a descriptive decode directory name\n",
        "      ckpt_name = \"ckpt-\" + ckpt_path.split('-')[-1]  + \"test\"# this is something of the form \"ckpt-123456\"\n",
        "      self._decode_dir = os.path.join(FLAGS.log_root, get_decode_dir_name(ckpt_name))\n",
        "      if os.path.exists(self._decode_dir):\n",
        "        raise Exception(\"single_pass decode directory %s should not already exist\" % self._decode_dir)\n",
        "\n",
        "    else: # Generic decode dir name\n",
        "      self._decode_dir = os.path.join(FLAGS.log_root, \"decode\")\n",
        "\n",
        "    # Make the decode dir if necessary\n",
        "    if not os.path.exists(self._decode_dir): os.mkdir(self._decode_dir)\n",
        "\n",
        "   \n",
        "    if FLAGS.single_pass:\n",
        "      # Make the dirs to contain output written in the correct format for pyrouge\n",
        "      self._rouge_ref_dir = os.path.join(self._decode_dir, \"reference\")\n",
        "      if not os.path.exists(self._rouge_ref_dir): os.mkdir(self._rouge_ref_dir)\n",
        "      self._rouge_dec_dir = os.path.join(self._decode_dir, \"decoded\")\n",
        "      if not os.path.exists(self._rouge_dec_dir): os.mkdir(self._rouge_dec_dir)\n",
        "      \n",
        "\n",
        "  def decode(self):\n",
        "    \"\"\"Decode examples until data is exhausted (if FLAGS.single_pass) and return, or decode indefinitely, loading latest checkpoint at regular intervals\"\"\"\n",
        "    t0 = time.time()\n",
        "    counter = 0\n",
        "    while True:\n",
        "      batch = self._batcher.next_batch()  # 1 example repeated across batch\n",
        "      if batch is None: # finished decoding dataset in single_pass mode\n",
        "        assert FLAGS.single_pass, \"Dataset exhausted, but we are not in single_pass mode\"\n",
        "        tf.compat.v1.logging.info(\"Decoder has finished reading dataset for single_pass.\")\n",
        "        tf.compat.v1.logging.info(\"Output has been saved in %s and %s. Now starting ROUGE eval...\", self._rouge_ref_dir, self._rouge_dec_dir)\n",
        "        results_dict = rouge_eval(self._rouge_ref_dir, self._rouge_dec_dir)\n",
        "        rouge_log(results_dict, self._decode_dir)\n",
        "        return\n",
        "\n",
        "      original_article = batch.original_articles[0]  # string\n",
        "      original_abstract = batch.original_abstracts[0]  # string\n",
        "      original_abstract_sents = batch.original_abstracts_sents[0]  # list of strings\n",
        "\n",
        "      article_withunks = show_art_oovs(original_article, self._vocab) # string\n",
        "      abstract_withunks =show_abs_oovs(original_abstract, self._vocab, (batch.art_oovs[0] if FLAGS.pointer_gen else None)) # string\n",
        "\n",
        "      # Run beam search to get best Hypothesis\n",
        "      best_hyp = run_beam_search(self._sess, self._model, self._vocab, batch)\n",
        "\n",
        "      # Extract the output ids from the hypothesis and convert back to words\n",
        "      output_ids = [int(t) for t in best_hyp.tokens[1:]]\n",
        "      decoded_words = outputids2words(output_ids, self._vocab, (batch.art_oovs[0] if FLAGS.pointer_gen else None))\n",
        "\n",
        "      # Remove the [STOP] token from decoded_words, if necessary\n",
        "      try:\n",
        "        fst_stop_idx = decoded_words.index(STOP_DECODING) # index of the (first) [STOP] symbol\n",
        "        decoded_words = decoded_words[:fst_stop_idx]\n",
        "      except ValueError:\n",
        "        decoded_words = decoded_words\n",
        "      decoded_output = ' '.join(decoded_words) # single string\n",
        "\n",
        "      if FLAGS.single_pass:\n",
        "        self.write_for_rouge(original_abstract_sents, decoded_words, counter) # write ref summary and decoded summary to file, to eval with pyrouge later\n",
        "        counter += 1 # this is how many examples we've decoded\n",
        "        if counter == 2000: #just stop when reading first 2000 samples\n",
        "          tf.compat.v1.logging.info(\"stopped at 2000 samples\")\n",
        "          tf.compat.v1.logging.info(\"Output has been saved in %s and %s. Now starting ROUGE eval...\", self._rouge_ref_dir, self._rouge_dec_dir)\n",
        "          results_dict = rouge_eval(self._rouge_ref_dir, self._rouge_dec_dir)\n",
        "          rouge_log(results_dict, self._decode_dir)\n",
        "          return\n",
        "      else:\n",
        "        print_results(article_withunks, abstract_withunks, decoded_output) # log output to screen\n",
        "        self.write_for_attnvis(article_withunks, abstract_withunks, decoded_words, best_hyp.attn_dists, best_hyp.p_gens) # write info to .json file for visualization tool\n",
        "        article_withunks_list.append(article_withunks)\n",
        "        abstract_withunks_list.append(abstract_withunks)\n",
        "        decoded_output_list.append(decoded_output)\n",
        "        counter += 1 # this is how many examples we've decoded\n",
        "        if counter == 2000: #just stop when reading first 2000 samples\n",
        "          tf.compat.v1.logging.info(\"stopped at 2000 samples\")\n",
        "          tf.compat.v1.logging.info(\"Now starting eval to %s...\", self._decode_dir )\n",
        "          write_sum(article_withunks_list,abstract_withunks_list ,decoded_output_list ,self._decode_dir)\n",
        "          return\n",
        "        \n",
        "        # Check if SECS_UNTIL_NEW_CKPT has elapsed; if so return so we can load a new checkpoint\n",
        "        t1 = time.time()\n",
        "        if t1-t0 > SECS_UNTIL_NEW_CKPT:\n",
        "          tf.compat.v1.logging.info('We\\'ve been decoding with same checkpoint for %i seconds. Time to load new checkpoint', t1-t0)\n",
        "          _ = load_ckpt(self._saver, self._sess)\n",
        "          t0 = time.time()\n",
        "\n",
        "  def write_for_rouge(self, reference_sents, decoded_words, ex_index):\n",
        "    \"\"\"Write output to file in correct format for eval with pyrouge. This is called in single_pass mode.\n",
        "    Args:\n",
        "      reference_sents: list of strings\n",
        "      decoded_words: list of strings\n",
        "      ex_index: int, the index with which to label the files\n",
        "    \"\"\"\n",
        "    # First, divide decoded output into sentences\n",
        "    decoded_sents = []\n",
        "    while len(decoded_words) > 0:\n",
        "      try:\n",
        "        fst_period_idx = decoded_words.index(\".\")\n",
        "      except ValueError: # there is text remaining that doesn't end in \".\"\n",
        "        fst_period_idx = len(decoded_words)\n",
        "      sent = decoded_words[:fst_period_idx+1] # sentence up to and including the period\n",
        "      decoded_words = decoded_words[fst_period_idx+1:] # everything else\n",
        "      decoded_sents.append(' '.join(sent))\n",
        "\n",
        "    # pyrouge calls a perl script that puts the data into HTML files.\n",
        "    # Therefore we need to make our output HTML safe.\n",
        "    decoded_sents = [make_html_safe(w) for w in decoded_sents]\n",
        "    reference_sents = [make_html_safe(w) for w in reference_sents]\n",
        "\n",
        "    # Write to file\n",
        "    ref_file = os.path.join(self._rouge_ref_dir, \"%06d_reference.txt\" % ex_index)\n",
        "    decoded_file = os.path.join(self._rouge_dec_dir, \"%06d_decoded.txt\" % ex_index)\n",
        "\n",
        "    with open(ref_file, \"w\") as f:\n",
        "      for idx,sent in enumerate(reference_sents):\n",
        "        f.write(sent) if idx==len(reference_sents)-1 else f.write(sent+\"\\n\")\n",
        "    with open(decoded_file, \"w\") as f:\n",
        "      for idx,sent in enumerate(decoded_sents):\n",
        "        f.write(sent) if idx==len(decoded_sents)-1 else f.write(sent+\"\\n\")\n",
        "\n",
        "    tf.compat.v1.logging.info(\"Wrote example %i to file\" % ex_index)\n",
        "\n",
        "\n",
        "  def write_for_attnvis(self, article, abstract, decoded_words, attn_dists, p_gens):\n",
        "    \"\"\"Write some data to json file, which can be read into the in-browser attention visualizer tool:\n",
        "      https://github.com/abisee/attn_vis\n",
        "    Args:\n",
        "      article: The original article string.\n",
        "      abstract: The human (correct) abstract string.\n",
        "      attn_dists: List of arrays; the attention distributions.\n",
        "      decoded_words: List of strings; the words of the generated summary.\n",
        "      p_gens: List of scalars; the p_gen values. If not running in pointer-generator mode, list of None.\n",
        "    \"\"\"\n",
        "    article_lst = article.split() # list of words\n",
        "    decoded_lst = decoded_words # list of decoded words\n",
        "    to_write = {\n",
        "        #'article_lst': [make_html_safe(t) for t in article_lst],\n",
        "        'decoded_lst': [make_html_safe(t) for t in decoded_lst],\n",
        "        'abstract_str': make_html_safe(abstract),\n",
        "        'attn_dists': attn_dists\n",
        "    }\n",
        "    if FLAGS.pointer_gen:\n",
        "      to_write['p_gens'] = p_gens\n",
        "    output_fname = os.path.join(self._decode_dir, 'attn_vis_data.json')\n",
        "    with open(output_fname, 'w') as output_file:\n",
        "      json.dump(to_write, output_file)\n",
        "    tf.compat.v1.logging.info('Wrote visualization data to %s', output_fname)\n",
        "\n",
        "\n",
        "def print_results(article, abstract, decoded_output):\n",
        "  \"\"\"Prints the article, the reference summmary and the decoded summary to screen\"\"\"\n",
        "  print( \"\")\n",
        "  tf.compat.v1.logging.info('ARTICLE:  %s', article)\n",
        "  tf.compat.v1.logging.info('REFERENCE SUMMARY: %s', abstract)\n",
        "  tf.compat.v1.logging.info('GENERATED SUMMARY: %s', decoded_output)\n",
        "  print (\"\")\n",
        "\n",
        "\n",
        "def make_html_safe(s):\n",
        "  \"\"\"Replace any angled brackets in string s to avoid interfering with HTML attention visualizer.\"\"\"\n",
        "  s.replace(\"<\", \"&lt;\")\n",
        "  s.replace(\">\", \"&gt;\")\n",
        "  return s\n",
        "\n",
        "\n",
        "def rouge_eval(ref_dir, dec_dir):\n",
        "  \"\"\"Evaluate the files in ref_dir and dec_dir with pyrouge, returning results_dict\"\"\"\n",
        "  r = pyrouge.Rouge155()\n",
        "  r.model_filename_pattern = '#ID#_reference.txt'\n",
        "  r.system_filename_pattern = '(\\d+)_decoded.txt'\n",
        "  r.model_dir = ref_dir\n",
        "  r.system_dir = dec_dir\n",
        "  logging.getLogger('global').setLevel(logging.WARNING) # silence pyrouge logging\n",
        "  rouge_results = r.convert_and_evaluate()\n",
        "  return r.output_to_dict(rouge_results)\n",
        "\n",
        "\n",
        "def rouge_log(results_dict, dir_to_write):\n",
        "  \"\"\"Log ROUGE results to screen and write to file.\n",
        "  Args:\n",
        "    results_dict: the dictionary returned by pyrouge\n",
        "    dir_to_write: the directory where we will write the results to\"\"\"\n",
        "  log_str = \"\"\n",
        "  for x in [\"1\",\"2\",\"l\"]:\n",
        "    log_str += \"\\nROUGE-%s:\\n\" % x\n",
        "    for y in [\"f_score\", \"recall\", \"precision\"]:\n",
        "      key = \"rouge_%s_%s\" % (x,y)\n",
        "      key_cb = key + \"_cb\"\n",
        "      key_ce = key + \"_ce\"\n",
        "      val = results_dict[key]\n",
        "      val_cb = results_dict[key_cb]\n",
        "      val_ce = results_dict[key_ce]\n",
        "      log_str += \"%s: %.4f with confidence interval (%.4f, %.4f)\\n\" % (key, val, val_cb, val_ce)\n",
        "  tf.compat.v1.logging.info(log_str) # log to screen\n",
        "  results_file = os.path.join(dir_to_write, \"ROUGE_results.txt\")\n",
        "  tf.compat.v1.logging.info(\"Writing final ROUGE results to %s...\", results_file)\n",
        "  with open(results_file, \"w\") as f:\n",
        "    f.write(log_str)\n",
        "\n",
        "def get_decode_dir_name(ckpt_name):\n",
        "  \"\"\"Make a descriptive name for the decode dir, including the name of the checkpoint we use to decode. This is called in single_pass mode.\"\"\"\n",
        "\n",
        "  if \"train\" in FLAGS.data_path: dataset = \"train\"\n",
        "  elif \"val\" in FLAGS.data_path: dataset = \"val\"\n",
        "  elif \"test\" in FLAGS.data_path: dataset = \"test\"\n",
        "  else: raise ValueError(\"FLAGS.data_path %s should contain one of train, val or test\" % (FLAGS.data_path))\n",
        "  dirname = \"decode_%s_%imaxenc_%ibeam_%imindec_%imaxdec\" % (dataset, FLAGS.max_enc_steps, FLAGS.beam_size, FLAGS.min_dec_steps, FLAGS.max_dec_steps)\n",
        "  if ckpt_name is not None:\n",
        "    dirname += \"_%s\" % ckpt_name\n",
        "  return dirname\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkmHEora4mG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"This is the top-level file to train, evaluate or test your summarization model\"\"\"\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "#from data import Vocab\n",
        "#from batcher import Batcher\n",
        "#from model import SummarizationModel\n",
        "#from decode import BeamSearchDecoder\n",
        "#import util\n",
        "from tensorflow.python import debug as tf_debug\n",
        "\n",
        "#FLAGS = tf.app.flags.FLAGS\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calc_running_avg_loss(loss, running_avg_loss, summary_writer, step, decay=0.99):\n",
        "  \"\"\"Calculate the running average loss via exponential decay.\n",
        "  This is used to implement early stopping w.r.t. a more smooth loss curve than the raw loss curve.\n",
        "  Args:\n",
        "    loss: loss on the most recent eval step\n",
        "    running_avg_loss: running_avg_loss so far\n",
        "    summary_writer: FileWriter object to write for tensorboard\n",
        "    step: training iteration step\n",
        "    decay: rate of exponential decay, a float between 0 and 1. Larger is smoother.\n",
        "  Returns:\n",
        "    running_avg_loss: new running average loss\n",
        "  \"\"\"\n",
        "  if running_avg_loss == 0:  # on the first iteration just take the loss\n",
        "    running_avg_loss = loss\n",
        "  else:\n",
        "    running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
        "  running_avg_loss = min(running_avg_loss, 12)  # clip\n",
        "  loss_sum = tf.Summary()\n",
        "  tag_name = 'running_avg_loss/decay=%f' % (decay)\n",
        "  loss_sum.value.add(tag=tag_name, simple_value=running_avg_loss)\n",
        "  summary_writer.add_summary(loss_sum, step)\n",
        "  tf.compat.v1.logging.info('running_avg_loss: %f', running_avg_loss)\n",
        "  return running_avg_loss\n",
        "\n",
        "\n",
        "def restore_best_model():\n",
        "  \"\"\"Load bestmodel file from eval directory, add variables for adagrad, and save to train directory\"\"\"\n",
        "  tf.compat.v1.compat.v1.logging.info(\"Restoring bestmodel for training...\")\n",
        "\n",
        "  # Initialize all vars in the model\n",
        "  sess = tf.compat.v1.Session(config=get_config())\n",
        "  print( \"Initializing all variables...\")\n",
        "  sess.run(tf.initialize_all_variables())\n",
        "\n",
        "  # Restore the best model from eval dir\n",
        "  saver = tf.compat.v1.train.Saver([v for v in tf.all_variables() if \"Adagrad\" not in v.name])\n",
        "  print( \"Restoring all non-adagrad variables from best model in eval dir...\")\n",
        "  curr_ckpt = load_ckpt(saver, sess, \"eval\")\n",
        "  print (\"Restored %s.\" % curr_ckpt)\n",
        "\n",
        "  # Save this model to train dir and quit\n",
        "  new_model_name = curr_ckpt.split(\"/\")[-1].replace(\"bestmodel\", \"model\")\n",
        "  new_fname = os.path.join(FLAGS.log_root, \"train\", new_model_name)\n",
        "  print (\"Saving model to %s...\" % (new_fname))\n",
        "  new_saver = tf.compat.v1.train.Saver() # this saver saves all variables that now exist, including Adagrad variables\n",
        "  new_saver.save(sess, new_fname)\n",
        "  print (\"Saved.\")\n",
        "  exit()\n",
        "\n",
        "\n",
        "def convert_to_coverage_model():\n",
        "  \"\"\"Load non-coverage checkpoint, add initialized extra variables for coverage, and save as new checkpoint\"\"\"\n",
        "  tf.compat.v1.logging.info(\"converting non-coverage model to coverage model..\")\n",
        "\n",
        "  # initialize an entire coverage model from scratch\n",
        "  sess = tf.compat.v1.Session(config=get_config())\n",
        "  print (\"initializing everything...\")\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  # load all non-coverage weights from checkpoint\n",
        "  saver = tf.compat.v1.train.Saver([v for v in tf.global_variables() if \"coverage\" not in v.name and \"Adagrad\" not in v.name])\n",
        "  print (\"restoring non-coverage variables...\")\n",
        "  curr_ckpt = load_ckpt(saver, sess)\n",
        "  print (\"restored.\")\n",
        "\n",
        "  # save this model and quit\n",
        "  new_fname = curr_ckpt + '_cov_init'\n",
        "  print (\"saving model to %s...\" % (new_fname))\n",
        "  new_saver = tf.compat.v1.train.Saver() # this one will save all variables that now exist\n",
        "  new_saver.save(sess, new_fname)\n",
        "  print (\"saved.\")\n",
        "  exit()\n",
        "\n",
        "\n",
        "def setup_training(model, batcher):\n",
        "  \"\"\"Does setup before starting training (run_training)\"\"\"\n",
        "  train_dir = os.path.join(FLAGS.log_root, \"train\")\n",
        "  if not os.path.exists(train_dir): os.makedirs(train_dir)\n",
        "\n",
        "  model.build_graph() # build the graph\n",
        "  if FLAGS.convert_to_coverage_model:\n",
        "    assert FLAGS.coverage, \"To convert your non-coverage model to a coverage model, run with convert_to_coverage_model=True and coverage=True\"\n",
        "    convert_to_coverage_model()\n",
        "  if FLAGS.restore_best_model:\n",
        "    restore_best_model()\n",
        "  saver = tf.compat.v1.train.Saver(max_to_keep=3) # keep 3 checkpoints at a time\n",
        "\n",
        "  sv = tf.compat.v1.train.Supervisor(logdir=train_dir,\n",
        "                     is_chief=True,\n",
        "                     saver=saver,\n",
        "                     summary_op=None,\n",
        "                     save_summaries_secs=60, # save summaries for tensorboard every 60 secs\n",
        "                     save_model_secs=60, # checkpoint every 60 secs\n",
        "                     global_step=model.global_step)\n",
        "  summary_writer = sv.summary_writer\n",
        "  tf.compat.v1.logging.info(\"Preparing or waiting for session...\")\n",
        "  sess_context_manager = sv.prepare_or_wait_for_session(config=get_config())\n",
        "  tf.compat.v1.logging.info(\"Created session.\")\n",
        "  try:\n",
        "    run_training(model, batcher, sess_context_manager, sv, summary_writer) # this is an infinite loop until interrupted\n",
        "  except KeyboardInterrupt:\n",
        "    tf.compat.v1.logging.info(\"Caught keyboard interrupt on worker. Stopping supervisor...\")\n",
        "    sv.stop()\n",
        "\n",
        "\n",
        "def run_training(model, batcher, sess_context_manager, sv, summary_writer):\n",
        "  \"\"\"Repeatedly runs training iterations, logging loss to screen and writing summaries\"\"\"\n",
        "  tf.compat.v1.logging.info(\"starting run_training\")\n",
        "  with sess_context_manager as sess:\n",
        "    if FLAGS.debug: # start the tensorflow debugger\n",
        "      sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
        "      sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n",
        "    while True: # repeats until interrupted\n",
        "      batch = batcher.next_batch()\n",
        "\n",
        "      tf.compat.v1.logging.info('running training step...')\n",
        "      t0=time.time()\n",
        "      results = model.run_train_step(sess, batch)\n",
        "      t1=time.time()\n",
        "      tf.compat.v1.logging.info('seconds for training step: %.3f', t1-t0)\n",
        "\n",
        "      loss = results['loss']\n",
        "      tf.compat.v1.logging.info('loss: %f', loss) # print the loss to screen\n",
        "\n",
        "      if not np.isfinite(loss):\n",
        "        raise Exception(\"Loss is not finite. Stopping.\")\n",
        "\n",
        "      if FLAGS.coverage:\n",
        "        coverage_loss = results['coverage_loss']\n",
        "        tf.compat.v1.logging.info(\"coverage_loss: %f\", coverage_loss) # print the coverage loss to screen\n",
        "\n",
        "      # get the summaries and iteration number so we can write summaries to tensorboard\n",
        "      summaries = results['summaries'] # we will write these summaries to tensorboard using summary_writer\n",
        "      train_step = results['global_step'] # we need this to update our running average loss\n",
        "\n",
        "      summary_writer.add_summary(summaries, train_step) # write the summaries\n",
        "      if train_step % 100 == 0: # flush the summary writer every so often\n",
        "        summary_writer.flush()\n",
        "\n",
        "\n",
        "def run_eval(model, batcher, vocab):\n",
        "  \"\"\"Repeatedly runs eval iterations, logging to screen and writing summaries. Saves the model with the best loss seen so far.\"\"\"\n",
        "  model.build_graph() # build the graph\n",
        "  saver = tf.compat.v1.train.Saver(max_to_keep=3) # we will keep 3 best checkpoints at a time\n",
        "  sess = tf.compat.v1.Session(config=get_config())\n",
        "  eval_dir = os.path.join(FLAGS.log_root, \"eval\") # make a subdir of the root dir for eval data\n",
        "  bestmodel_save_path = os.path.join(eval_dir, 'bestmodel') # this is where checkpoints of best models are saved\n",
        "  summary_writer = tf.compat.v1.summary.FileWriter(eval_dir)\n",
        "  running_avg_loss = 0 # the eval job keeps a smoother, running average loss to tell it when to implement early stopping\n",
        "  best_loss = None  # will hold the best loss achieved so far\n",
        "\n",
        "  while True:\n",
        "    _ = load_ckpt(saver, sess) # load a new checkpoint\n",
        "    batch = batcher.next_batch() # get the next batch\n",
        "\n",
        "    # run eval on the batch\n",
        "    t0=time.time()\n",
        "    results = model.run_eval_step(sess, batch)\n",
        "    t1=time.time()\n",
        "    tf.compat.v1.logging.info('seconds for batch: %.2f', t1-t0)\n",
        "\n",
        "    # print the loss and coverage loss to screen\n",
        "    loss = results['loss']\n",
        "    tf.compat.v1.logging.info('loss: %f', loss)\n",
        "    if FLAGS.coverage:\n",
        "      coverage_loss = results['coverage_loss']\n",
        "      tf.compat.v1.logging.info(\"coverage_loss: %f\", coverage_loss)\n",
        "\n",
        "    # add summaries\n",
        "    summaries = results['summaries']\n",
        "    train_step = results['global_step']\n",
        "    summary_writer.add_summary(summaries, train_step)\n",
        "\n",
        "    # calculate running avg loss\n",
        "    running_avg_loss = calc_running_avg_loss(np.asscalar(loss), running_avg_loss, summary_writer, train_step)\n",
        "\n",
        "    # If running_avg_loss is best so far, save this checkpoint (early stopping).\n",
        "    # These checkpoints will appear as bestmodel-<iteration_number> in the eval dir\n",
        "    if best_loss is None or running_avg_loss < best_loss:\n",
        "      tf.compat.v1.logging.info('Found new best model with %.3f running_avg_loss. Saving to %s', running_avg_loss, bestmodel_save_path)\n",
        "      saver.save(sess, bestmodel_save_path, global_step=train_step, latest_filename='checkpoint_best')\n",
        "      best_loss = running_avg_loss\n",
        "\n",
        "    # flush the summary writer every so often\n",
        "    if train_step % 100 == 0:\n",
        "      summary_writer.flush()\n",
        "\n",
        "\n",
        "def main():\n",
        "  #if len(unused_argv) != 1: # prints a message if you've entered flags incorrectly\n",
        "  #  raise Exception(\"Problem with flags: %s\" % unused_argv)\n",
        "  tf.compat.v1.reset_default_graph()\n",
        "\n",
        "\n",
        "  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO) # choose what level of logging you want\n",
        "  tf.compat.v1.logging.info('Starting seq2seq_attention in %s mode...', (FLAGS.mode))\n",
        "\n",
        "  # Change log_root to FLAGS.log_root/FLAGS.exp_name and create the dir if necessary\n",
        "  FLAGS.log_root = os.path.join(FLAGS.log_root, FLAGS.exp_name)\n",
        "  if not os.path.exists(FLAGS.log_root):\n",
        "    if FLAGS.mode==\"train\":\n",
        "      os.makedirs(FLAGS.log_root)\n",
        "    else:\n",
        "      raise Exception(\"Logdir %s doesn't exist. Run in train mode to create it.\" % (FLAGS.log_root))\n",
        "\n",
        "  vocab = Vocab(FLAGS.vocab_path, FLAGS.vocab_size) # create a vocabulary\n",
        "\n",
        "  # If in decode mode, set batch_size = beam_size\n",
        "  # Reason: in decode mode, we decode one example at a time.\n",
        "  # On each step, we have beam_size-many hypotheses in the beam, so we need to make a batch of these hypotheses.\n",
        "  if FLAGS.mode == 'decode':\n",
        "    FLAGS.batch_size = FLAGS.beam_size\n",
        "\n",
        "  # If single_pass=True, check we're in decode mode\n",
        "  if FLAGS.single_pass and FLAGS.mode!='decode':\n",
        "    raise Exception(\"The single_pass flag should only be True in decode mode\")\n",
        "\n",
        "  # Make a namedtuple hps, containing the values of the hyperparameters that the model needs\n",
        "  hparam_list = ['mode', 'lr', 'adagrad_init_acc', 'rand_unif_init_mag', 'trunc_norm_init_std', 'max_grad_norm', 'hidden_dim', 'emb_dim', 'batch_size', 'max_dec_steps', 'max_enc_steps', 'coverage', 'cov_loss_wt', 'pointer_gen']\n",
        "  hps_dict = {}\n",
        "\n",
        "  flag_members = [attr for attr in dir(FLAGS) if not callable(getattr(FLAGS, attr)) and not attr.startswith(\"__\")]\n",
        "  for m in flag_members:\n",
        "    hps_dict[m] = getattr(FLAGS, m)\n",
        "  \n",
        "  hps = namedtuple(\"HParams\", hps_dict.keys())(**hps_dict)\n",
        "\n",
        "  # Create a batcher object that will create minibatches of data\n",
        "  batcher = Batcher(FLAGS.data_path, vocab, hps, single_pass=FLAGS.single_pass)\n",
        "\n",
        "  tf.compat.v1.set_random_seed(111) # a seed value for randomness\n",
        "\n",
        "  if hps.mode == 'train':\n",
        "    print( \"creating model...\")\n",
        "    model = SummarizationModel(hps, vocab)\n",
        "    setup_training(model, batcher)\n",
        "  elif hps.mode == 'eval':\n",
        "    model = SummarizationModel(hps, vocab)\n",
        "    run_eval(model, batcher, vocab)\n",
        "  elif hps.mode == 'decode':\n",
        "    decode_model_hps = hps  # This will be the hyperparameters for the decoder model\n",
        "    decode_model_hps = hps._replace(max_dec_steps=1) # The model is configured with max_dec_steps=1 because we only ever run one step of the decoder at a time (to do beam search). Note that the batcher is initialized with max_dec_steps equal to e.g. 100 because the batches need to contain the full summaries\n",
        "    model = SummarizationModel(decode_model_hps, vocab)\n",
        "    decoder = BeamSearchDecoder(model, batcher, vocab)\n",
        "    decoder.decode() # decode indefinitely (unless single_pass=True, in which case deocde the dataset exactly once)\n",
        "  else:\n",
        "    raise ValueError(\"The 'mode' flag must be one of train/eval/decode\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7l1Xlfc4qYW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "871943ed-4ea8-4f93-f32f-e2213324cbab"
      },
      "source": [
        "\"\"\"\n",
        "#train\n",
        "class flags_:\n",
        "  pass\n",
        "FLAGS = flags_()\n",
        "\n",
        "# Where to find data\n",
        "FLAGS.data_path= default_path + 'finished_files/chunked/train_*'\t#, 'Path expression to tf.Example datafiles. Can include wildcards to access multiple datafiles.')\n",
        "#FLAGS.data_path= default_path + 'finished_files/chunked/val_*'\t#, 'Path expression to tf.Example datafiles. Can include wildcards to access multiple datafiles.')\n",
        "FLAGS.vocab_path= default_path + 'finished_files/vocab'\t#, 'Path expression to text vocabulary file.')\n",
        "\n",
        "# Important settings\n",
        "FLAGS.mode= 'train' #'train'#, 'must be one of train/eval/decode')\n",
        "FLAGS.single_pass= False #False\n",
        "#, 'For decode mode only. If True, run eval on the full dataset using a fixed checkpoint, i.e. take the current checkpoint, \n",
        "#and use it to produce one summary for each example in the dataset, write \n",
        "#the summaries to file and then get ROUGE scores for the whole dataset.\n",
        "#If False (default), run concurrent decoding, \n",
        "#i.e. repeatedly load latest checkpoint,\n",
        "#use it to produce summaries for randomly-chosen examples and log the results to screen, indefinitely.')\n",
        "\n",
        "# Where to save output\n",
        "FLAGS.log_root= default_path +'logs'#, 'Root directory for all logging.')\n",
        "FLAGS.exp_name= 'myexperiment'#, 'Name for experiment. Logs will be saved in a directory with this name, under log_root.')\n",
        "\n",
        "# Hyperparameters\n",
        "FLAGS.hidden_dim= 256#, 'dimension of RNN hidden states')\n",
        "FLAGS.emb_dim= 128#, 'dimension of word embeddings')\n",
        "FLAGS.batch_size= 16#, 'minibatch size')\n",
        "FLAGS.max_enc_steps= 300#, 'max timesteps of encoder (max source text tokens)')\n",
        "FLAGS.max_dec_steps= 100#, 'max timesteps of decoder (max summary tokens)')\n",
        "FLAGS.beam_size= 8 #8#, 'beam size for beam search decoding.')\n",
        "FLAGS.min_dec_steps= 50#, 'Minimum sequence length of generated summary. Applies only for beam search decoding mode')\n",
        "FLAGS.vocab_size= 50000#, 'Size of vocabulary. These will be read from the vocabulary file in order. If the vocabulary file contains fewer words than this number, or if this number is set to 0, will take all words in the vocabulary file.')\n",
        "FLAGS.lr= 0.1#, 'learning rate')\n",
        "FLAGS.adagrad_init_acc= 0.1#, 'initial accumulator value for Adagrad')\n",
        "FLAGS.rand_unif_init_mag= 0.02#, 'magnitude for lstm cells random uniform inititalization')\n",
        "FLAGS.trunc_norm_init_std= 1e-4#, 'std of trunc norm init, used for initializing everything else')\n",
        "FLAGS.max_grad_norm= 2.0#, 'for gradient clipping')\n",
        "\n",
        "# Pointer-generator or baseline model\n",
        "FLAGS.pointer_gen= True#, 'If True, use pointer-generator model. If False, use baseline model.')\n",
        "\n",
        "# Coverage hyperparameters\n",
        "FLAGS.coverage= False#, 'Use coverage mechanism. Note, the experiments reported in the ACL paper train WITHOUT coverage until converged, and then train for a short phase WITH coverage afterwards. i.e. to reproduce the results in the ACL paper, turn this off for most of training then turn on for a short phase at the end.')\n",
        "FLAGS.cov_loss_wt= 1.0#, 'Weight of coverage loss (lambda in the paper). If zero, then no incentive to minimize coverage loss.')\n",
        "\n",
        "# Utility flags, for restoring and changing checkpoints\n",
        "FLAGS.convert_to_coverage_model= False#, 'Convert a non-coverage model to a coverage model. Turn this on and run in train mode. Your current training model will be copied to a new version (same name with _cov_init appended) that will be ready to run with coverage flag turned on, for the coverage training stage.')\n",
        "FLAGS.restore_best_model= False#, 'Restore the best model in the eval/ dir and save it in the train/ dir, ready to be used for further training. Useful for early stopping, or if your training checkpoint has become corrupted with e.g. NaN values.')\n",
        "\n",
        "# Debugging. See https://www.tensorflow.org/programmers_guide/debugger\n",
        "FLAGS.debug= False#, \"Run in tensorflow's debug mode (watches for NaN/inf values)\")\n",
        "\n",
        "main()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-9569ca7f6dd3>\"\u001b[0;36m, line \u001b[0;32m55\u001b[0m\n\u001b[0;31m    main()\u001b[0m\n\u001b[0m          \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_q7whHdgtfB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b6a4672-4eb1-4243-a4fa-19a637e2f951"
      },
      "source": [
        "#validation\n",
        "class flags_:\n",
        "  pass\n",
        "FLAGS = flags_()\n",
        "\n",
        "# Where to find data\n",
        "#FLAGS.data_path= default_path + 'finished_files/chunked/train_*'\t#, 'Path expression to tf.Example datafiles. Can include wildcards to access multiple datafiles.')\n",
        "FLAGS.data_path= default_path + 'finished_files/chunked/val_*'\t#, 'Path expression to tf.Example datafiles. Can include wildcards to access multiple datafiles.')\n",
        "FLAGS.vocab_path= default_path + 'finished_files/vocab'\t#, 'Path expression to text vocabulary file.')\n",
        "\n",
        "# Important settings\n",
        "FLAGS.mode= 'decode' #'train'#, 'must be one of train/eval/decode')\n",
        "FLAGS.single_pass= False #False\n",
        "#, 'For decode mode only. If True, run eval on the full dataset using a fixed checkpoint, i.e. take the current checkpoint, \n",
        "#and use it to produce one summary for each example in the dataset, write \n",
        "#the summaries to file and then get ROUGE scores for the whole dataset.\n",
        "#If False (default), run concurrent decoding, \n",
        "#i.e. repeatedly load latest checkpoint,\n",
        "#use it to produce summaries for randomly-chosen examples and log the results to screen, indefinitely.')\n",
        "\n",
        "# Where to save output\n",
        "FLAGS.log_root= default_path +'logs'#, 'Root directory for all logging.')\n",
        "FLAGS.exp_name= 'myexperiment'#, 'Name for experiment. Logs will be saved in a directory with this name, under log_root.')\n",
        "\n",
        "# Hyperparameters\n",
        "FLAGS.hidden_dim= 256#, 'dimension of RNN hidden states')\n",
        "FLAGS.emb_dim= 128#, 'dimension of word embeddings')\n",
        "FLAGS.batch_size= 16#, 'minibatch size')\n",
        "FLAGS.max_enc_steps= 300#, 'max timesteps of encoder (max source text tokens)')\n",
        "FLAGS.max_dec_steps= 100#, 'max timesteps of decoder (max summary tokens)')\n",
        "FLAGS.beam_size= 8 #8#, 'beam size for beam search decoding.')\n",
        "FLAGS.min_dec_steps= 50#, 'Minimum sequence length of generated summary. Applies only for beam search decoding mode')\n",
        "FLAGS.vocab_size= 50000#, 'Size of vocabulary. These will be read from the vocabulary file in order. If the vocabulary file contains fewer words than this number, or if this number is set to 0, will take all words in the vocabulary file.')\n",
        "FLAGS.lr= 0.1#, 'learning rate')\n",
        "FLAGS.adagrad_init_acc= 0.1#, 'initial accumulator value for Adagrad')\n",
        "FLAGS.rand_unif_init_mag= 0.02#, 'magnitude for lstm cells random uniform inititalization')\n",
        "FLAGS.trunc_norm_init_std= 1e-4#, 'std of trunc norm init, used for initializing everything else')\n",
        "FLAGS.max_grad_norm= 2.0#, 'for gradient clipping')\n",
        "\n",
        "# Pointer-generator or baseline model\n",
        "FLAGS.pointer_gen= True#, 'If True, use pointer-generator model. If False, use baseline model.')\n",
        "\n",
        "# Coverage hyperparameters\n",
        "FLAGS.coverage= False#, 'Use coverage mechanism. Note, the experiments reported in the ACL paper train WITHOUT coverage until converged, and then train for a short phase WITH coverage afterwards. i.e. to reproduce the results in the ACL paper, turn this off for most of training then turn on for a short phase at the end.')\n",
        "FLAGS.cov_loss_wt= 1.0#, 'Weight of coverage loss (lambda in the paper). If zero, then no incentive to minimize coverage loss.')\n",
        "\n",
        "# Utility flags, for restoring and changing checkpoints\n",
        "FLAGS.convert_to_coverage_model= False#, 'Convert a non-coverage model to a coverage model. Turn this on and run in train mode. Your current training model will be copied to a new version (same name with _cov_init appended) that will be ready to run with coverage flag turned on, for the coverage training stage.')\n",
        "FLAGS.restore_best_model= False#, 'Restore the best model in the eval/ dir and save it in the train/ dir, ready to be used for further training. Useful for early stopping, or if your training checkpoint has become corrupted with e.g. NaN values.')\n",
        "\n",
        "# Debugging. See https://www.tensorflow.org/programmers_guide/debugger\n",
        "FLAGS.debug= False#, \"Run in tensorflow's debug mode (watches for NaN/inf values)\")\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting seq2seq_attention in decode mode...\n",
            "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
            "Finished constructing vocabulary of 50000 total words. Last word added: nonconfrontational\n",
            "INFO:tensorflow:Building graph...\n",
            "WARNING:tensorflow:From <ipython-input-8-c6f8ee501824>:68: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-8-c6f8ee501824>:70: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:966: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:970: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "INFO:tensorflow:Adding attention_decoder timestep 0 of 1\n",
            "INFO:tensorflow:Time to build graph: 0 seconds\n",
            "INFO:tensorflow:Loading checkpoint /content/drive/My Drive/MA_colab/PG_Model2/logs/myexperiment/train/model.ckpt-15726\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/MA_colab/PG_Model2/logs/myexperiment/train/model.ckpt-15726\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  notice that most settings show time growth greater than . , we assume that this combination probability is the continuation degree divided by the total number of passive labels , categorical or tag . taking averages directly from the data , we have our first model , shown on the right in figure 9. there are some extremely minimal savings in traversals due to topdown filtering effects , but there is a corresponding penalty in edges as rules whose leftcorner can not be built are introduced . the single biggest factor in the time and traversal performance turned out to be the encoding , which is fortunate because the choice of grammar transform will depend greatly on the application . for example , the state np np np cc . these 23 categories plus the tag __nonecreate__ a passive saturation of 24 for __zerospans__ for __notransform__ . __noempties__ , __empties__ were removed by pruning nonterminals which covered no overt words . however , this is a useful observation about real world parsing performance . the linear terms are larger for __notransform__ and therefore drag the exponent down __more.11__ without unaries , the more gradual saturation growth increases the total exponent , more so for __nounarieslow__ than __nounarieshigh__ . second , active saturation grows with span size because , as spans increase , the tags in a given active edge are more likely to find a matching arrangement over a span . we frequently speak in terms of the following : the parser has an theoretical time bound , where is the number of words in the sentence to be parsed , is the number of nonterminal categories in the grammar and is the number of states in the fsa encoding of the grammar . the savings are less than for prefix compaction . alternately , the effective term could be growing with , which turns out to be true , as discussed in section 4.3. note that while the rule encoding may dramatically affect the efficiency of a parser , it does not change the actual set of parses for a given sentence in any __way.3__ 2in no case were the __nonterminaltoword__ or __toptononterminal__ unaries altered . the sentences were parsed using an implementation of the probabilistic chartparsing algorithm presented in . however , with some straightforward but __spaceconsuming__ recurrences , we can calculate the expected chance that a random rule of a given signature will match a given span length . furthermore , while passive saturation was relatively constant in span size , at least after a point , active saturation quite clearly grows with span size , even for spans well beyond those shown in figure 11. the grammar representation does not affect which passive edges will occur for a given span . however , unlike time and traversals which in practice can diverge , memory requirements match the number of edges in the chart almost exactly , since the large data structures are all proportional in size to the number of edges .6 thus , for a given span size , we report the average over all spans of that size occurring anywhere in any sentence parsed . __<s>__ this paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the penn treebank with the treebank’s own cfg grammar __</s>__ __<s>__ we show how performance is dramatically affected by rule representation and tree transformations but little by topdown vs __</s>__ __<s>__ bottomup strategies __</s>__ __<s>__ we discuss grammatical saturation including analysis of the strongly connected components of the phrasal nonterminals in the treebank and model how as sentence length increases the effective grammar rule size increases as regions of the grammar are unlocked yielding __supercubic__ observed time behavior in some configurations __</s>__ __<s>__ this paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the penn treebank with the treebank’s own cfg grammar __</s>__ __<s>__ we show how performance is dramatically affected by rule representation and tree transformations but little by topdown vs __</s>__ __<s>__ bottomup strategies __</s>__ __<s>__ we discuss grammatical saturation including analysis of the strongly connected components of the phrasal nonterminals in the treebank and model how as sentence length increases the effective grammar rule size increases as regions of the grammar are unlocked yielding __supercubic__ observed time behavior in some configurations __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the penn treebank with the treebank’s own cfg grammar we show how performance is dramatically affected by rule representation and tree transformations but little by topdown vs bottomup strategies we discuss grammatical saturation including analysis of the strongly connected components of the phrasal nonterminals in the treebank and model how as sentence length increases the effective grammar rule size increases as regions of the grammar are unlocked yielding !!__supercubic__!! observed time behavior in some configurations\n",
            "INFO:tensorflow:GENERATED SUMMARY: notice that most settings show time growth than . , we assume that this combination probability is the continuation degree divided by the total number of passive labels , categorical or tag . taking averages directly from the data , more a given active edge are more likely to find a matching arrangement over a span . we frequently speak\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  work on more efficient encoding schemes began with __a¨ıtka´ci__ et al . , and this seminal paper has __macneille__ completion at . while the continued efficiency of compiletime completion of signatures as they further increase in size can only be verified empirically , what can be said at this stage is that the only reason that signatures like lingo can be tractably compiled at all is sparseness of consistent types . that yields something roughly of the form shown in figure 3 , which is an example of a recent trend in using __typeintensive__ encodings of linguistic information into typed feature logic in hpsg , beginning with sag . we will call types such as verb __deranged__ types . compilation , if efficient , is to be preferred from the standpoint of static error detection , which incremental methods may elect to skip . this can be implemented very transparently in systems like ale that are built on top of another logic programming language with support for constraint logic programming such as sicstus prolog . the latter interpretation is __subscribed__ to by pollard and sag , for example . otherwise , the algorithm added because of a bounded set , with minimal upper bounds , , which did not have a least upper bound , i.e. , . although the maximum subtype and supertype branching factors in this family increase linearly with size , the partial orders can grow in depth instead in order to contain this . proposition 1 the resulting construction is the finite restriction of the __dedekindmacneille__ completion . the detection of __deranged__ types themselves is also a potential problem . in hpsg , it is generally assumed that __nonmaximallyspecific__ types are simply a convenient shorthand for talking about sets of maximally specific types , sometimes called species , over which the principles of a grammar are stated . let be the set of minimal types to which f is appropriate , ’ s concepts as a meet semilattice , nor that it would be convenient to add all of the types necessary to a wouldbe type hierarchy to ensure __meetsemilatticehood__ . it is , in fact , possible to embed any finite partial order into a smallest lattice that preserves existing meets and joins by adding extra elements . every type is normal except for __truedisj__ , for which the combination , __disj1falseform__ __disj2falseform__ , is not attested in either of its subtypes . they are : can be partially ordered and taken to represent partial information states about some set of objects . in other geometric respects , it bears a close enough resemblance to the theoretical worst case to cause concern about scalability . the resulting description is always __nondisjunctive__ , since logical disjunction is encoded in subtyping . this was also the choice made in the lkb parsing system for hpsg . practical performance is again much better because this algorithm can exploit the empirical observation that most types in a realistic signature are normal and that most feature value restrictions on subtypes do not vary widely . so with added is a complete lattice . __<s>__ this paper considers three assumptions conventionally made about signatures in typed feature logic that are in potential disagreement with current practice among grammar developers and linguists working within featurebased frameworks such as __hpsg:__ __meetsemilatticehood__ unique feature introduction and the absence of subtype covering __</s>__ __<s>__ it also discusses the conditions under which each of these can be tractably restored in realistic grammar signatures where they do not already exist __</s>__ __<s>__ this paper considers three assumptions conventionally made about signatures in typed feature logic that are in potential disagreement with current practice among grammar developers and linguists working within featurebased frameworks such as __hpsg:__ __meetsemilatticehood__ unique feature introduction and the absence of subtype covering __</s>__ __<s>__ it also discusses the conditions under which each of these can be tractably restored in realistic grammar signatures where they do not already exist __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper considers three assumptions conventionally made about signatures in typed feature logic that are in potential disagreement with current practice among grammar developers and linguists working within featurebased frameworks such as !!__hpsg:__!! !!__meetsemilatticehood__!! unique feature introduction and the absence of subtype covering it also discusses the conditions under which each of these can be tractably restored in realistic grammar signatures where they do not already exist\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is sparseness of consistent types . that yields something roughly of the form shown size completion at . that yields something roughly of the form shown completion at . that yields something roughly of the form shown size completion at . that yields something roughly of the form shown size completion at . that yields something roughly of the form shown size completion at . that yields something roughly of the form shown size completion at . that yields something roughly of the form shown size completion at . that yields something roughly of the form shown\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  some q & a systems , like relied both on ne recognizers and some empirical indicators . feedback loop 2 illustrated in figure 1 is activated when the question semantic form and the answer semantic form can not by unified . one of the few q & a systems that takes into account morphological , lexical and semantic alternations of terms is described in . moreover , this knowledge can be translated in axiomatic form and used for abductive proofs . the first step marks all possible concepts that are answer candidates . since in wordnet the verb prefer has verb like as a hypernym , and moreover , its glossed definition is liking better , the query becomes : sometimes multiple keywords are replaced by a semantic alternation . frequently , question reformulations use different words , but imply the same answer . answer extraction based on grammatical information is also promoted by the system described in . we encoded a total of 38 possible answer types . both the question keywords and the expected answer type are identified by using the dependencies derived from the question parse . answer in trec9 243 questions were reformulations of 54 inquiries , thus asking for the same answer . moreover , these dependencies largely cover the question semantic __form2__ . for example : nouns head and government are constituents of a possible paraphrase of president , i.e . “ head of government ” . for example , stateoftheart named entity recognizers developed for ie systems were readily available to be incorporated in q & a systems and helped recognize names of people , organizations , locations or dates . moreover , since our retrieval mechanism does not stem keywords , all the inflections of the verb are also considered . unfortunately this system is not fully autonomous , as it depends on ir results provided by external search engines . loop 1 was generated more often than any other loop . the expected answer type is determined based on the question stem , e.g . who , where or how much and eventually one of the question concepts , when the stem is ambiguous , as described in . the unification involves three steps : step 1 : the recognition of the expected answer type . for example , in the case of question __q209__ : “ who invented the paper clip ? ” , the expected answer type is person and so is the subject of the verb invented , lexicalized as the nominalization inventor . when lexical alternations are necessary because no answer was found yet , the first keyword that is altered is determined by the question word that either prompted the expected answer type or is in the same semantic class with the expected answer type . table 4 lists also the combined effect of the feedbacks , showing that when all feedbacks are enabled , for short answers we obtained an __mrar__ of __0.568__ , i.e . 76 % increase over q & a without feedbacks . for such cases several approaches have been developed . this concept is searched in an answer taxonomy comprising several tops linked to a significant number of wordnet noun and verb hierarchies . __<s>__ this paper presents an opendomain textual questionanswering system that uses several feedback loops to enhance its performance __</s>__ __<s>__ these feedback loops combine in a new way statistical results with syntactic semantic or pragmatic information derived from texts and lexical databases __</s>__ __<s>__ the paper presents the contribution of each feedback loop to the overall performance of 76% humanassessed precise answers __</s>__ __<s>__ this paper presents an opendomain textual questionanswering system that uses several feedback loops to enhance its performance __</s>__ __<s>__ these feedback loops combine in a new way statistical results with syntactic semantic or pragmatic information derived from texts and lexical databases __</s>__ __<s>__ the paper presents the contribution of each feedback loop to the overall performance of 76% humanassessed precise answers __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper presents an opendomain textual questionanswering system that uses several feedback loops to enhance its performance these feedback loops combine in a new way statistical results with syntactic semantic or pragmatic information derived from texts and lexical databases the paper presents the contribution of each feedback loop to the overall performance of 76% humanassessed precise answers\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is proposed . moreover a systems , like relied both on systems , like relied both on systems , like relied both on ne recognizers . feedback loop 2 illustrated the q & a systems , like relied both on ne some empirical indicators . feedback loop 2 illustrated the empirical indicators . feedback loop 2 illustrated . moreover , this knowledge can be translated\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the tagged sentences are further analyzed by a cascade of finite state machines leveraging patterns with lexical and syntactic information , to identify constructions such as preand postmodifying appositive phrases , e.g. , “ presidential candidate george bush ” , “ bush , the presidential candidate ” , and relative clauses , e.g. , “ senator ... , who is running for __reelection__ this fall , ” . strength of association between subject i and verb j is measured using mutual information : here __tfij__ is the maximum frequency of subjectverb pair ij in the reuters corpus the system also filters out redundant descriptions , both duplicate descriptions as well as similar ones . we took the raw descriptions that the system produced before merging , and wrote a brief description by hand for each person who had two or more raw descriptions . the component evaluation tests how accurately the tagger can identify whether a head noun in a description is appropriate as a person description the evaluation uses the wordnet 1.6 semcor semantic concordance , which has files from the brown corpus whose words have semantic tags indicating wordnet sense numbers . the fundamental differences in our work are as follows : we extract not only appositive phrases , but also clauses at large based on corpus statistics ; we make heavy use of coreference , whereas they don ’ t use coreference at all ; we focus on generating succinct descriptions by removing redundancy and merging , whereas they categorize descriptions using wordnet , without a focus on __succinctness__ . i in the corpus however , the preprocessing component described in section 2.1 does produce errors in appositive extraction , which are filtered out by syntactic and semantic tests . we report here on the development of a biographical mds summarizer that summarizes information about people described in the news . for each relative clause description , the subject was given the description , a person name to whom that description __pertained__ , and a capsule description consisting of merged appositives created by the system . a portion of the results of doing this is shown in table 1. the sentential descriptions are filtered in part based on the presence of verbs like “ testify , “ __plead__ ” , or “ greet ” that are strongly associated with the head noun of the appositive , namely “ friend ” . the weighting is based on how often the relative clause ’ s main verb is strongly associated with a subject in a large corpus , compared to its total number of appearances in the corpus . the system compares each pair of appositive descriptions of a person , merging them based on corpus frequencies of the description head stem , syntactic information , and semantic information based on the relationship between the heads in wordnet . future directions could include improved sentential descriptions as well as further intrinsic and extrinsic evaluations of the summarizer as a whole . third , there may be redundancy between the sentential descriptions , on one hand , and the appositive and relative clause descriptions , on the other . how many possessive : possessive pronouns are there ? although the sample is small , the results are very promising . __<s>__ we describe a biographical multidocument summarizer that summarizes information about people described in the news __</s>__ __<s>__ the summarizer uses corpus statistics along with linguistic knowledge to select and merge descriptions of people from a document collection removing redundant descriptions __</s>__ __<s>__ the summarization components have been extensively evaluated for coherence accuracy and nonredundancy of the descriptions produced __</s>__ __<s>__ we describe a biographical multidocument summarizer that summarizes information about people described in the news __</s>__ __<s>__ the summarizer uses corpus statistics along with linguistic knowledge to select and merge descriptions of people from a document collection removing redundant descriptions __</s>__ __<s>__ the summarization components have been extensively evaluated for coherence accuracy and nonredundancy of the descriptions produced __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we describe a biographical multidocument summarizer that summarizes information about people described in the news the summarizer uses corpus statistics along with linguistic knowledge to select and merge descriptions of people from a document collection removing redundant descriptions the summarization components have been extensively evaluated for coherence accuracy and nonredundancy of the descriptions produced\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a head noun the corpus however , the preprocessing component described section 2.1 does produce errors , to identify constructions such as preand postmodifying appositive phrases , e.g. , “ presidential candidate george bush ” , “ bush , the presidential candidate ” , the presidential candidate ” , e.g. , “ senator ... , who is running for the corpus however , the preprocessing component described section 2.1 does produce errors the corpus however , the preprocessing component described\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  that the __mcle__ does not . table 1 compares the mle and __mcle__ pcfgs . these parsers are direct simplifications of the structured language model . because of the computational complexity of these problems , the method is only applied to a simple pcfg based on the atis corpus . this paper has investigated the difference between maximum likelihood estimation and maximum conditional likelihood estimation for three different kinds of models : pcfg parsers , hmm taggers and shiftreduce parsers . one possible explanation for this result is that the way in which the interpolated estimate of p0 is calculated , rather than conditional likelihood estimation per se , is lowering tagger accuracy somehow . the labelled precision and recall parsing results for the __mcle__ pcfg were slightly higher than those of the mle pcfg . the next section formulates the __mcles__ for hmms and pcfgs as constrained optimization problems and describes an iterative dynamicprogramming method for solving them . tm and a word sequence we also append a ‘ * ’ to end of the actual terminal string being parsed , as this simplifies the formulation of the parsers , i.e. , if the string to be parsed is w1 ... a priori , one can advance plausible arguments in favour of both the mle and the __mcle__ . maximum conditional likelihood is consistent for the conditional distribution . in this application , the pairs consist of a parse tree y and its terminal string or yield x . the maximum likelihood estimates were easy to compute and for others of which the maximum conditional likelihood estimates could be easily computed . then given a training corpus d = , ... , ) , where yi is a parse tree for the string xi , the log conditional likelihood of the training data log p and its derivative are given by : � � here now consider the following conditional model of the conditional distribution of tags given words a conditional shiftreduce parser differs only minimally from the shiftreduce parser just described : it is defined by a distribution p over next moves m given the top and __nexttotop__ stack labels s1 , s2 and the next input symbol thus for bitag tagging at least , the conditional model has a considerably higher error rate than any of the joint models examined here . . on the other hand , applications which involve predicting the value of the hidden variable from the visible variable usually only involve the conditional distribution , which the __mcle__ estimates directly . for all s1 , s2 ; i.e. , shift moves can only shift the current lookahead symbol . yet adding this term improves tagging accuracy considerably , to __95.3__ % . data . however , given that we often have insufficient data in computational linguistics , and there are good reasons to believe that the true distribution of sentences or parses can not be described by our models , there is no reason to expect these asymptotic results to hold in practice , and in the experiments reported below the mle and __mcle__ behave differently experimentally . the previous section compared __similiar__ joint and conditional tagging models . __<s>__ this paper compares two different ways of estimating statistical language models __</s>__ __<s>__ many statistical nlp tagging and parsing models are estimated by maximizing the likelihood of the __fullyobserved__ training data __</s>__ __<s>__ however since these applications only require the conditional probability distributions these distributions can in principle be learnt by maximizing the conditional likelihood of the training data __</s>__ __<s>__ perhaps somewhat surprisingly models estimated by maximizing the joint were superior to models estimated by maximizing the conditional even though some of the latter models intuitively had access to __“more__ __information”__ __</s>__ __<s>__ this paper compares two different ways of estimating statistical language models __</s>__ __<s>__ many statistical nlp tagging and parsing models are estimated by maximizing the likelihood of the __fullyobserved__ training data __</s>__ __<s>__ however since these applications only require the conditional probability distributions these distributions can in principle be learnt by maximizing the conditional likelihood of the training data __</s>__ __<s>__ perhaps somewhat surprisingly models estimated by maximizing the joint were superior to models estimated by maximizing the conditional even though some of the latter models intuitively had access to __“more__ __information”__ __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper compares two different ways of estimating statistical language models many statistical nlp tagging and parsing models are estimated by maximizing the likelihood of the !!__fullyobserved__!! training data however since these applications only require the conditional probability distributions these distributions can in principle be learnt by maximizing the conditional likelihood of the training data perhaps somewhat surprisingly models estimated by maximizing the joint were superior to models estimated by maximizing the conditional even though some of the latter models intuitively had access to !!__“more__!! !!__information”__!!\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is proposed . table 1 compares the mle . table 1 compares the mle . these parsers are direct simplifications of the structured language model . because parsers are direct simplifications of the structured language model . because of the computational complexity of these problems , the method is only applied to a simple pcfg based on the atis corpus . this paper has investigated the difference between maximum likelihood estimation per likelihood estimation for three different kinds of models : pcfg parsers , hmm taggers shiftreduce parsers . one possible explanation for this result is that\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  in distinction to , the nonterminals can be structured . in fig . then is a reduction of the structure to its type , the weak generative capacity of __pdgrammars__ is stronger than that of __cfgrammars__ . theoretically , this might blow up times the size of a grammar with defect valencies and the maximal length of left parts of rules . 3 is determined by on its reduction combined with the diagram of local and long dependencies is presented in fig . meanwhile , the projectivity is not the norm in natural languages . definition 6. : we should just introduce into the dictionary both variants of the verb description – with the local dependency to the right and with the positive valency to the left . __dstructures__ with valencies , __dvstructures__ , are defined so that valencies saturation would imply connectivity . we will select the following particular one : let be the first non saturated positive valency in and be the closest corresponding 5 for the space reasons , we don ’ t cite its definition , the more so that the linguistic __pdgrammars__ should certainly be __lcfree__ . let be a __pdgrammar__ . the mechanism of establishing long dependencies is orthogonal to reduction and is implemented by a universal and simple rule of valencies saturation . as most dependency grammars , the __pdgrammars__ are analyzing . the valencies are used to specify discontinuous dependencies lacking in partial dependency structures . we change the reduction semantics as follows . this reduction is successful due to the fact that the negative valency is assigned to the preposition and the corresponding positive valency is assigned to the verb what might serve the formal basis for these assignments ? for example , the __pdgrammar__ : generates a noncf language dtree in fig . it is for this reason that dtrees determined by grammars of robinson , categorial grammars , classical lambek calculus , and some other formalisms are projective . 4 , the group of the preposition is moved , which is of course a sufficient condition for assigning the positive valency to the verb . the __dstructures__ we will use will be polarized in the sense that some words will have valencies specifying long dependencies which must enter or go from them . in the special case , where the __dvstructures__ in the rules are dtrees , the __pdgrammar__ is __local7__ . the yield of a successful reduction is a dtree . 1. for example , in most european languages there are such regular nonprojective constructions as __whor__ relative clause extraction , topicalization , comparative constructions , and some constructions specific to a language , e.g . french pronominal clitics or left dislocation . syntactic theories based on the concept of dependency have a long tradition . intuitively , we can think of as of the combined information available after the phase of morphological analysis . let us start with this verb has the strong government over prepositions in the clause in fig . dtree itself does not presume a linear order on words . let and be the unit __dvstructure__ with then is a reduction of the structure to its type and is the integral valency of this reduction denoted by 2 . __<s>__ polarized dependency grammars are proposed as a means of efficient treatment of discontinuous constructions __</s>__ __<s>__ __pdgrammars__ describe two kinds of dependencies : local explicitly derived by the rules and long implicitly specified by negative and positive valencies of words __</s>__ __<s>__ if in a __pdgrammar__ the number of __nonsaturated__ valencies in derived structures is bounded by a constant then it is weakly equivalent to a __cfgrammar__ and has a time parsing algorithm __</s>__ __<s>__ it happens that such bounded __pdgrammars__ are strong enough to express such phenomena as unbounded raising extraction and extraposition __</s>__ __<s>__ polarized dependency grammars are proposed as a means of efficient treatment of discontinuous constructions __</s>__ __<s>__ __pdgrammars__ describe two kinds of dependencies : local explicitly derived by the rules and long implicitly specified by negative and positive valencies of words __</s>__ __<s>__ if in a __pdgrammar__ the number of __nonsaturated__ valencies in derived structures is bounded by a constant then it is weakly equivalent to a __cfgrammar__ and has a time parsing algorithm __</s>__ __<s>__ it happens that such bounded __pdgrammars__ are strong enough to express such phenomena as unbounded raising extraction and extraposition __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: polarized dependency grammars are proposed as a means of efficient treatment of discontinuous constructions __pdgrammars__ describe two kinds of dependencies : local explicitly derived by the rules and long implicitly specified by negative and positive valencies of words if in a __pdgrammar__ the number of !!__nonsaturated__!! valencies in derived structures is bounded by a constant then it is weakly equivalent to a !!__cfgrammar__!! and has a time parsing algorithm it happens that such bounded __pdgrammars__ are strong enough to express such phenomena as unbounded raising extraction and extraposition\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a the norm generative capacity of the fact that the negative valency is assigned . meanwhile , the projectivity of the fact that the negative valency is assigned . meanwhile , the projectivity the corresponding positive valency the corresponding positive valency the formal basis for these assignments ? for example , the diagram are analyzing . the valencies are used to specify discontinuous\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  b is required to try and find values for these references . __representations.5__ indeed we assume that a component of dialogue competence is knowledge of these coercion operations . in fact , having posited __constits__ one could eliminate __dtrs__ : this by making the value of __constits__ be a set of sets whose first level elements are the immediate constituents . the protocol involves the assumption that an agent always initially tries to integrate an utterance by assuming it constitutes an adjacency pair with the existing __latestmove__ . __7within__ it would be a great advantage in negotiative dialogues , where , for example , the system and the user might be discussing several options and the system may make alternative suggestions , for a system to be able to recognize and interpret a ce . indeed , in principle , relation names should also be included , since they vary with context and are subject to clarification as well . these we take to be specified by a set of coercion operations on utterance __11in__ the version of hpsg information about phrases is encoded by __crossclassifying__ them in a multidimensional type hierarchy . relative to certain goals , one might decide simply to existentially quantify the problematic referent . such an utterance will have as its __maxqud__ a question __cqparaphrasable__ as who , named bo , are you asking if t left , whereas its __salutt__ is the subutterance of bo . below , we will show how to extend this account of parallelism to clarification queries . we start by informally describing the __grounding/clarification__ processes and the representations on which they operate . we sketch an algorithm for the process of utterance integration which leads to grounding or clarification . finally , we formalize the operations which underpin clarification and sketch a grammatical analysis of ce . 2 utterance representation : grounding and clarification we start by offering an informal description of how an __utterancesuch__ as can get grounded or spawn a clarification by an addressee b : a is attempting to convey to b thus , we posit constit __clar__ __intcl__ , a new phrasal subtype of __headfragph__ and of __intercl__ which encapsulates the two idiosyncratic facets of such utterances , namely the phonological parallelism and the __maxqud/content__ identity : in this paper we offered an analysis of the types of representations needed to analyze ce , the requisite operations thereon , and how these update iss during grounding and clarification . for current purposes , we stick with tradition and tolerate the redundancy of both __dtrs__ and __constits__ . intuitively , is a parameter whose value is problematic or lacking . this much is familiar already from early work on context dependence by et seq . these presuppose no sophisticated linguistic analysis . the second modification we make concerns the encoding of phrasal constituency . we posit a set valued first , we revamp the existing treatment of the feature __cindices__ . consider the following dialogue in the __routeplanning__ domain : at this point the system has to consider a number of possible __intepretations__ for the user ’ s utterance all of which involve recognizing that this is a clarification request concerning the system ’ s last utterance . __<s>__ we offer a computational analysis of the resolution of ellipsis in certain cases of dialogue clarification __</s>__ __<s>__ we show that this goes beyond standard techniques used in anaphora and ellipsis resolution and requires operations on highly structured linguistically heterogeneous representations __</s>__ __<s>__ we characterize these operations and the representations on which they operate __</s>__ __<s>__ we offer an analysis couched in a version of headdriven phrase structure grammar combined with a theory of information states in dialogue __</s>__ __<s>__ we sketch an algorithm for the process of utterance integration in iss which leads to grounding or clarification __</s>__ __<s>__ we offer a computational analysis of the resolution of ellipsis in certain cases of dialogue clarification __</s>__ __<s>__ we show that this goes beyond standard techniques used in anaphora and ellipsis resolution and requires operations on highly structured linguistically heterogeneous representations __</s>__ __<s>__ we characterize these operations and the representations on which they operate __</s>__ __<s>__ we offer an analysis couched in a version of headdriven phrase structure grammar combined with a theory of information states in dialogue __</s>__ __<s>__ we sketch an algorithm for the process of utterance integration in iss which leads to grounding or clarification __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we offer a computational analysis of the resolution of ellipsis in certain cases of dialogue clarification we show that this goes beyond standard techniques used in anaphora and ellipsis resolution and requires operations on highly structured linguistically heterogeneous representations we characterize these operations and the representations on which they operate we offer an analysis couched in a version of headdriven phrase structure grammar combined with a theory of information states in dialogue we sketch an algorithm for the process of utterance integration in iss which leads to grounding or clarification\n",
            "INFO:tensorflow:GENERATED SUMMARY: is required to try find values for these for these references . the protocol involves the assumption that an agent always initially tries to integrate an utterance by assuming it constitutes an adjacency pair with the existing . these it would be discussing several options be a set of sets whose first level elements are the immediate constituents . the protocol involves the assumption that an agent always initially tries to integrate an utterance by assuming it constitutes an adjacency pair with the existing . these it would be discussing several options elements are the immediate constituents . the protocol\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  still , many compound nouns are not encoded as wordnet entries , and need to be recognized as a single nominal . , all verb predicates have three arguments : __action/state/eventpredicate__ , where : x3 ) . different parsing methods of dictionary definitions were used : patternmatching , genus disambiguation , especially constructed definition parsers or broad coverage parsers , , . we are able to match a few predicates : similar to conjunction predicates , the nn predicates can have a variable number of arguments , with the first one representing the result of the aggregation of the nouns corresponding to the rest of the arguments . the goal of this research project is to transform all the wordnet glosses into logic representations that enables reasoning mechanisms for many practical applications . the first argument represents the & ; result & ; of the logical operation induced by the conjunction . fix __slotallocation__ in the spirit of the davidsonian treatment of the action predicates in a successful unification the arguments of question predicate will be bound to the arguments of answer predicate and the qlf and __alf__ updated to reflect the new status of arguments . hobbs explains that for many linguistic applications it is acceptable to relax ontological __scruples__ , intricate syntactic explanations , and the desire for efficient deductions in favor of a simpler notation closer to english . since in wordnet glosses not many verbs have indirect objects , the argument x3 is used only when necessary , otherwise is __ommited__ . the second technique consists of rearranging the parse trees so that more complex structures are reduced to simpler ones . this decision is based on our desire to provide manageable and consistent logic representation that otherwise would be unfeasible . example 2 consider the trec9 's question : __q481__ : who shot billy the kid ? we have not noticed that these simplifications had any adverse effect on the trec questions . we have presented here a procedure to transform wordnet glosses into logic forms . the preposition predicates always have two arguments : the new qlf to be proven contains only the predicate die . predicates a predicate is generated for every noun , verb , adjective or adverb encountered in any gloss . the way of doing this was first devised in tacitus , when the predicate nn was first introduced . table 2 shows some examples of preposition predicates . conjunctions conjunctions are transformed in predicates , which enable the aggregation of several predicates under the same syntactic role . in this paper we limit the discussion to the definitions and ignore the gloss examples . this paper presents a simple but consistent logic notation suitable for representing the english texts of the wordnet glosses . the first argument corresponding to the predicate of the head of the phrase to which prepositional phrase is attached , whereas the second argument corresponds to the prepositional object . it is well understood and agreed that world knowledge is necessary for many common sense reasoning problems . then step 3 is derived using : kill cause die . consider this extra knowledge is found in wordnet glosses . complex nominals __<s>__ wordnet is a rich source of world knowledge from which formal axioms can be derived __</s>__ __<s>__ in this paper we present a method for transforming the wordnet glosses into logic forms and further into axioms __</s>__ __<s>__ the transformation of wordnet glosses into logic forms is useful for theorem proving and other applications __</s>__ __<s>__ the paper demonstrates the utility of the wordnet axioms in a question answering system to rank and extract answers __</s>__ __<s>__ wordnet is a rich source of world knowledge from which formal axioms can be derived __</s>__ __<s>__ in this paper we present a method for transforming the wordnet glosses into logic forms and further into axioms __</s>__ __<s>__ the transformation of wordnet glosses into logic forms is useful for theorem proving and other applications __</s>__ __<s>__ the paper demonstrates the utility of the wordnet axioms in a question answering system to rank and extract answers __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: wordnet is a rich source of world knowledge from which formal axioms can be derived in this paper we present a method for transforming the wordnet glosses into logic forms and further into axioms the transformation of wordnet glosses into logic forms is useful for theorem proving and other applications the paper demonstrates the utility of the wordnet axioms in a question answering system to rank and extract answers\n",
            "INFO:tensorflow:GENERATED SUMMARY: verbs have indirect objects , the argument x3 is used only when necessary , otherwise is used . the second technique consists of rearranging the parse trees so that more complex structures are reduced to simpler ones . this decision is based on our desire to provide manageable the spirit of the davidsonian treatment of the action\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  while it is possible to transcribe the continuous stream of audio data without any prior segmentation , partitioning offers several advantages over this straightforward solution . stemming is used to reduce the number of lexical items for a given word sense . the result of the partitioning process is a set of speech segments usually corresponding to speaker turns with speaker , gender and telephone a histogram of the __2096__ sections is shown in figure 4. the information retrieval system relies on a unigram model per story . the score of a combined document is set to maximum score of any one of the components . the system has two main components , the speech transcription component and the information retrieval component . finally , eliminating nonspeech segments substantially reduces the computation time . our transcription systems for french and german have comparable error rates for news broadcasts . the acoustic and language models are trained on large , representative corpora for each task and language . as shown in figure 1 the limsi broadcast news transcription system for automatic indexation consists of an audio __partitioner__ and a speech recognizer . using an a priori acoustic segmentation , the mean average precision is significantly reduced compared to a “ perfect ” manual segmentation , whereas the windowbased search engine results are much closer . the information retrieval results are given in terms of mean average precision , as is done for the trec benchmarks in table 1 with and without query expansion . of the __2096__ sections manually marked as reports , 40 % start without a manually annotated speaker change . although it is usually assumed that processing time is not a major issue since computer power has been increasing continuously , it is also known that the amount of data appearing on information channels is increasing at a close rate . since there are many stories significantly shorter than 30s in broadcast shows we __conjunctured__ that it may be of interest to use a double windowing system in order to better target short stories . transcription word error rates of about 20 % have been reported for unrestricted broadcast news data in several languages . in particular the project addresses the development of generic speech recognition technology and methods to rapidly port technology to new domains and languages with limited supervision , and to produce enriched symbolic speech transcriptions . this research has been carried out in a multilingual environment in the context of several recent and ongoing european projects . the stemming lexicon contains about __32000__ entries and was constructed using porter ’ s algorithm on the most frequent words in the collection , and then manually corrected . as proposed in , we segment the audio stream into overlapping documents of a fixed duration . as a result of optimization , we chose a 30 second window duration with a 15 second overlap . this information can be used both directly and indirectly for indexation and retrieval purposes . automatic speech recognition is a key technology for audio and video indexing . __<s>__ this paper addresses recent progress in speakerindependent large vocabulary continuous speech recognition which has opened up a wide range of near and midterm applications __</s>__ __<s>__ one rapidly expanding application area is the processing of broadcast audio for information access __</s>__ __<s>__ at limsi broadcast news transcription systems have been developed for english french german mandarin and portuguese and systems for other languages are under development __</s>__ __<s>__ audio indexation must take into account the specificities of audio data such as needing to deal with the continuous data stream and an imperfect word transcription __</s>__ __<s>__ some nearterm applications areas are audio data mining selective dissemination of information and media monitoring __</s>__ __<s>__ this paper addresses recent progress in speakerindependent large vocabulary continuous speech recognition which has opened up a wide range of near and midterm applications __</s>__ __<s>__ one rapidly expanding application area is the processing of broadcast audio for information access __</s>__ __<s>__ at limsi broadcast news transcription systems have been developed for english french german mandarin and portuguese and systems for other languages are under development __</s>__ __<s>__ audio indexation must take into account the specificities of audio data such as needing to deal with the continuous data stream and an imperfect word transcription __</s>__ __<s>__ some nearterm applications areas are audio data mining selective dissemination of information and media monitoring __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper addresses recent progress in speakerindependent large vocabulary continuous speech recognition which has opened up a wide range of near and midterm applications one rapidly expanding application area is the processing of broadcast audio for information access at limsi broadcast news transcription systems have been developed for english french german mandarin and portuguese and systems for other languages are under development audio indexation must take into account the specificities of audio data such as needing to deal with the continuous data stream and an imperfect word transcription some nearterm applications areas are audio data mining selective dissemination of information and media monitoring\n",
            "INFO:tensorflow:GENERATED SUMMARY: power manually marked as reports , 40 % start without a manually annotated speaker change . although it is usually assumed that processing time is not a major issue since computer power has been increasing continuously , it is usually assumed that processing time is not a major issue since computer power has been increasing continuously , it is possible to transcribe the continuous stream of audio data without any prior segmentation , partitioning offers several advantages over this straightforward solution . stemming is possible to transcribe the continuous stream of audio data without any prior segmentation , partitioning offers\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  their approach also raises a number of interesting followup questions , some concerned with problem detection , others with the use of machine learning techniques . the current results show that for online detection of communication problems at the utterance level it is already beneficial to pay attention only to the lexical information in the word graph and the sequence of system question types , features which are present in most spoken dialogue system and which can be obtained with little or no computational overhead . communication problems are generally easy to label since the spoken dialogue system under consideration here always provides direct feedback about what it believes the user intends . this latter feature is the one to be predicted . one important property of many other rules is that they explicitly combine pieces of information from the three main sources of information . finally , we agree with walker et al . table 1 summarizes the baselines . u this strategy has an accuracy of 58.2 % , and a recall of 0 % . rules are induced per class . in sum , it is uncertain whether other spoken dialogue systems can benefit from the findings described by walker et al . , since it is unclear which features are important and to what extent these features are available in other spoken dialogue systems . walker et al . for instance , krahmer et al . , in their descriptive analysis of dialogue problems , found that repeated material is often an indication of problems , as is the use of a marked vocabulary . this shows that spoken dialogue systems may use these features to better predict whether the ongoing dialogue is problematic . a word graph is a lattice of word hypotheses , and we conjecture that various features which have been shown to cue communication problems have correlates in the word graph . o open questions i implicit verification see the best result is obtained by taking the two most recent word graphs and the six most recent system question types as input . : this accuracy result is significantly better than the ib1ig result given in table 2 for this particular task , with .001. here , for the sake of generality , we abstract over such differences and simply represent a word graph as a bag of words , collecting all words that occur in one of the paths , irrespective of the associated acoustic confidence score . different systems determine the plausibility of paths in the word graph in different ways . the best result is obtained using all features : communication problems are detected with an accuracy of 86 % , a precision of 83 % and a recall of 75 % . these features are present in many spoken dialogue systems and do not require additional computation , which makes this a very cheap method to detect problems . a lexicon was derived of all the words and phrases that occurred in the corpus . __<s>__ we address the issue of online detection of communication problems in spoken dialogue systems __</s>__ __<s>__ the usefulness is investigated of the sequence of system question types and the word graphs corresponding to the respective user utterances __</s>__ __<s>__ by applying both ruleinduction and memorybased learning techniques to data obtained with a dutch train timetable information system the current paper demonstrates that the aforementioned features indeed lead to a method for problem detection that performs significantly above baseline __</s>__ __<s>__ the results are interesting from a dialogue perspective since they employ features that are present in the majority of spoken dialogue systems and can be obtained with little or no computational overhead __</s>__ __<s>__ the results are interesting from a machine learning perspective since they show that the rulebased method performs significantly better than the memorybased method because the former is better capable of representing interactions between features __</s>__ __<s>__ we address the issue of online detection of communication problems in spoken dialogue systems __</s>__ __<s>__ the usefulness is investigated of the sequence of system question types and the word graphs corresponding to the respective user utterances __</s>__ __<s>__ by applying both ruleinduction and memorybased learning techniques to data obtained with a dutch train timetable information system the current paper demonstrates that the aforementioned features indeed lead to a method for problem detection that performs significantly above baseline __</s>__ __<s>__ the results are interesting from a dialogue perspective since they employ features that are present in the majority of spoken dialogue systems and can be obtained with little or no computational overhead __</s>__ __<s>__ the results are interesting from a machine learning perspective since they show that the rulebased method performs significantly better than the memorybased method because the former is better capable of representing interactions between features __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we address the issue of online detection of communication problems in spoken dialogue systems the usefulness is investigated of the sequence of system question types and the word graphs corresponding to the respective user utterances by applying both ruleinduction and memorybased learning techniques to data obtained with a dutch train timetable information system the current paper demonstrates that the aforementioned features indeed lead to a method for problem detection that performs significantly above baseline the results are interesting from a dialogue perspective since they employ features that are present in the majority of spoken dialogue systems and can be obtained with little or no computational overhead the results are interesting from a machine learning perspective since they show that the rulebased method performs significantly better than the memorybased method because the former is better capable of representing interactions between features\n",
            "INFO:tensorflow:GENERATED SUMMARY: their only to the lexical information the word graph interesting followup questions , some concerned with problem detection , others with the use of machine learning techniques . the current results show that for online detection of communication problems at a number of interesting followup questions , some concerned with problem detection , others with the use of machine learning techniques . the current results show that for online detection of communication problems at the utterance level it is already beneficial to pay attention only to the lexical information\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  this is called a shared forest of parse trees , because it can represent an exponential number of possible parses using a polynomial number of nodes which are shared between alternative analyses , and can be constructed and traversed in time of the same complexity . each node in this forest is then annotated with the set of objects and intervals that it could refer to in the environment . for example , adjectives , prepositional phrases , and relative clauses , which are typically represented as __situationallydependent__ properties will be extremely large until composed with modifiers and arguments . the average reduction in number of possible parse trees due to the __environmentbased__ filtering mechanism described above was for successfully parsed and filtered __forests.7__ it incorporates ideas from type theory to represent a broad range of linguistic phenomena in a manner for which their extensions or potential referents in the environment are welldefined in every case . but since there is an interval corresponding to a __draining__ process at the root , the whole vp will still be preferred as constituent due to the other interpretation . in order to evaluate the possible interpretations of a sentence , as described in the previous section , an interface needs to define referent sets for every possible __constituent.2__ the proposed solution draws on a theory of constituent types from formal linguistic semantics , in which constituents such as nouns and verb phrases are represented as __composeable__ functions that take __entitiess__ or situations as inputs and ultimately return a truth value for the sentence . if there is a button next to an adapter , but no handle next to an adapter , the tree representing ‘ handle beside adapter ’ as a constituent may be dispreferred in disambiguation , but the np constituent at the root is still preferred because it has potential referents in the environment due to the other interpretation . in this preliminary accuracy test , forest nodes that correspond to noun phrase or modifier categories are dispreferred if they have no potential entity referents , and forest nodes corresponding to other categories are dispreferred if their arguments have no potential entity referents . a record of the derivation paths in any dynamic programming recognition algorithm or earley ) can be interpreted as a polynomial sized __andor__ graph with space complexity equal to the time complexity of recognition , whose disjunctive nodes represent possible constituents in the analysis , and whose conjunctive nodes represent binary applications of rules in the grammar . fortunately , since the logical function forest shares structure between alternative analyses , many of the sets of potential referents can be shared between analyses during evaluation as well . existing __environmentbased__ methods only calculate the referents of noun phrases , so they only consult the objects in an environment database when interpreting input sentences . this scarcity may be a result of the significant computational complexity of evaluating them in isolation . since there were no testing intervals at __3:00__ in the environment , the referent set for the np ‘ test after __3:00__ ’ is evaluated to the null set . __<s>__ the standard pipeline approach to semantic processing in which sentences are morphologically and syntactically resolved to a single tree before they are interpreted is a poor fit for applications such as natural language interfaces __</s>__ __<s>__ this is because the environment information in the form of the objects and events in the application’s runtime environment cannot be used to inform parsing decisions unless the input sentence is semantically analyzed but this does not occur until after parsing in the __singletree__ semantic architecture __</s>__ __<s>__ this paper describes the computational properties of an alternative architecture in which semantic analysis is performed on all possible interpretations during parsing in polynomial time __</s>__ __<s>__ the standard pipeline approach to semantic processing in which sentences are morphologically and syntactically resolved to a single tree before they are interpreted is a poor fit for applications such as natural language interfaces __</s>__ __<s>__ this is because the environment information in the form of the objects and events in the application’s runtime environment cannot be used to inform parsing decisions unless the input sentence is semantically analyzed but this does not occur until after parsing in the __singletree__ semantic architecture __</s>__ __<s>__ this paper describes the computational properties of an alternative architecture in which semantic analysis is performed on all possible interpretations during parsing in polynomial time __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: the standard pipeline approach to semantic processing in which sentences are morphologically and syntactically resolved to a single tree before they are interpreted is a poor fit for applications such as natural language interfaces this is because the environment information in the form of the objects and events in the application’s runtime environment cannot be used to inform parsing decisions unless the input sentence is semantically analyzed but this does not occur until after parsing in the !!__singletree__!! semantic architecture this paper describes the computational properties of an alternative architecture in which semantic analysis is performed on all possible interpretations during parsing in polynomial time\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . information a shared forest of parse trees , because it can represent an exponential number of possible parses using a polynomial number of nodes which are shared between alternative analyses , can be constructed traversed traversed time of the same complexity . each node it can represent an exponential number of possible parses using a polynomial number of nodes which are shared between alternative analyses , can be constructed traversed traversed time of the same complexity . each node it can represent an exponential number of possible parses using a polynomial number of nodes\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  while it is possible to transcribe the continuous stream of audio data without any prior segmentation , partitioning offers several advantages over this straightforward solution . stemming is used to reduce the number of lexical items for a given word sense . the result of the partitioning process is a set of speech segments usually corresponding to speaker turns with speaker , gender and telephone a histogram of the __2096__ sections is shown in figure 4. the information retrieval system relies on a unigram model per story . the score of a combined document is set to maximum score of any one of the components . the system has two main components , the speech transcription component and the information retrieval component . finally , eliminating nonspeech segments substantially reduces the computation time . our transcription systems for french and german have comparable error rates for news broadcasts . the acoustic and language models are trained on large , representative corpora for each task and language . as shown in figure 1 the limsi broadcast news transcription system for automatic indexation consists of an audio __partitioner__ and a speech recognizer . using an a priori acoustic segmentation , the mean average precision is significantly reduced compared to a “ perfect ” manual segmentation , whereas the windowbased search engine results are much closer . the information retrieval results are given in terms of mean average precision , as is done for the trec benchmarks in table 1 with and without query expansion . of the __2096__ sections manually marked as reports , 40 % start without a manually annotated speaker change . although it is usually assumed that processing time is not a major issue since computer power has been increasing continuously , it is also known that the amount of data appearing on information channels is increasing at a close rate . since there are many stories significantly shorter than 30s in broadcast shows we __conjunctured__ that it may be of interest to use a double windowing system in order to better target short stories . transcription word error rates of about 20 % have been reported for unrestricted broadcast news data in several languages . in particular the project addresses the development of generic speech recognition technology and methods to rapidly port technology to new domains and languages with limited supervision , and to produce enriched symbolic speech transcriptions . this research has been carried out in a multilingual environment in the context of several recent and ongoing european projects . the stemming lexicon contains about __32000__ entries and was constructed using porter ’ s algorithm on the most frequent words in the collection , and then manually corrected . as proposed in , we segment the audio stream into overlapping documents of a fixed duration . as a result of optimization , we chose a 30 second window duration with a 15 second overlap . this information can be used both directly and indirectly for indexation and retrieval purposes . automatic speech recognition is a key technology for audio and video indexing . __<s>__ this paper addresses recent progress in speakerindependent large vocabulary continuous speech recognition which has opened up a wide range of near and midterm applications __</s>__ __<s>__ one rapidly expanding application area is the processing of broadcast audio for information access __</s>__ __<s>__ at limsi broadcast news transcription systems have been developed for english french german mandarin and portuguese and systems for other languages are under development __</s>__ __<s>__ audio indexation must take into account the specificities of audio data such as needing to deal with the continuous data stream and an imperfect word transcription __</s>__ __<s>__ some nearterm applications areas are audio data mining selective dissemination of information and media monitoring __</s>__ __<s>__ this paper addresses recent progress in speakerindependent large vocabulary continuous speech recognition which has opened up a wide range of near and midterm applications __</s>__ __<s>__ one rapidly expanding application area is the processing of broadcast audio for information access __</s>__ __<s>__ at limsi broadcast news transcription systems have been developed for english french german mandarin and portuguese and systems for other languages are under development __</s>__ __<s>__ audio indexation must take into account the specificities of audio data such as needing to deal with the continuous data stream and an imperfect word transcription __</s>__ __<s>__ some nearterm applications areas are audio data mining selective dissemination of information and media monitoring __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper addresses recent progress in speakerindependent large vocabulary continuous speech recognition which has opened up a wide range of near and midterm applications one rapidly expanding application area is the processing of broadcast audio for information access at limsi broadcast news transcription systems have been developed for english french german mandarin and portuguese and systems for other languages are under development audio indexation must take into account the specificities of audio data such as needing to deal with the continuous data stream and an imperfect word transcription some nearterm applications areas are audio data mining selective dissemination of information and media monitoring\n",
            "INFO:tensorflow:GENERATED SUMMARY: power manually marked as reports , 40 % start without a manually annotated speaker change . although it is usually assumed that processing time is not a major issue since computer power has been increasing continuously , it is usually assumed that processing time is not a major issue since computer power has been increasing continuously , it is possible to transcribe the continuous stream of audio data without any prior segmentation , partitioning offers several advantages over this straightforward solution . stemming is possible to transcribe the continuous stream of audio data without any prior segmentation , partitioning offers\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  something s ; somebody s somebody into __ving__ something . but in some cases the probability is 1.0. in our multilingual applications , the grid information provides a contextbased means of associating a verb with a __levin+__ class according to its usage in the sl sentence . because a word sense was assigned even if only one coder judged it to apply , human coding has been treated as having a precision of 100 % . this probability is the prior probability of specific wordnet verb senses . when the errors of multiple classifiers are not significantly correlated , the result of combining votes from a set of individual classifiers often outperforms the best result from any single classifier . fourth , whereas a single word sense for each token in a text corpus is often assumed , the absence of sentential context leads to a situation where several wordnet senses may be equally appropriate for a database entry . , where is the occurrence of the entire grid for verb entry and table 1 illustrates the relation between __levin+__ classes and wordnet for the verb drop . some of the assignments were in doubt , since class splitting had occurred subsequent to those assignments , with all old wordnet senses carried over to new subclasses . seven semantic relationship types exist between synsets , including , for example , antonymy , hyperonymy , and entailment . 2 gives recall and precision measures for all variations of this voting scheme , both with and without enforcement of the __samesynset__ assumption . much , but not all , of this mapping was accomplished manually . on the one hand , even the higher of the kappa coefficients mentioned above is significantly lower than the standard suggested for good reliability or even the level where tentative conclusions may be drawn , . there are 48 grid classes , with a onetomany relationship between grid and __levin+__ classes . this suggests that the precision of the human coding is approximately 87 % . our mapping operation uses several other data elements pertaining to wordnet : semantic relationships between synsets , frequency data , and syntactic information . voting first occurs within the group , and the group ’ s vote is brought forward with a weight equaling the sum of the group members ’ weights . all words in the lexical database “ text ” are disambiguated , not just a small number for which detailed knowledge is available . where __☆__ __★__ , is an occurrence of tag in semcor and is an occurrence of any of a set of tags for verb in semcor , with being one of the senses possible for verb . extrapolating from this sample to the full set of solo judgments in the database leads to an estimate that approximately __1725__ of those senses are incorrect . no consensus was reached on the remaining 6 instances . this suggests that the grid and __levin+__ probabilities could profitably be dropped from further use . computed values generally range between __.25__ our mapping of verbs in __levin+__ classes to wordnet senses relies in part on the relation between thematic roles in __levin+__ and verb frames in wordnet . values average __<s>__ this paper describes automatic techniques for mapping __9611__ entries in a database of english verbs to wordnet senses __</s>__ __<s>__ the verbs were initially grouped into 491 classes based on syntactic features __</s>__ __<s>__ mapping these verbs into wordnet senses provides a resource that supports disambiguation in multilingual applications such as machine translation and crosslanguage information retrieval __</s>__ __<s>__ our techniques make use of a training set of __1791__ disambiguated entries representing __1442__ verb entries from 167 classes __</s>__ __<s>__ this paper describes automatic techniques for mapping __9611__ entries in a database of english verbs to wordnet senses __</s>__ __<s>__ the verbs were initially grouped into 491 classes based on syntactic features __</s>__ __<s>__ mapping these verbs into wordnet senses provides a resource that supports disambiguation in multilingual applications such as machine translation and crosslanguage information retrieval __</s>__ __<s>__ our techniques make use of a training set of __1791__ disambiguated entries representing __1442__ verb entries from 167 classes __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper describes automatic techniques for mapping !!__9611__!! entries in a database of english verbs to wordnet senses the verbs were initially grouped into 491 classes based on syntactic features mapping these verbs into wordnet senses provides a resource that supports disambiguation in multilingual applications such as machine translation and crosslanguage information retrieval our techniques make use of a training set of !!__1791__!! disambiguated entries representing !!__1442__!! verb entries from 167 classes\n",
            "INFO:tensorflow:GENERATED SUMMARY: s ; somebody s ; somebody s somebody into ; somebody s somebody s somebody s ; somebody s somebody s ; somebody s somebody s ; somebody s somebody s ; somebody s somebody s ; somebody s somebody s ; somebody s somebody s ; somebody s somebody into\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  structures the user was attempting . we have undertaken a series of studies to establish an __orderofacquisition__ model for our learner population , native users of american sign language . the model we are developing is called slalom . other data which is more relevant to our goals also presents itself . the theory is that one expects to find errors on elements currently being acquired ; thus errors made by early learners and not by more advanced learners represent structures which the early learners are working on but which the advanced learners have acquired . another problem which additional samples will help to solve is sparseness of data . as icicle is intended to be used by an individual over time and across many pieces of writing , the cycle will be repeated with the same individual many times . in order for icicle to deliver relevant instruction , it needs to determine which of these possibilities most likely reflects the actual performance of the student . we are therefore currently developing a user model to address the system ’ s need to make these parse selections intelligently and to adapt tutoring choices to the individual . we have proposed that slalom be structured in such a way as to capture these expectations by explicitly representing the relationships between grammatical structures in terms of when they are acquired ; namely , indicating which features are typically acquired before other features , and which are typically acquired at the same time . “ she is taught piano on __tuesdays.__ ” although our samples were scored on the __sixpoint__ twe scale , we had sparse data at either end of the scale , so we concentrated our efforts on the three middle levels , which we renamed low , middle , and high . although we experimented in this work with __equalizing__ the error counts using different length measures , we did not have access to the numbers that would have provided the most meaningful normalization : namely , the number of times a structure is attempted . if two of the three levels are both marked , it means that they both committed the error more frequently than the third , but the difference between those two levels was unremarkable . intermediate : missing appropriate __+ing__ morphology . advanced : __botched__ attempt at passive formation . the low and middle levels are insufficiently distinguished from each other , and there were very few errors committed most often by the highest level . the system determines the grammatical errors in the writing , and responds with tutorial feedback aimed at enabling the student to perform corrections . in our first efforts , we have been guided by the observation that the errors committed by learners at different stages of acquisition are clues to the natural order that acquisition follows . all of this begins to draw a picture of the sequence in which these structures are mastered across these levels . a large percentage of the deaf population has reading/writing proficiency levels significantly below their hearing peers , and yet the population represents a broad range of ability . this work has been supported by nsf grants # __ger9354869__ and # __iis9978021__ . __<s>__ in this paper we discuss our approach toward establishing a model of the acquisition of english grammatical structures by users of our english language tutoring system which has been designed for deaf users of american sign language __</s>__ __<s>__ we explore the correlation between a corpus of errortagged texts and their holistic proficiency scores assigned by experts in order to draw initial conclusions about what language errors typically occur at different levels of proficiency in this population __</s>__ __<s>__ since errors made at lower levels presumably represent constructions acquired before those on which errors are found only at higher levels this should provide insight into the order of acquisition of english grammatical forms __</s>__ __<s>__ in this paper we discuss our approach toward establishing a model of the acquisition of english grammatical structures by users of our english language tutoring system which has been designed for deaf users of american sign language __</s>__ __<s>__ we explore the correlation between a corpus of errortagged texts and their holistic proficiency scores assigned by experts in order to draw initial conclusions about what language errors typically occur at different levels of proficiency in this population __</s>__ __<s>__ since errors made at lower levels presumably represent constructions acquired before those on which errors are found only at higher levels this should provide insight into the order of acquisition of english grammatical forms __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in this paper we discuss our approach toward establishing a model of the acquisition of english grammatical structures by users of our english language tutoring system which has been designed for deaf users of american sign language we explore the correlation between a corpus of errortagged texts and their holistic proficiency scores assigned by experts in order to draw initial conclusions about what language errors typically occur at different levels of proficiency in this population since errors made at lower levels presumably represent constructions acquired before those on which errors are found only at higher levels this should provide insight into the order of acquisition of english grammatical forms\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a series elements currently being a series of studies sparse data at either end of the scale , so we concentrated our efforts a way as to capture these expectations by explicitly representing the relationships between grammatical structures across many pieces of writing , the cycle will be repeated with the same individual many\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  in japanese , interword spacing is rarely used . our __rg+dt__ system generates a recognition rule from each ne in the training data . * : __locationname__ , shi if the word is a proper noun or a number , its character type is not replaced by ‘ * ’ . : __locationname__ , if the person ’ s name is composed of only one word , it is classified as __personsingle__ . we generate the word boundary rewriting rules as follows . once the refined rules are generated , we can apply them to a new document . * : __orgname__ > __organization,0,0__ since __yokohama__ honda and kyoto sony also follow this pattern , the second element training time was about 3 minutes on a pentium iii 866 mhz 256 mb memory linux machine . in japanese , there is no such useful hint . the third rule extracts __yokohamaginkou__ as an organization . the __rg+dt__ system attained __84.10__ % for , __84.02__ % for , and __84.03__ % for . accordingly , __shinai__ is classified as __locationend__ . irex ) was held in 1999 , and fifteen systems participated in the formal run of the japanese ne __excercise__ . we use 17 character types for words , e.g. , __singlekanji__ , __allkanji__ , __allkatakana__ , __alluppercase__ , float , __smallinteger__ . * : in this case , we get the following recognition rule . by following uchimoto , we disregard words that appear fewer than five times and other features that appear fewer than three times . and __ioc__ is an unknown word the character type of a oneword ne gives a useful hint for its classification . thus , the last word of an ne is often a head that is more useful than other words for the classification . for instance , we get the following rule when a person ’ s name is incorrectly tagged as a location ’ s name by a pos tagger . accordingly , the decision tree systems did not directly use words as features . first , we compare their starting points and select the earliest ones . by adding inhouse data , the proposed system ’ s performance was improved by several points , while a standard me toolkit crashed . we also replace numbers by dummy constants because most numerical nes follow typical patterns , and their specific values are often useless for ne recognition . the second approach employs a statistical method , which is expected to be more robust and to require less human intervention . now , we obtain the following recognition rules from the above examples . * : __alluppercase__ : __miscpropernoun__ > __organization,0,0__ . for instance , __oosakawan__ follows this pattern , but it is a location ’ s name . table 1 shows the details . it is relatively easy to detect english nes because of capitalization . suffixes of numerical nes are not replaced , either . therefore , we do not restrict proper nouns by a suffix dictionary , and we do not restrict numbers either . ’ s toolkit crashes even on a 2 gb memory machine . recall = for instance , a morphological analyzer may divide a __fourcharacter__ expression __oosakashinai__ into two words __oosaka__ and __shinai__ , but the training data would be tagged as < location > __oosakashi__ < __/location__ > nai . all of the training data were based on the mainichi newspaper ’ s 1994 and 1995 __cdroms__ . __borthwick__ ’ s and uchimoto ’ s __<s>__ named entity recognition is a task in which proper nouns and numerical information in a document are detected and classified into categories such as person organization location and date __</s>__ __<s>__ ne recognition plays an essential role in information extraction systems and question answering systems __</s>__ __<s>__ it is well known that handcrafted systems with a large set of heuristic rules are difficult to maintain and corpusbased statistical approaches are expected to be more robust and require less human intervention __</s>__ __<s>__ several statistical approaches have been reported in the literature __</s>__ __<s>__ in a recent japanese ne workshop a maximum entropy system outperformed decision tree systems and most handcrafted systems __</s>__ __<s>__ here we propose an alternative method based on a simple rule generator and decision tree learning __</s>__ __<s>__ our experiments show that its performance is comparable to the me approach __</s>__ __<s>__ we also found that it can be trained more efficiently with a large set of training data and that it improves readability __</s>__ __<s>__ named entity recognition is a task in which proper nouns and numerical information in a document are detected and classified into categories such as person organization location and date __</s>__ __<s>__ ne recognition plays an essential role in information extraction systems and question answering systems __</s>__ __<s>__ it is well known that handcrafted systems with a large set of heuristic rules are difficult to maintain and corpusbased statistical approaches are expected to be more robust and require less human intervention __</s>__ __<s>__ several statistical approaches have been reported in the literature __</s>__ __<s>__ in a recent japanese ne workshop a maximum entropy system outperformed decision tree systems and most handcrafted systems __</s>__ __<s>__ here we propose an alternative method based on a simple rule generator and decision tree learning __</s>__ __<s>__ our experiments show that its performance is comparable to the me approach __</s>__ __<s>__ we also found that it can be trained more efficiently with a large set of training data and that it improves readability __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: named entity recognition is a task in which proper nouns and numerical information in a document are detected and classified into categories such as person organization location and date ne recognition plays an essential role in information extraction systems and question answering systems it is well known that handcrafted systems with a large set of heuristic rules are difficult to maintain and corpusbased statistical approaches are expected to be more robust and require less human intervention several statistical approaches have been reported in the literature in a recent japanese ne workshop a maximum entropy system outperformed decision tree systems and most handcrafted systems here we propose an alternative method based on a simple rule generator and decision tree learning our experiments show that its performance is comparable to the me approach we also found that it can be trained more efficiently with a large set of training data and that it improves readability\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a generates a recognition rule from each ne the training data . * : shi if the training data . * : shi if the training data . * : shi if the training data . * : shi if the training data . * : shi if the word boundary rewriting rules as follows . once the refined rules are generated , the second element training time was about 3 get the following recognition rule . by following uchimoto , we disregard words that appear fewer than five times . once\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the experimental results reported in this paper are based on a grammar under development at __riacs__ for a spoken dialogue interface to a semiautonomous robot , the personal satellite assistant . the feature structure for each lexical item and grammar rule is rewritten such that singleton variables are unified with a special value ’ any ’ , and every nonsingleton variable expression is embedded in a val term . grammar specification language , which is a form of contextfree grammar in a __bnflike__ notation , with one rule defining each nonterminal , and allowing alternation and kleene closure on the righthandside . as a sideeffect of this compilation , productions are eliminated . as can be readily seen , the compilation time for the k & k algorithm is dramatically lower than the m & g algorithm , while producing a similarly lower recognition performance , measured in both word error rate and recognition speed . additional rules are then introduced to deal with the singleton variable cases . this is in turn converted into a grammar in nuance ’ s this approach has been used in several systems , commandtalk , __rialist__ psa simulator , __witas__ , and __sethivoice__ . if after checking all rule applications bottom up , no new feature structures have been added to , then the least fixedpoint had been found , and the process terminates . for future work we plan to explore possible integrations of these two algorithms . in particular , in order to derive a finite cf grammar , we will need to consider only those features that have a finite number of possible values , or at least consider only finitely many of the possible values for infinitely valued features . n : the computation of the fixedpoint , described in table 1 , proceeds as follows . n : ] the recognition results were obtained on a test set of 250 utterances . np : another technique to reduce ambiguity was motivated by a desire to reduce the amount of prepositional phrase attachment ambiguity in our grammar . note that the approximation need not incorporate all of the features from the original grammar in order to provide a sound approximation . table 2 contains a summary of some key statistics generated using both techniques . a follows b iff now , the follows relation can be used to reduce ambiguity by modifying other productions where then remove the production for . a major problem with grammarbased finitestate or contextfree language models is that they can be tedious to build and difficult to maintain , as they can become quite large very quickly as the scope of the grammar increases . the first step in the instantiation is to unify every combination of daughters with all possible feature structures from . after this transformation , singleton variables will not unify with nonsingleton variable expressions , only with other singletons . since each nonterminal is now ground , it is trivial to assign each nonterminal a unique atomic symbol , and rewrite the grammar as a cfg . while this may sound small , if the grammar were expanded by instantiating variables in all legal permutations , it would contain over contextfree rules . __<s>__ current alternatives for language modeling are statistical techniques based on large amounts of training data and handcrafted contextfree or finitestate grammars that are difficult to build and maintain __</s>__ __<s>__ one way to address the problems of the grammarbased approach is to compile recognition grammars from grammars written in a more expressive formalism __</s>__ __<s>__ while theoretically straightforward the compilation process can exceed memory and time bounds and might not always result in accurate and efficient speech recognition __</s>__ __<s>__ we will describe and evaluate two approaches to this compilation problem __</s>__ __<s>__ we will also describe and evaluate additional techniques to reduce the structural ambiguity of the language model __</s>__ __<s>__ current alternatives for language modeling are statistical techniques based on large amounts of training data and handcrafted contextfree or finitestate grammars that are difficult to build and maintain __</s>__ __<s>__ one way to address the problems of the grammarbased approach is to compile recognition grammars from grammars written in a more expressive formalism __</s>__ __<s>__ while theoretically straightforward the compilation process can exceed memory and time bounds and might not always result in accurate and efficient speech recognition __</s>__ __<s>__ we will describe and evaluate two approaches to this compilation problem __</s>__ __<s>__ we will also describe and evaluate additional techniques to reduce the structural ambiguity of the language model __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: current alternatives for language modeling are statistical techniques based on large amounts of training data and handcrafted contextfree or finitestate grammars that are difficult to build and maintain one way to address the problems of the grammarbased approach is to compile recognition grammars from grammars written in a more expressive formalism while theoretically straightforward the compilation process can exceed memory and time bounds and might not always result in accurate and efficient speech recognition we will describe and evaluate two approaches to this compilation problem we will also describe and evaluate additional techniques to reduce the structural ambiguity of the language model\n",
            "INFO:tensorflow:GENERATED SUMMARY: nuance ’ s this approach has been used or on a grammar under development at a spoken dialogue interface to a semiautonomous robot , the personal satellite assistant . the feature structure for each lexical item a spoken dialogue interface to a semiautonomous robot , the personal satellite assistant . the feature structure for each lexical item a spoken dialogue interface to a semiautonomous robot , the personal satellite assistant . the feature structure for each lexical item a spoken dialogue interface to a semiautonomous robot , the personal satellite assistant . the feature structure for each lexical item grammar\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  in order to simplify the notation we write , and define : , with respect to the distribution . features of ngrams and features of triggers were used in both kinds of models , and the wsme model trained with ps had better performance . the great number of such sentences makes it impossible , from computing perspective , to calculate the sum , even for a moderate __length3__ . in order to define the grammatical features , we first introduce some notation . the corpus was divided into sentences according to the bracketing . the model parameters were completed with the estimation of the global normalization constant . the sum extends over all the sentences of a given length . the goal of the me principle is that , given a set of features , a set of functions and a set of __constraints1__ , we have to find the probability distribution that satisfies the constraints and minimizes the relative entropy where is the normalization constant andare parameters to be found . now , if all the paths collapse at any given time , from that point in time , we are sure that we are sampling from the true target distribution . finally , section 4 presents the experiments carried out using a part of the wall street journal in order __evalute__ the behavior of this proposal . in mcmc , a path of the markov chain is ran for a long time , after which the visited states are considered as a sampling element . the reason for this can be found in the capability of scfgs to model the longterm dependencies established between the different lexical units of a sentence , and the possibility to incorporate the stochastic information that allows for an adequate modeling of the variability phenomena . however , only one of them is here given that the results were very similar . the parameters of the two models are estimated from a training sample . the grammatical information is combined with features of ngrams and triggers . this corpus was automatically labelled and manually checked . and , so , we smooth the model . in section 3 , we define the grammatical features and the way of obtaining them from the scfg . a part of the wall street journal which had been processed in the penn __treebanck__ project was used in the experiments . a formal framework to include longdistance and local information in the same language model is based on the maximum entropy principle . : the first row represents the set of features used . we then considered it appropriate to use ps in the training procedure of the wsme . therefore , a sentence may be a grammatically incorrect sentence , if derivations with low frequency appears . thus if two paths coincide in the same state in time , they will remain in the same states the rest of the time . the training procedure to estimate the parameters of the model is the improved iterative scaling __algorithmn__ . there were two kinds of labelling : postag labelling and syntactic labelling . __<s>__ in this paper we propose adding longterm grammatical information in a whole sentence __maximun__ entropy language model in order to improve the performance of the model __</s>__ __<s>__ the grammatical information was added to the wsme model as features and were obtained from a stochastic contextfree grammar __</s>__ __<s>__ finally experiments using a part of the penn treebank corpus were carried out and significant improvements were __acheived__ __</s>__ __<s>__ where which is usually known as the history of __</s>__ __<s>__ the effort in the language modeling techniques is usually directed to the estimation of __</s>__ __<s>__ the language model defined by the expression is named the conditional language model __</s>__ __<s>__ in principle the determination of the conditional probability in is expensive because the possible number of word sequences is very great __</s>__ __<s>__ traditional conditional language models assume that the probability of the __worddoes__ not depend on the entire history and the history is limited by an equivalence relation and is rewritten as: __</s>__ __<s>__ in this paper we propose adding longterm grammatical information in a whole sentence __maximun__ entropy language model in order to improve the performance of the model __</s>__ __<s>__ the grammatical information was added to the wsme model as features and were obtained from a stochastic contextfree grammar __</s>__ __<s>__ finally experiments using a part of the penn treebank corpus were carried out and significant improvements were __acheived__ __</s>__ __<s>__ where which is usually known as the history of __</s>__ __<s>__ the effort in the language modeling techniques is usually directed to the estimation of __</s>__ __<s>__ the language model defined by the expression is named the conditional language model __</s>__ __<s>__ in principle the determination of the conditional probability in is expensive because the possible number of word sequences is very great __</s>__ __<s>__ traditional conditional language models assume that the probability of the __worddoes__ not depend on the entire history and the history is limited by an equivalence relation and is rewritten as: __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in this paper we propose adding longterm grammatical information in a whole sentence !!__maximun__!! entropy language model in order to improve the performance of the model the grammatical information was added to the wsme model as features and were obtained from a stochastic contextfree grammar finally experiments using a part of the penn treebank corpus were carried out and significant improvements were !!__acheived__!! where which is usually known as the history of the effort in the language modeling techniques is usually directed to the estimation of the language model defined by the expression is named the conditional language model in principle the determination of the conditional probability in is expensive because the possible number of word sequences is very great traditional conditional language models assume that the probability of the !!__worddoes__!! not depend on the entire history and the history is limited by an equivalence relation and is rewritten as:\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a the used of the behavior of the behavior of this proposal to on can is ran the learning of scfgs to model the longterm dependencies established : , with respect to the distribution . features of ngrams features of triggers were used the capability of scfgs to model the longterm dependencies established : , with respect to the distribution . features of ngrams features of triggers were used\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  figure 3 is an example . the functions lhs and rhs map rules to their left hand and right hand sides , respectively . an example is given in figure 4. thus the call to __pfinside__ in line 1 of __pfgovernors__ may involve either a computation of pcfg inside probabilities , or __headlexicalized__ inside probabilities . the governor algorithm annotates parse forest symbols and rules with functions from governor labels to real numbers . and carroll et the algorithm we will define pools governor labels in this way . we believe this is because , for our grammars and corpora , there is limited ambiguity in the position of the head within a given __categoryspan__ combination . is also extended to map trees licensed by the parse forest grammar to trees licensed by the underlying grammar . is defined to be the multiset image of under . in these terms , is the conditional expectation of , conditioned on the yield being . working top down , if fills in an array which is supposed to agree with the quantity defined above . the governor tuple is then replaced by in the definition of the governor label for a terminal vertex . our parser constructs parse forests organized according to span . for a given markup triple , let __termined__ by tree . note that is a vector space , so that expectations and conditional expectations may be defined . the governor labels defined above are derived from the specific symbols of a context free grammar . in this case , just as in the original tree , the label for the fourth terminal position is np , vp , read . given an inside algorithm , the flow may be computed by the flow algorithm in figure 6 , or by the insideoutside algorithm . lopar already provided functions for the computation of the __headmarked__ parse forest , for the flow computation and for traversing the parse forest in depthfirst and __topologicallysorted__ order ) . , the number of tree analyses for a sentence is frequently large , greater than for about 1/10 of the sentences in the british national corpus . the strategy has the advantage , in our view , that it allows one to base markup algorithms on relatively sophisticated grammars , and to take advantage of the lexically sensitive probabilistic weighting of trees which is provided by a lexicalized probability model . see eisner and satta for discussion and an algorithm with time and space requirements proportional to the fourth power of the length of the input sentence in the worst case . the inside algorithm for ordinary pcfgs is given in figure 5. suppose that a probabilistic grammar licenses headed tree analyses for a sentence , and assigns them probabilistic weights . the flow and governor algorithms stated below call an algorithm __pfinside__ which computes inside probabilities in so it was only necessary to add functions for data initialization , for the computation of the governor vector at each node and for printing the result . governor labels for , where is a function giving probability parameters for the underlying grammar . __<s>__ in a headed tree each terminal word can be uniquely labeled with a governing word and grammatical relation __</s>__ __<s>__ this labeling is a summary of a syntactic analysis which eliminates detail reflects aspects of semantics and for some grammatical relations is nearly uncontroversial __</s>__ __<s>__ we define a notion of expected governor markup which sums vectors indexed by governors and scaled by probabilistic tree weights __</s>__ __<s>__ the quantity is computed in a parse forest representation of the set of tree analyses for a given sentence using vector sums and scaling by inside probability and flow __</s>__ __<s>__ in a headed tree each terminal word can be uniquely labeled with a governing word and grammatical relation __</s>__ __<s>__ this labeling is a summary of a syntactic analysis which eliminates detail reflects aspects of semantics and for some grammatical relations is nearly uncontroversial __</s>__ __<s>__ we define a notion of expected governor markup which sums vectors indexed by governors and scaled by probabilistic tree weights __</s>__ __<s>__ the quantity is computed in a parse forest representation of the set of tree analyses for a given sentence using vector sums and scaling by inside probability and flow __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in a headed tree each terminal word can be uniquely labeled with a governing word and grammatical relation this labeling is a summary of a syntactic analysis which eliminates detail reflects aspects of semantics and for some grammatical relations is nearly uncontroversial we define a notion of expected governor markup which sums vectors indexed by governors and scaled by probabilistic tree weights the quantity is computed in a parse forest representation of the set of tree analyses for a given sentence using vector sums and scaling by inside probability and flow\n",
            "INFO:tensorflow:GENERATED SUMMARY: 1 word may involve either a computation of pcfg inside probabilities , or from governor labels to real numbers . the governor algorithm annotates parse forest to their left hand right hand sides , respectively . an example is given figure 4. thus the call to corpora , there is supposed to agree thus the call to corpora , there is supposed to agree thus the call to corpora , there is supposed to agree thus the call to corpora , there is supposed to agree with the quantity defined above . working top down , if fills by the\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  our work differs from traditional qa research in its use of statistical models to predict variables that represent a user ’ s informational goals . precise indicates that an exact answer has been requested , e.g. , a name or date ; additional refers to a level of detail characterized by a __oneparagraph__ answer ; and extended indicates a longer , more detailed answer . a total of __6,436__ questions were tagged by hand . in both cases , the same __1291__ queries were used for testing . figure 2 depicts the average of the results obtained over five runs , while figure 3 shows the results of a single run . these variables represent the topic of discussion , the type of the expected answer , and information that restricts the scope of the answer , respectively . we extracted __97,640__ questions , which constitute about 6 % of the __1,649,404__ queries in the log files collected during a period of three weeks in the year 2000. in section 4 , we discuss our predictive models . further , the predictive accuracy for information need , topic , focus and restriction drops substantially for queries that have 11 words or more . the nlp components of these systems employed handcrafted rules to infer the type of answer expected . the decision trees described in this section are those that yield the best predictive performance . ” is a topic itself query . we extracted a total of 21 structural features , including the number of distinct partsofspeech – nouns , verbs , nps , etc – in a question , whether the main noun is plural or singular , which noun is a proper noun , and the pos of the head verb postmodifier . however , our focus decision tree includes additional attributes in its second split . size of the training set . one approach to the qa task consists of applying the ir methods to retrieve documents relevant to a user ’ s question , and then using the shallow nlp to extract features from both the user ’ s question and the most promising retrieved documents . this difference in performance may be attributed to the fact that the __predictiononly__ model is a “ smoothed ” version of the mixed model . however , since the set of good queries is 10 % smaller , it is considered a better option . these results suggest using modeling techniques which can take advantage of dependencies among target variables . this approach was adopted in . in the next section , we review related research . is an attribute query ; “ how does __lightningform__ ? ” topic , focus and restriction contain a pos in the parse tree of a user ’ s question . the effect of the size of the training set on predictive performance was assessed by considering four sizes of training/test sets : small , medium , large , and __xlarge__ . further , the predictive performance obtained for the set __good4617__ is only slightly better than that obtained for the set __all5145__ . in this paper , we focus on the predictive models , rather than on the provision of answers to users ’ questions . we then evaluate the predictions obtained from models built under different training and modeling conditions . __<s>__ we describe a set of supervised machine learning experiments centering on the construction of statistical models of whquestions __</s>__ __<s>__ these models which are built from shallow linguistic features of questions are employed to predict target variables which represent a user’s informational goals __</s>__ __<s>__ we report on different aspects of the predictive performance of our models including the influence of various training and testing factors on predictive performance and examine the relationships among the target variables __</s>__ __<s>__ we describe a set of supervised machine learning experiments centering on the construction of statistical models of whquestions __</s>__ __<s>__ these models which are built from shallow linguistic features of questions are employed to predict target variables which represent a user’s informational goals __</s>__ __<s>__ we report on different aspects of the predictive performance of our models including the influence of various training and testing factors on predictive performance and examine the relationships among the target variables __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we describe a set of supervised machine learning experiments centering on the construction of statistical models of whquestions these models which are built from shallow linguistic features of questions are employed to predict target variables which represent a user’s informational goals we report on different aspects of the predictive performance of our models including the influence of various training and testing factors on predictive performance and examine the relationships among the target variables\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is proposed from traditional qa research for = from traditional qa research the log files collected during . precise indicates that an exact answer has been requested , e.g. , a name or date ; additional refers to a level of detail characterized by a name or date ; additional refers to a level of detail characterized by a name or date ; additional refers to a level of detail characterized by a name or date ; additional refers to a level of detail characterized by a name or date ; additional refers to a level of\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the next section describes the probability model underlying __incdrop__ and the objective function that results from the model . word tokens . the prior distribution is derived from a __fivestep__ model for the generation of texts . however , it was designed as a computational model of how children segment speech in the course of acquiring their native language , an application that does not permit the use of manually segmented training texts . the test corpus was divided into 16 samples of 1,000 words each so we could assess the variance in performance across samples of a given genre . that maps each position in the text to be generated to the index of the word that will appear in that position . the conditional distribution determines the most probable segmentation of t , according to the model . manually curated linguistic knowledge tends to be more accurate than what can be gleaned by an adaptive learning system , especially when handling relatively rare cases . $ , the reserved sentenceboundary marker . it __bootsraps__ its own dictionary , which is initially empty , using a probability model and viterbistyle optimization algorithm . using these terms , we define precision and recall as follows : true positives recall = this is a deterministic process , so the unique possible outcome has probability 1.0. the __6/72__ term normalizes the sum to one . the first term on the right hand side of is the relative frequency of the word so far , with one added to both the numerator and the denominator . because speech contains no known acoustic marking of word boundaries , children must segment the utterances they hear in order to learn the words of their language . following the procedure used by __teahan__ et al . , we treated each byte as a separate input symbol for both mbdp1 and ppm . considering precision and recall together , it appears that mbdp1 performs much better when the available training corpus is smaller than 212 words , somewhat better when the training corpus is between 212 words and 218 words , and __indistinguishably__ when the training corpus is 220 words . thus , mbdp1 — the algorithm underlying an abstract cognitive model known as __incdrop__ , for incremental distributional regularity optimization — requires neither a dictionary nor a segmented training corpus . this manually segmented corpus is represented in the standard gb coding scheme , which uses two bytes for each chinese character . occasionally , phrases do in fact consist of only one or a few words . all of these methods require either a preexisting dictionary or else a supervised training regimen using a manually segmented corpus . { __t471__ be the resulting lexicon . choose an ordering function s : { 1 , . . . , m } — > { 1 , . , n } the first corpus we used is guo jin 's mandarin chinese ph corpus , containing more than one million words of newspaper stories from the xinhua news agency of pr china written between january , 1990 and march , __1991.__ section 4 reports experiments in which mbdp1 is compared to ppm using the ph corpus of chinese newspaper text and the english portion of the hansard corpus . __<s>__ this paper describes a system for segmenting chinese text into words using the mbdp1 algorithm __</s>__ __<s>__ mbdp1 is a knowledgefree segmentation algorithm that bootstraps its own lexicon which starts out empty __</s>__ __<s>__ experiments on chinese and english corpora show that mbdp1 reliably outperforms the best previous algorithm when the available handsegmented training corpus is small __</s>__ __<s>__ as the size of the handsegmented training corpus grows the performance of mbdp1 converges toward that of the best previous algorithm __</s>__ __<s>__ the fact that mbdp1 can be used with a small corpus is expected to be useful not only for the rare event of adapting to a new language but also for the common event of adapting to a new genre within the same language __</s>__ __<s>__ this paper describes a system for segmenting chinese text into words using the mbdp1 algorithm __</s>__ __<s>__ mbdp1 is a knowledgefree segmentation algorithm that bootstraps its own lexicon which starts out empty __</s>__ __<s>__ experiments on chinese and english corpora show that mbdp1 reliably outperforms the best previous algorithm when the available handsegmented training corpus is small __</s>__ __<s>__ as the size of the handsegmented training corpus grows the performance of mbdp1 converges toward that of the best previous algorithm __</s>__ __<s>__ the fact that mbdp1 can be used with a small corpus is expected to be useful not only for the rare event of adapting to a new language but also for the common event of adapting to a new genre within the same language __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper describes a system for segmenting chinese text into words using the mbdp1 algorithm mbdp1 is a knowledgefree segmentation algorithm that bootstraps its own lexicon which starts out empty experiments on chinese and english corpora show that mbdp1 reliably outperforms the best previous algorithm when the available handsegmented training corpus is small as the size of the handsegmented training corpus grows the performance of mbdp1 converges toward that of the best previous algorithm the fact that mbdp1 can be used with a small corpus is expected to be useful not only for the rare event of adapting to a new language but also for the common event of adapting to a new genre within the same language\n",
            "INFO:tensorflow:GENERATED SUMMARY: the probability model underlying the objective function that results from the model . word tokens . the prior distribution determines the most probable segmentation of how children segment speech the course of acquiring their native language , an application that does not permit the use of manually segmented training texts . the test corpus was divided into 16 samples of 1,000 words each so we could assess the variance performance across samples of a given genre . that maps each position performance across samples of a given genre . that maps each position\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the tagset totalled over 200 tags . this definition entails that l g w and also that a pattern is also a pattern , etc . getting these transcripts is a highly expensive task involving the cooperation and time of nurses and physicians in the busy icu . moreover , as they have fixed length , they tend to be pretty similar . the algorithm proceeds as follows the support of pattern p given a set of sequences s is the number of sequences that contain at least one match of p. furthermore , some tags occur fairly regularly towards either the beginning or the end of the transcript , while others are spread more or less evenly throughout . we can think of these patterns as the basic elements of a plan , representing small clusters of semantic units that are similar in size , for example , to the nucleussatellite pairs of __rst.1__ by learning ordering constraints over these elements , we produce a plan that can be expressed as a constraintsatisfaction problem . constraint confidence . the text was subsequently annotated with semantic tags as shown in figure 1. see figure 4 for an example . we used an adaptation of __teiresias__ . in biological he is __58yearold__ male . generalizing . the last step of our algorithm measures the frequencies of all possible order constraints among pairs of clusters , retaining those that occur often enough to be considered important , according to some relevancy measure . currently , when a patient is brought to the intensive care unit after surgery , one of the residents who was present in the operating room gives a briefing to the icu nurses and residents . as explained in , motif detection is usually targeted with alignment techniques ) or with combinatorial pattern discovery techniques such as the ones we used here . this concept is useful for visualization of the cluster in qualitative evaluation . the resident was equipped with a wearable tape recorder to tape the briefings , which were transcribed to provide the base of our empirical data . given the difficulty for humans in finding patterns systematically in our data , we needed unsupervised techniques such as those developed in computational genomics . we based our unsupervised learning algorithm on techniques used in computational genomics , where from large amounts of seemingly unorganized genetic sequences , patterns representing meaningful biological features are discovered . support . this difference is central to our current research , given that order constraints are our main focus . each of the resulting clusters has a single pattern represented by the centroid of the cluster . these categories are the ones used for our current research . we also discard any constraint that it is violated in any training sequence . in this paper , we focus on learning the plan elements and the ordering constraints between them . * e , where ? in this section , we provide a brief explanation of our pattern discovery methodology . represents a don ’ t care position . they are sets of ordered pairs , where the first position records the sequence number and the second position records the offset in that sequence where p matches . __<s>__ in a language generation system a content planner embodies one or more __“plans”__ that are usually __hand–crafted__ sometimes through manual analysis of target text __</s>__ __<s>__ in this paper we present a system that we developed to automatically learn elements of a plan and the ordering constraints among them __</s>__ __<s>__ as training data we use semantically annotated transcripts of domain experts performing the task our system is designed to mimic __</s>__ __<s>__ given the large degree of variation in the spoken language of the transcripts we developed a novel algorithm to find parallels between transcripts based on techniques used in computational genomics __</s>__ __<s>__ our proposed methodology was evaluated __two–fold:__ the learning and generalization capabilities were quantitatively evaluated using cross validation obtaining a level of accuracy of 89% __</s>__ __<s>__ a qualitative evaluation is also provided __</s>__ __<s>__ in a language generation system a content planner embodies one or more __“plans”__ that are usually __hand–crafted__ sometimes through manual analysis of target text __</s>__ __<s>__ in this paper we present a system that we developed to automatically learn elements of a plan and the ordering constraints among them __</s>__ __<s>__ as training data we use semantically annotated transcripts of domain experts performing the task our system is designed to mimic __</s>__ __<s>__ given the large degree of variation in the spoken language of the transcripts we developed a novel algorithm to find parallels between transcripts based on techniques used in computational genomics __</s>__ __<s>__ our proposed methodology was evaluated __two–fold:__ the learning and generalization capabilities were quantitatively evaluated using cross validation obtaining a level of accuracy of 89% __</s>__ __<s>__ a qualitative evaluation is also provided __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in a language generation system a content planner embodies one or more !!__“plans”__!! that are usually !!__hand–crafted__!! sometimes through manual analysis of target text in this paper we present a system that we developed to automatically learn elements of a plan and the ordering constraints among them as training data we use semantically annotated transcripts of domain experts performing the task our system is designed to mimic given the large degree of variation in the spoken language of the transcripts we developed a novel algorithm to find parallels between transcripts based on techniques used in computational genomics our proposed methodology was evaluated !!__two–fold:__!! the learning and generalization capabilities were quantitatively evaluated using cross validation obtaining a level of accuracy of 89% a qualitative evaluation is also provided\n",
            "INFO:tensorflow:GENERATED SUMMARY: the tagset totalled over 200 tags . this definition entails that l g w the busy icu . moreover , as they have fixed length , they tend to be pretty similar . the algorithm proceeds as follows the support of pattern p given a set of sequences s is the number of sequences that contain at least one match of p. furthermore , some tags occur fairly regularly towards either the busy icu . moreover , as they have fixed length , they tend to be pretty similar . the algorithm proceeds as follows the support of pattern p\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the hierarchy in figure 1.1 , __caroll__ , minnen , and briscoe . document , thereby providing means to associate information at any level of detail or complexity to the annotated structure . the categories are organized in a hierarchy , from general to specific . again , the provision of a standard set of categories , together with the requirement that __schemespecific__ categories at the highest level of abstraction , syntactic annotation schemes represent the following kinds of information : for example , the annotation in figure 1 , drawn from the penn treebank __ii3__ , uses lisplike list structures to specify constituency relations and provide syntactic category labels for constituents . note that in this example , relations are encoded only when they appear explicitly in the original annotation an xslt script could be used to create a second xml document that includes the relations implicit in the embedding . figure 3 shows the overall architecture of the xces framework for syntactic annotation . however , especially for existing formats , it is typically more straightforward to perform the twostep process . structural skeleton : a domaindependent abstract structural framework for syntactic 5 cf . in practice , annotators and users of annotated corpora will rarely see xml and rdf instantiations of annotated data ; rather , they will access the data via interfaces that automatically generate , interpret , and display the data in easytoread formats . dialect specification : defines , using xml schemas , xslt scripts , and xsl style sheets , the projectspecific xml format for syntactic annotations . we accomplish this by treating the description of any specific syntactic annotation scheme as a process involving several knowledge sources that interact at various levels . it has also been noted that that the parseval __bracketprecision__ measure penalizes parsers that return more structure than exists in the relatively flat treebank structures , even if they are correct . figure 4 shows the annotation from the ptb rendered in the abstract xml format . __etc.5__ note that rdf descriptions function much like class definitions in an objectoriented programming language the hierarchy of < struct > elements corresponds to the nodes in a phrase structure analysis ; each < struct conversely , the grammar rules implicit in annotated treebanks , which are typically not annotated according to a formal grammar , can be easily extracted from the abstract structural encoding . > element is typed accordingly . the framework has been applied to the representation of terminology and computational lexicons , thus demonstrating its general applicability for a variety of linguistic annotation types . 4 socalled hybrid systems combine constituency analysis and functional dependencies , usually producing a shallow constituent parse that brackets major phrase types and identifying the dependencies between heads of constituents . it is widely recognized that the proliferation of annotation schemes runs counter to the need to reuse language resources , and that standards for linguistic annotation are becoming increasingly mandatory . to answer this need , we have developed a representation framework comprised of an abstract model for a variety of different annotation types , which can be instantiated in different ways depending on the annotator __<s>__ it is widely recognized that the proliferation of annotation schemes runs counter to the need to reuse language resources and that standards for linguistic annotation are becoming increasingly mandatory __</s>__ __<s>__ to answer this need we have developed a representation framework comprised of an abstract model for a variety of different annotation types which can be instantiated in different ways depending on the annotator __❑s__ approach and goals __</s>__ __<s>__ in this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation __</s>__ __<s>__ we show how the framework can contribute to comparative evaluation and merging of parser output and diverse syntactic annotation schemes __</s>__ __<s>__ it is widely recognized that the proliferation of annotation schemes runs counter to the need to reuse language resources and that standards for linguistic annotation are becoming increasingly mandatory __</s>__ __<s>__ to answer this need we have developed a representation framework comprised of an abstract model for a variety of different annotation types which can be instantiated in different ways depending on the annotator __❑s__ approach and goals __</s>__ __<s>__ in this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation __</s>__ __<s>__ we show how the framework can contribute to comparative evaluation and merging of parser output and diverse syntactic annotation schemes __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: it is widely recognized that the proliferation of annotation schemes runs counter to the need to reuse language resources and that standards for linguistic annotation are becoming increasingly mandatory to answer this need we have developed a representation framework comprised of an abstract model for a variety of different annotation types which can be instantiated in different ways depending on the annotator !!__❑s__!! approach and goals in this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation we show how the framework can contribute to comparative evaluation and merging of parser output and diverse syntactic annotation schemes\n",
            "INFO:tensorflow:GENERATED SUMMARY: 1 , drawn from the penn treebank , thereby providing means to associate information at any level of detail or complexity to the annotated structure . the categories are organized figure 1 , drawn from the penn treebank , thereby providing means to associate information at any level of detail or complexity to the annotated structure . the categories are organized\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  see or for a detailed discussion of this translation model and a description of its parameters . how did this all come about ? we extracted only tuples in which the english and french phrases contained at least two words . tables 4 and 5 show that the translation memories significantly help the decoder find translations of high probability . we decided to use such harsh evaluation criteria because , in previous experiments , we repeatedly found that harsh criteria can be applied consistently . correct we selected only “ contiguous ” alignments , i.e. , alignments in which the words in the english phrase generated only words in the french phrase and each word in the french phrase was generated either by the null word or a word from the english phrase . it is clear that ebmt and smt systems have different strengths and weaknesses . to do this , one would simply have to train the statistical model on the translation memory provided as input , determine the viterbi alignments , and enhance the existing translation memory with wordlevel alignments as produced by the statistical translation model . for example , “ autres __r´egions__ de le pays que ” and “ other parts of canada than ” were judged as incorrect . each english word __eis__ then translated with probability t __einto__ a french word , where ranges from 1 to the number of words into which __eis__ translated . for example , in translating the french sentence “ bien __entendu__ , il __parle__ de une __belle__ __victoire__ . ” , the greedy decoder initially assumes that a good translation of it is “ well heard , it talking a beautiful victory ” because the best translation of “ bien ” is “ well ” , the best translation of “ __entendu__ ” is “ heard ” , and so on . we tried out two distinct methods for choosing a translation equivalent , thus constructing two different probabilistic __tmems__ : the frequencybased translation memory was created by associating with each french phrase the english equivalent that occurred most often in the collection of phrases that we extracted . if the input sentence is found “ as is ” in the translation memory , its translation is simply returned and there is no further processing . ibm model 4 revolves around the notion of word alignment over a pair of sentences . for example , “ this is rather provision disturbing ” was judged as a correct semantical translation of “ __voil`a__ une disposition __plotˆot__ __inqui´etante__ ” , but “ this disposal is rather disturbing ” was judged as incorrect . correct ? if the meaning was just a little different , the translation was considered semantically incorrect . the probability of a given alignment a and target sentence f given a source sentence e is given by d e d null where the factors delineated by symbols correspond to hypothetical steps in the following generative process : each english word __eis__ assigned with probability n e a fertility , which corresponds to the number of french words into which e is going to be translated . no __<s>__ we present a set of algorithms that enable us to translate natural language sentences by exploiting both a translation memory and a statisticalbased translation model __</s>__ __<s>__ our results show that an automatically derived translation memory can be used within a statistical framework to often find translations of higher probability than those found using solely a statistical model __</s>__ __<s>__ the translations produced using both the translation memory and the statistical model are significantly better than translations produced by two commercial systems: our hybrid system translated perfectly 58% of the 505 sentences in a test collection while the commercial systems translated perfectly only __4042%__ of them __</s>__ __<s>__ we present a set of algorithms that enable us to translate natural language sentences by exploiting both a translation memory and a statisticalbased translation model __</s>__ __<s>__ our results show that an automatically derived translation memory can be used within a statistical framework to often find translations of higher probability than those found using solely a statistical model __</s>__ __<s>__ the translations produced using both the translation memory and the statistical model are significantly better than translations produced by two commercial systems: our hybrid system translated perfectly 58% of the 505 sentences in a test collection while the commercial systems translated perfectly only __4042%__ of them __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we present a set of algorithms that enable us to translate natural language sentences by exploiting both a translation memory and a statisticalbased translation model our results show that an automatically derived translation memory can be used within a statistical framework to often find translations of higher probability than those found using solely a statistical model the translations produced using both the translation memory and the statistical model are significantly better than translations produced by two commercial systems: our hybrid system translated perfectly 58% of the 505 sentences in a test collection while the commercial systems translated perfectly only !!__4042%__!! of them\n",
            "INFO:tensorflow:GENERATED SUMMARY: , il de une . ” , the greedy decoder initially assumes that a good translation of it is “ well heard , it talking a beautiful victory ” because the best translation of “ bien ” is “ well ” , the english we extracted the english phrase generated only words\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  fˆ : ’ s solution by having only one abstract constant s together with the following type assignment : in other terms , for any lexicon l : , where the atomic strings ‘ a ’ , ‘ b ’ , and ‘ c ’ are declared to be constants of type . this allows lexicons to be sequentially composed . this __proofsearch__ relies on linear higherorder matching , which is a decidable problem . __−◦__ s ) condition 3 , in the above definition of a lexicon , is necessary and sufficient to ensure that the homomorphisms induced by a lexicon commute with the typing relations . ay . indeed , two regular __acgs__ that shares the same abstract language correspond to a regular language homomorphism composed with a regular language inverse homomorphism . σ2 = , a lexicon l from __σ1__ to σ2 is defined to be a pair l this implies that , contrarily to the usual categorial approaches , word order constraints can not be expressed at the logical level . a __|∃u__ ∈ a . from a practical point of view , this means that the sequential composition of two lexicons may be compiled . the object language of this second __acg__ is defined as follows : this allows the __acg__ __g13__ to be defined as __he1__ , e3 , __l13__ , si . they present a deductive approach in which linear logic is used as a glue language for assembling meanings . this is mandatory for the applicative paradigm of section 3. the solution is well known : it suffices to represent strings of symbols as compositions of functions . this type assignment obeys an inference system whose judgements are __sequents__ of the following form : we now introduce the abstract notions of a vocabulary and a lexicon , on which the central notion of an abstract categorial grammar is based . the additive connectives of linear logic ‘ & ’ and ‘ ® ’ corresponds respectively to the cartesian product and the disjoint union . in other words , we must show how to encode strings as linear __aterms__ . given two vocabularies __σ1__ = and it allows __aterms__ to be used at a syntactic level , which is an approach that has been advocated by . = __he1__ , e2 , __l12__ , si . consider an arbitrary atomic type * , and define the type ‘ string ’ to be . s 7→ * t that extends f. similarly this is only a matter of choice . first of all , we must explain how __acgs__ may manipulate strings of symbols . as we will see this apparent loss in expressive power is compensated by the first point . consequently , we have two distinct entries in the semantic lexicon , one for each possible reading . the syntactic lexicon , however , would be more involved , with entries such as : s 7→ the __exponentials__ of linear logic are modal operators that may be used to go beyond linearity . 2 definition of a multiplicative kernel in this section , we define an elementary grammatical formalism based on the ideas presented in the introduction . x ∧ __tryto__ john and indeed , each new logical connective may be interpreted , through the __curryhoward__ isomorphism , as anew type constructor . __<s>__ we introduce a new categorial formalism based on intuitionistic linear logic __</s>__ __<s>__ this formalism which derives from current typelogical grammars is abstract in the sense that both syntax and semantics are handled by the same set of primitives __</s>__ __<s>__ as a consequence the formalism is reversible and provides different computational paradigms that may be freely composed together __</s>__ __<s>__ we introduce a new categorial formalism based on intuitionistic linear logic __</s>__ __<s>__ this formalism which derives from current typelogical grammars is abstract in the sense that both syntax and semantics are handled by the same set of primitives __</s>__ __<s>__ as a consequence the formalism is reversible and provides different computational paradigms that may be freely composed together __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we introduce a new categorial formalism based on intuitionistic linear logic this formalism which derives from current typelogical grammars is abstract in the sense that both syntax and semantics are handled by the same set of primitives as a consequence the formalism is reversible and provides different computational paradigms that may be freely composed together\n",
            "INFO:tensorflow:GENERATED SUMMARY: fˆ : ’ s solution by having only one abstract constant s together with the following type assignment : other terms , for any lexicon l : , where the atomic strings ‘ a ’ , ‘ b ’ , other terms , for any lexicon l : , where the atomic strings ‘ a ’ , ‘ b ’ , other terms , for any lexicon l : , where the atomic strings ‘ a ’ , ‘ b ’ , other terms , for any lexicon l : , where the atomic strings ‘ a ’ , ‘\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  notice that most settings show time growth greater than . , we assume that this combination probability is the continuation degree divided by the total number of passive labels , categorical or tag . taking averages directly from the data , we have our first model , shown on the right in figure 9. there are some extremely minimal savings in traversals due to topdown filtering effects , but there is a corresponding penalty in edges as rules whose leftcorner can not be built are introduced . the single biggest factor in the time and traversal performance turned out to be the encoding , which is fortunate because the choice of grammar transform will depend greatly on the application . for example , the state np np np cc . these 23 categories plus the tag __nonecreate__ a passive saturation of 24 for __zerospans__ for __notransform__ . __noempties__ , __empties__ were removed by pruning nonterminals which covered no overt words . however , this is a useful observation about real world parsing performance . the linear terms are larger for __notransform__ and therefore drag the exponent down __more.11__ without unaries , the more gradual saturation growth increases the total exponent , more so for __nounarieslow__ than __nounarieshigh__ . second , active saturation grows with span size because , as spans increase , the tags in a given active edge are more likely to find a matching arrangement over a span . we frequently speak in terms of the following : the parser has an theoretical time bound , where is the number of words in the sentence to be parsed , is the number of nonterminal categories in the grammar and is the number of states in the fsa encoding of the grammar . the savings are less than for prefix compaction . alternately , the effective term could be growing with , which turns out to be true , as discussed in section 4.3. note that while the rule encoding may dramatically affect the efficiency of a parser , it does not change the actual set of parses for a given sentence in any __way.3__ 2in no case were the __nonterminaltoword__ or __toptononterminal__ unaries altered . the sentences were parsed using an implementation of the probabilistic chartparsing algorithm presented in . however , with some straightforward but __spaceconsuming__ recurrences , we can calculate the expected chance that a random rule of a given signature will match a given span length . furthermore , while passive saturation was relatively constant in span size , at least after a point , active saturation quite clearly grows with span size , even for spans well beyond those shown in figure 11. the grammar representation does not affect which passive edges will occur for a given span . however , unlike time and traversals which in practice can diverge , memory requirements match the number of edges in the chart almost exactly , since the large data structures are all proportional in size to the number of edges .6 thus , for a given span size , we report the average over all spans of that size occurring anywhere in any sentence parsed . __<s>__ this paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the penn treebank with the treebank’s own cfg grammar __</s>__ __<s>__ we show how performance is dramatically affected by rule representation and tree transformations but little by topdown vs __</s>__ __<s>__ bottomup strategies __</s>__ __<s>__ we discuss grammatical saturation including analysis of the strongly connected components of the phrasal nonterminals in the treebank and model how as sentence length increases the effective grammar rule size increases as regions of the grammar are unlocked yielding __supercubic__ observed time behavior in some configurations __</s>__ __<s>__ this paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the penn treebank with the treebank’s own cfg grammar __</s>__ __<s>__ we show how performance is dramatically affected by rule representation and tree transformations but little by topdown vs __</s>__ __<s>__ bottomup strategies __</s>__ __<s>__ we discuss grammatical saturation including analysis of the strongly connected components of the phrasal nonterminals in the treebank and model how as sentence length increases the effective grammar rule size increases as regions of the grammar are unlocked yielding __supercubic__ observed time behavior in some configurations __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the penn treebank with the treebank’s own cfg grammar we show how performance is dramatically affected by rule representation and tree transformations but little by topdown vs bottomup strategies we discuss grammatical saturation including analysis of the strongly connected components of the phrasal nonterminals in the treebank and model how as sentence length increases the effective grammar rule size increases as regions of the grammar are unlocked yielding !!__supercubic__!! observed time behavior in some configurations\n",
            "INFO:tensorflow:GENERATED SUMMARY: notice that most settings show time growth than . , we assume that this combination probability is the continuation degree divided by the total number of passive labels , categorical or tag . taking averages directly from the data , more a given active edge are more likely to find a matching arrangement over a span . we frequently speak\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the probability of each rule is assigned by observing how often each rule was used in the training corpus , yielding a probabilistic contextfree grammar . moreover , the advantage of tt is exploited : a simple estimation procedure , and a definite analysis of a given phoneme string . therefore , we split the training corpus into 9 corpora , where the size of the corpora increases logarithmically from 4500 to 2.1 million words . the contextfree grammar fragment in figure 3 describes a so called training grammar with brackets . he reported a generalisation error for words of __2.22__ % on english data . figure 4 shows the distribution of the number of syllables in the test corpus ranked by the number of syllables , which is a decreasing function . there are no intermediate levels between the syllable and the phonemes . however , only those analyses are considered that meet the tagged brackets . the unambiguous analysis of the input word with the syllable structure grammar is shown in figure 1. the rules depict that the syllables comprise different phoneme strings . daelemans and van den bosch report a 96 % accuracy on finding syllable boundaries for dutch with a backpropagation learning algorithm . in section 4 we describe the grammars and experiments for german data . the probability of a rule is inferred by an iterative training procedure with an extended version of the insideoutside algorithm . grammar transformation . this means that the consonants in the onset and in the coda are assigned different weights . our approach offers a machine learning algorithm for predicting syllable boundaries . that provides syllable boundaries , our socalled treebank . the test corpus without syllable boundaries , is processed by a parser ) and the probabilistic contextfree grammars sustaining the most probable parse of each word . we use a part of a german newspaper corpus , the __stuttgarter__ zeitung , consisting of 3 million words which are divided into 9/10 training and 1/10 test corpus . a be the nonterminal that r expands , then the probability assigned to r is given by we then transform the pcfg by dropping the brackets in the rules resulting in an analysis grammar . the input of the system is a phoneme string without brackets . in section 5 the evaluation of the system is described . the grammar differentiate between monosyllabic words , syllables that occur in inital , medial , and final position . in one of her experiments , the standard probability model was applied to a syllabification task , yielding about 89.9 % accuracy . the remainder of the paper is organized as follows . a tts system needs a module where the words converted from graphemes to phonemes are syllabified before they can be further processed to speech . in a first step , we look up the words and their syllabification in a pronunciation dictionary . these restrictions are language specific ; e.g. , the phoneme sequence is quite frequent in english codas but it never appears in english onsets . when the corpus size is doubled , the accuracy of the treebank grammar is still 1.5 % below the positional syllable structure grammar . __<s>__ an approach to automatic detection of syllable boundaries is presented __</s>__ __<s>__ we demonstrate the use of several manually constructed grammars trained with a novel algorithm combining the advantages of treebank and bracketed corpora training __</s>__ __<s>__ we investigate the effect of the training corpus size on the performance of our system __</s>__ __<s>__ the evaluation shows that a handwritten grammar performs better on finding syllable boundaries than does a treebank grammar __</s>__ __<s>__ an approach to automatic detection of syllable boundaries is presented __</s>__ __<s>__ we demonstrate the use of several manually constructed grammars trained with a novel algorithm combining the advantages of treebank and bracketed corpora training __</s>__ __<s>__ we investigate the effect of the training corpus size on the performance of our system __</s>__ __<s>__ the evaluation shows that a handwritten grammar performs better on finding syllable boundaries than does a treebank grammar __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: an approach to automatic detection of syllable boundaries is presented we demonstrate the use of several manually constructed grammars trained with a novel algorithm combining the advantages of treebank and bracketed corpora training we investigate the effect of the training corpus size on the performance of our system the evaluation shows that a handwritten grammar performs better on finding syllable boundaries than does a treebank grammar\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is assigned by observing how often each rule was used the training corpus , yielding a probabilistic contextfree grammar . moreover , the advantage of tt the probability corpus , yielding a probabilistic contextfree grammar . moreover , the advantage of tt the probability corpus , yielding a probabilistic contextfree grammar . moreover , the advantage of tt is assigned by observing how often each rule was used\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  work on more efficient encoding schemes began with __a¨ıtka´ci__ et al . , and this seminal paper has __macneille__ completion at . while the continued efficiency of compiletime completion of signatures as they further increase in size can only be verified empirically , what can be said at this stage is that the only reason that signatures like lingo can be tractably compiled at all is sparseness of consistent types . that yields something roughly of the form shown in figure 3 , which is an example of a recent trend in using __typeintensive__ encodings of linguistic information into typed feature logic in hpsg , beginning with sag . we will call types such as verb __deranged__ types . compilation , if efficient , is to be preferred from the standpoint of static error detection , which incremental methods may elect to skip . this can be implemented very transparently in systems like ale that are built on top of another logic programming language with support for constraint logic programming such as sicstus prolog . the latter interpretation is __subscribed__ to by pollard and sag , for example . otherwise , the algorithm added because of a bounded set , with minimal upper bounds , , which did not have a least upper bound , i.e. , . although the maximum subtype and supertype branching factors in this family increase linearly with size , the partial orders can grow in depth instead in order to contain this . proposition 1 the resulting construction is the finite restriction of the __dedekindmacneille__ completion . the detection of __deranged__ types themselves is also a potential problem . in hpsg , it is generally assumed that __nonmaximallyspecific__ types are simply a convenient shorthand for talking about sets of maximally specific types , sometimes called species , over which the principles of a grammar are stated . let be the set of minimal types to which f is appropriate , ’ s concepts as a meet semilattice , nor that it would be convenient to add all of the types necessary to a wouldbe type hierarchy to ensure __meetsemilatticehood__ . it is , in fact , possible to embed any finite partial order into a smallest lattice that preserves existing meets and joins by adding extra elements . every type is normal except for __truedisj__ , for which the combination , __disj1falseform__ __disj2falseform__ , is not attested in either of its subtypes . they are : can be partially ordered and taken to represent partial information states about some set of objects . in other geometric respects , it bears a close enough resemblance to the theoretical worst case to cause concern about scalability . the resulting description is always __nondisjunctive__ , since logical disjunction is encoded in subtyping . this was also the choice made in the lkb parsing system for hpsg . practical performance is again much better because this algorithm can exploit the empirical observation that most types in a realistic signature are normal and that most feature value restrictions on subtypes do not vary widely . so with added is a complete lattice . __<s>__ this paper considers three assumptions conventionally made about signatures in typed feature logic that are in potential disagreement with current practice among grammar developers and linguists working within featurebased frameworks such as __hpsg:__ __meetsemilatticehood__ unique feature introduction and the absence of subtype covering __</s>__ __<s>__ it also discusses the conditions under which each of these can be tractably restored in realistic grammar signatures where they do not already exist __</s>__ __<s>__ this paper considers three assumptions conventionally made about signatures in typed feature logic that are in potential disagreement with current practice among grammar developers and linguists working within featurebased frameworks such as __hpsg:__ __meetsemilatticehood__ unique feature introduction and the absence of subtype covering __</s>__ __<s>__ it also discusses the conditions under which each of these can be tractably restored in realistic grammar signatures where they do not already exist __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper considers three assumptions conventionally made about signatures in typed feature logic that are in potential disagreement with current practice among grammar developers and linguists working within featurebased frameworks such as !!__hpsg:__!! !!__meetsemilatticehood__!! unique feature introduction and the absence of subtype covering it also discusses the conditions under which each of these can be tractably restored in realistic grammar signatures where they do not already exist\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is sparseness of consistent types . that yields something roughly of the form shown size completion at . that yields something roughly of the form shown completion at . that yields something roughly of the form shown size completion at . that yields something roughly of the form shown size completion at . that yields something roughly of the form shown size completion at . that yields something roughly of the form shown size completion at . that yields something roughly of the form shown size completion at . that yields something roughly of the form shown\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  some q & a systems , like relied both on ne recognizers and some empirical indicators . feedback loop 2 illustrated in figure 1 is activated when the question semantic form and the answer semantic form can not by unified . one of the few q & a systems that takes into account morphological , lexical and semantic alternations of terms is described in . moreover , this knowledge can be translated in axiomatic form and used for abductive proofs . the first step marks all possible concepts that are answer candidates . since in wordnet the verb prefer has verb like as a hypernym , and moreover , its glossed definition is liking better , the query becomes : sometimes multiple keywords are replaced by a semantic alternation . frequently , question reformulations use different words , but imply the same answer . answer extraction based on grammatical information is also promoted by the system described in . we encoded a total of 38 possible answer types . both the question keywords and the expected answer type are identified by using the dependencies derived from the question parse . answer in trec9 243 questions were reformulations of 54 inquiries , thus asking for the same answer . moreover , these dependencies largely cover the question semantic __form2__ . for example : nouns head and government are constituents of a possible paraphrase of president , i.e . “ head of government ” . for example , stateoftheart named entity recognizers developed for ie systems were readily available to be incorporated in q & a systems and helped recognize names of people , organizations , locations or dates . moreover , since our retrieval mechanism does not stem keywords , all the inflections of the verb are also considered . unfortunately this system is not fully autonomous , as it depends on ir results provided by external search engines . loop 1 was generated more often than any other loop . the expected answer type is determined based on the question stem , e.g . who , where or how much and eventually one of the question concepts , when the stem is ambiguous , as described in . the unification involves three steps : step 1 : the recognition of the expected answer type . for example , in the case of question __q209__ : “ who invented the paper clip ? ” , the expected answer type is person and so is the subject of the verb invented , lexicalized as the nominalization inventor . when lexical alternations are necessary because no answer was found yet , the first keyword that is altered is determined by the question word that either prompted the expected answer type or is in the same semantic class with the expected answer type . table 4 lists also the combined effect of the feedbacks , showing that when all feedbacks are enabled , for short answers we obtained an __mrar__ of __0.568__ , i.e . 76 % increase over q & a without feedbacks . for such cases several approaches have been developed . this concept is searched in an answer taxonomy comprising several tops linked to a significant number of wordnet noun and verb hierarchies . __<s>__ this paper presents an opendomain textual questionanswering system that uses several feedback loops to enhance its performance __</s>__ __<s>__ these feedback loops combine in a new way statistical results with syntactic semantic or pragmatic information derived from texts and lexical databases __</s>__ __<s>__ the paper presents the contribution of each feedback loop to the overall performance of 76% humanassessed precise answers __</s>__ __<s>__ this paper presents an opendomain textual questionanswering system that uses several feedback loops to enhance its performance __</s>__ __<s>__ these feedback loops combine in a new way statistical results with syntactic semantic or pragmatic information derived from texts and lexical databases __</s>__ __<s>__ the paper presents the contribution of each feedback loop to the overall performance of 76% humanassessed precise answers __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper presents an opendomain textual questionanswering system that uses several feedback loops to enhance its performance these feedback loops combine in a new way statistical results with syntactic semantic or pragmatic information derived from texts and lexical databases the paper presents the contribution of each feedback loop to the overall performance of 76% humanassessed precise answers\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is proposed . moreover a systems , like relied both on systems , like relied both on systems , like relied both on ne recognizers . feedback loop 2 illustrated the q & a systems , like relied both on ne some empirical indicators . feedback loop 2 illustrated the empirical indicators . feedback loop 2 illustrated . moreover , this knowledge can be translated\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the tagged sentences are further analyzed by a cascade of finite state machines leveraging patterns with lexical and syntactic information , to identify constructions such as preand postmodifying appositive phrases , e.g. , “ presidential candidate george bush ” , “ bush , the presidential candidate ” , and relative clauses , e.g. , “ senator ... , who is running for __reelection__ this fall , ” . strength of association between subject i and verb j is measured using mutual information : here __tfij__ is the maximum frequency of subjectverb pair ij in the reuters corpus the system also filters out redundant descriptions , both duplicate descriptions as well as similar ones . we took the raw descriptions that the system produced before merging , and wrote a brief description by hand for each person who had two or more raw descriptions . the component evaluation tests how accurately the tagger can identify whether a head noun in a description is appropriate as a person description the evaluation uses the wordnet 1.6 semcor semantic concordance , which has files from the brown corpus whose words have semantic tags indicating wordnet sense numbers . the fundamental differences in our work are as follows : we extract not only appositive phrases , but also clauses at large based on corpus statistics ; we make heavy use of coreference , whereas they don ’ t use coreference at all ; we focus on generating succinct descriptions by removing redundancy and merging , whereas they categorize descriptions using wordnet , without a focus on __succinctness__ . i in the corpus however , the preprocessing component described in section 2.1 does produce errors in appositive extraction , which are filtered out by syntactic and semantic tests . we report here on the development of a biographical mds summarizer that summarizes information about people described in the news . for each relative clause description , the subject was given the description , a person name to whom that description __pertained__ , and a capsule description consisting of merged appositives created by the system . a portion of the results of doing this is shown in table 1. the sentential descriptions are filtered in part based on the presence of verbs like “ testify , “ __plead__ ” , or “ greet ” that are strongly associated with the head noun of the appositive , namely “ friend ” . the weighting is based on how often the relative clause ’ s main verb is strongly associated with a subject in a large corpus , compared to its total number of appearances in the corpus . the system compares each pair of appositive descriptions of a person , merging them based on corpus frequencies of the description head stem , syntactic information , and semantic information based on the relationship between the heads in wordnet . future directions could include improved sentential descriptions as well as further intrinsic and extrinsic evaluations of the summarizer as a whole . third , there may be redundancy between the sentential descriptions , on one hand , and the appositive and relative clause descriptions , on the other . how many possessive : possessive pronouns are there ? although the sample is small , the results are very promising . __<s>__ we describe a biographical multidocument summarizer that summarizes information about people described in the news __</s>__ __<s>__ the summarizer uses corpus statistics along with linguistic knowledge to select and merge descriptions of people from a document collection removing redundant descriptions __</s>__ __<s>__ the summarization components have been extensively evaluated for coherence accuracy and nonredundancy of the descriptions produced __</s>__ __<s>__ we describe a biographical multidocument summarizer that summarizes information about people described in the news __</s>__ __<s>__ the summarizer uses corpus statistics along with linguistic knowledge to select and merge descriptions of people from a document collection removing redundant descriptions __</s>__ __<s>__ the summarization components have been extensively evaluated for coherence accuracy and nonredundancy of the descriptions produced __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we describe a biographical multidocument summarizer that summarizes information about people described in the news the summarizer uses corpus statistics along with linguistic knowledge to select and merge descriptions of people from a document collection removing redundant descriptions the summarization components have been extensively evaluated for coherence accuracy and nonredundancy of the descriptions produced\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a head noun the corpus however , the preprocessing component described section 2.1 does produce errors , to identify constructions such as preand postmodifying appositive phrases , e.g. , “ presidential candidate george bush ” , “ bush , the presidential candidate ” , the presidential candidate ” , e.g. , “ senator ... , who is running for the corpus however , the preprocessing component described section 2.1 does produce errors the corpus however , the preprocessing component described\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  that the __mcle__ does not . table 1 compares the mle and __mcle__ pcfgs . these parsers are direct simplifications of the structured language model . because of the computational complexity of these problems , the method is only applied to a simple pcfg based on the atis corpus . this paper has investigated the difference between maximum likelihood estimation and maximum conditional likelihood estimation for three different kinds of models : pcfg parsers , hmm taggers and shiftreduce parsers . one possible explanation for this result is that the way in which the interpolated estimate of p0 is calculated , rather than conditional likelihood estimation per se , is lowering tagger accuracy somehow . the labelled precision and recall parsing results for the __mcle__ pcfg were slightly higher than those of the mle pcfg . the next section formulates the __mcles__ for hmms and pcfgs as constrained optimization problems and describes an iterative dynamicprogramming method for solving them . tm and a word sequence we also append a ‘ * ’ to end of the actual terminal string being parsed , as this simplifies the formulation of the parsers , i.e. , if the string to be parsed is w1 ... a priori , one can advance plausible arguments in favour of both the mle and the __mcle__ . maximum conditional likelihood is consistent for the conditional distribution . in this application , the pairs consist of a parse tree y and its terminal string or yield x . the maximum likelihood estimates were easy to compute and for others of which the maximum conditional likelihood estimates could be easily computed . then given a training corpus d = , ... , ) , where yi is a parse tree for the string xi , the log conditional likelihood of the training data log p and its derivative are given by : � � here now consider the following conditional model of the conditional distribution of tags given words a conditional shiftreduce parser differs only minimally from the shiftreduce parser just described : it is defined by a distribution p over next moves m given the top and __nexttotop__ stack labels s1 , s2 and the next input symbol thus for bitag tagging at least , the conditional model has a considerably higher error rate than any of the joint models examined here . . on the other hand , applications which involve predicting the value of the hidden variable from the visible variable usually only involve the conditional distribution , which the __mcle__ estimates directly . for all s1 , s2 ; i.e. , shift moves can only shift the current lookahead symbol . yet adding this term improves tagging accuracy considerably , to __95.3__ % . data . however , given that we often have insufficient data in computational linguistics , and there are good reasons to believe that the true distribution of sentences or parses can not be described by our models , there is no reason to expect these asymptotic results to hold in practice , and in the experiments reported below the mle and __mcle__ behave differently experimentally . the previous section compared __similiar__ joint and conditional tagging models . __<s>__ this paper compares two different ways of estimating statistical language models __</s>__ __<s>__ many statistical nlp tagging and parsing models are estimated by maximizing the likelihood of the __fullyobserved__ training data __</s>__ __<s>__ however since these applications only require the conditional probability distributions these distributions can in principle be learnt by maximizing the conditional likelihood of the training data __</s>__ __<s>__ perhaps somewhat surprisingly models estimated by maximizing the joint were superior to models estimated by maximizing the conditional even though some of the latter models intuitively had access to __“more__ __information”__ __</s>__ __<s>__ this paper compares two different ways of estimating statistical language models __</s>__ __<s>__ many statistical nlp tagging and parsing models are estimated by maximizing the likelihood of the __fullyobserved__ training data __</s>__ __<s>__ however since these applications only require the conditional probability distributions these distributions can in principle be learnt by maximizing the conditional likelihood of the training data __</s>__ __<s>__ perhaps somewhat surprisingly models estimated by maximizing the joint were superior to models estimated by maximizing the conditional even though some of the latter models intuitively had access to __“more__ __information”__ __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper compares two different ways of estimating statistical language models many statistical nlp tagging and parsing models are estimated by maximizing the likelihood of the !!__fullyobserved__!! training data however since these applications only require the conditional probability distributions these distributions can in principle be learnt by maximizing the conditional likelihood of the training data perhaps somewhat surprisingly models estimated by maximizing the joint were superior to models estimated by maximizing the conditional even though some of the latter models intuitively had access to !!__“more__!! !!__information”__!!\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is proposed . table 1 compares the mle . table 1 compares the mle . these parsers are direct simplifications of the structured language model . because parsers are direct simplifications of the structured language model . because of the computational complexity of these problems , the method is only applied to a simple pcfg based on the atis corpus . this paper has investigated the difference between maximum likelihood estimation per likelihood estimation for three different kinds of models : pcfg parsers , hmm taggers shiftreduce parsers . one possible explanation for this result is that\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  in distinction to , the nonterminals can be structured . in fig . then is a reduction of the structure to its type , the weak generative capacity of __pdgrammars__ is stronger than that of __cfgrammars__ . theoretically , this might blow up times the size of a grammar with defect valencies and the maximal length of left parts of rules . 3 is determined by on its reduction combined with the diagram of local and long dependencies is presented in fig . meanwhile , the projectivity is not the norm in natural languages . definition 6. : we should just introduce into the dictionary both variants of the verb description – with the local dependency to the right and with the positive valency to the left . __dstructures__ with valencies , __dvstructures__ , are defined so that valencies saturation would imply connectivity . we will select the following particular one : let be the first non saturated positive valency in and be the closest corresponding 5 for the space reasons , we don ’ t cite its definition , the more so that the linguistic __pdgrammars__ should certainly be __lcfree__ . let be a __pdgrammar__ . the mechanism of establishing long dependencies is orthogonal to reduction and is implemented by a universal and simple rule of valencies saturation . as most dependency grammars , the __pdgrammars__ are analyzing . the valencies are used to specify discontinuous dependencies lacking in partial dependency structures . we change the reduction semantics as follows . this reduction is successful due to the fact that the negative valency is assigned to the preposition and the corresponding positive valency is assigned to the verb what might serve the formal basis for these assignments ? for example , the __pdgrammar__ : generates a noncf language dtree in fig . it is for this reason that dtrees determined by grammars of robinson , categorial grammars , classical lambek calculus , and some other formalisms are projective . 4 , the group of the preposition is moved , which is of course a sufficient condition for assigning the positive valency to the verb . the __dstructures__ we will use will be polarized in the sense that some words will have valencies specifying long dependencies which must enter or go from them . in the special case , where the __dvstructures__ in the rules are dtrees , the __pdgrammar__ is __local7__ . the yield of a successful reduction is a dtree . 1. for example , in most european languages there are such regular nonprojective constructions as __whor__ relative clause extraction , topicalization , comparative constructions , and some constructions specific to a language , e.g . french pronominal clitics or left dislocation . syntactic theories based on the concept of dependency have a long tradition . intuitively , we can think of as of the combined information available after the phase of morphological analysis . let us start with this verb has the strong government over prepositions in the clause in fig . dtree itself does not presume a linear order on words . let and be the unit __dvstructure__ with then is a reduction of the structure to its type and is the integral valency of this reduction denoted by 2 . __<s>__ polarized dependency grammars are proposed as a means of efficient treatment of discontinuous constructions __</s>__ __<s>__ __pdgrammars__ describe two kinds of dependencies : local explicitly derived by the rules and long implicitly specified by negative and positive valencies of words __</s>__ __<s>__ if in a __pdgrammar__ the number of __nonsaturated__ valencies in derived structures is bounded by a constant then it is weakly equivalent to a __cfgrammar__ and has a time parsing algorithm __</s>__ __<s>__ it happens that such bounded __pdgrammars__ are strong enough to express such phenomena as unbounded raising extraction and extraposition __</s>__ __<s>__ polarized dependency grammars are proposed as a means of efficient treatment of discontinuous constructions __</s>__ __<s>__ __pdgrammars__ describe two kinds of dependencies : local explicitly derived by the rules and long implicitly specified by negative and positive valencies of words __</s>__ __<s>__ if in a __pdgrammar__ the number of __nonsaturated__ valencies in derived structures is bounded by a constant then it is weakly equivalent to a __cfgrammar__ and has a time parsing algorithm __</s>__ __<s>__ it happens that such bounded __pdgrammars__ are strong enough to express such phenomena as unbounded raising extraction and extraposition __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: polarized dependency grammars are proposed as a means of efficient treatment of discontinuous constructions __pdgrammars__ describe two kinds of dependencies : local explicitly derived by the rules and long implicitly specified by negative and positive valencies of words if in a __pdgrammar__ the number of !!__nonsaturated__!! valencies in derived structures is bounded by a constant then it is weakly equivalent to a !!__cfgrammar__!! and has a time parsing algorithm it happens that such bounded __pdgrammars__ are strong enough to express such phenomena as unbounded raising extraction and extraposition\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a the norm generative capacity of the fact that the negative valency is assigned . meanwhile , the projectivity of the fact that the negative valency is assigned . meanwhile , the projectivity the corresponding positive valency the corresponding positive valency the formal basis for these assignments ? for example , the diagram are analyzing . the valencies are used to specify discontinuous\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  b is required to try and find values for these references . __representations.5__ indeed we assume that a component of dialogue competence is knowledge of these coercion operations . in fact , having posited __constits__ one could eliminate __dtrs__ : this by making the value of __constits__ be a set of sets whose first level elements are the immediate constituents . the protocol involves the assumption that an agent always initially tries to integrate an utterance by assuming it constitutes an adjacency pair with the existing __latestmove__ . __7within__ it would be a great advantage in negotiative dialogues , where , for example , the system and the user might be discussing several options and the system may make alternative suggestions , for a system to be able to recognize and interpret a ce . indeed , in principle , relation names should also be included , since they vary with context and are subject to clarification as well . these we take to be specified by a set of coercion operations on utterance __11in__ the version of hpsg information about phrases is encoded by __crossclassifying__ them in a multidimensional type hierarchy . relative to certain goals , one might decide simply to existentially quantify the problematic referent . such an utterance will have as its __maxqud__ a question __cqparaphrasable__ as who , named bo , are you asking if t left , whereas its __salutt__ is the subutterance of bo . below , we will show how to extend this account of parallelism to clarification queries . we start by informally describing the __grounding/clarification__ processes and the representations on which they operate . we sketch an algorithm for the process of utterance integration which leads to grounding or clarification . finally , we formalize the operations which underpin clarification and sketch a grammatical analysis of ce . 2 utterance representation : grounding and clarification we start by offering an informal description of how an __utterancesuch__ as can get grounded or spawn a clarification by an addressee b : a is attempting to convey to b thus , we posit constit __clar__ __intcl__ , a new phrasal subtype of __headfragph__ and of __intercl__ which encapsulates the two idiosyncratic facets of such utterances , namely the phonological parallelism and the __maxqud/content__ identity : in this paper we offered an analysis of the types of representations needed to analyze ce , the requisite operations thereon , and how these update iss during grounding and clarification . for current purposes , we stick with tradition and tolerate the redundancy of both __dtrs__ and __constits__ . intuitively , is a parameter whose value is problematic or lacking . this much is familiar already from early work on context dependence by et seq . these presuppose no sophisticated linguistic analysis . the second modification we make concerns the encoding of phrasal constituency . we posit a set valued first , we revamp the existing treatment of the feature __cindices__ . consider the following dialogue in the __routeplanning__ domain : at this point the system has to consider a number of possible __intepretations__ for the user ’ s utterance all of which involve recognizing that this is a clarification request concerning the system ’ s last utterance . __<s>__ we offer a computational analysis of the resolution of ellipsis in certain cases of dialogue clarification __</s>__ __<s>__ we show that this goes beyond standard techniques used in anaphora and ellipsis resolution and requires operations on highly structured linguistically heterogeneous representations __</s>__ __<s>__ we characterize these operations and the representations on which they operate __</s>__ __<s>__ we offer an analysis couched in a version of headdriven phrase structure grammar combined with a theory of information states in dialogue __</s>__ __<s>__ we sketch an algorithm for the process of utterance integration in iss which leads to grounding or clarification __</s>__ __<s>__ we offer a computational analysis of the resolution of ellipsis in certain cases of dialogue clarification __</s>__ __<s>__ we show that this goes beyond standard techniques used in anaphora and ellipsis resolution and requires operations on highly structured linguistically heterogeneous representations __</s>__ __<s>__ we characterize these operations and the representations on which they operate __</s>__ __<s>__ we offer an analysis couched in a version of headdriven phrase structure grammar combined with a theory of information states in dialogue __</s>__ __<s>__ we sketch an algorithm for the process of utterance integration in iss which leads to grounding or clarification __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we offer a computational analysis of the resolution of ellipsis in certain cases of dialogue clarification we show that this goes beyond standard techniques used in anaphora and ellipsis resolution and requires operations on highly structured linguistically heterogeneous representations we characterize these operations and the representations on which they operate we offer an analysis couched in a version of headdriven phrase structure grammar combined with a theory of information states in dialogue we sketch an algorithm for the process of utterance integration in iss which leads to grounding or clarification\n",
            "INFO:tensorflow:GENERATED SUMMARY: is required to try find values for these for these references . the protocol involves the assumption that an agent always initially tries to integrate an utterance by assuming it constitutes an adjacency pair with the existing . these it would be discussing several options be a set of sets whose first level elements are the immediate constituents . the protocol involves the assumption that an agent always initially tries to integrate an utterance by assuming it constitutes an adjacency pair with the existing . these it would be discussing several options elements are the immediate constituents . the protocol\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  still , many compound nouns are not encoded as wordnet entries , and need to be recognized as a single nominal . , all verb predicates have three arguments : __action/state/eventpredicate__ , where : x3 ) . different parsing methods of dictionary definitions were used : patternmatching , genus disambiguation , especially constructed definition parsers or broad coverage parsers , , . we are able to match a few predicates : similar to conjunction predicates , the nn predicates can have a variable number of arguments , with the first one representing the result of the aggregation of the nouns corresponding to the rest of the arguments . the goal of this research project is to transform all the wordnet glosses into logic representations that enables reasoning mechanisms for many practical applications . the first argument represents the & ; result & ; of the logical operation induced by the conjunction . fix __slotallocation__ in the spirit of the davidsonian treatment of the action predicates in a successful unification the arguments of question predicate will be bound to the arguments of answer predicate and the qlf and __alf__ updated to reflect the new status of arguments . hobbs explains that for many linguistic applications it is acceptable to relax ontological __scruples__ , intricate syntactic explanations , and the desire for efficient deductions in favor of a simpler notation closer to english . since in wordnet glosses not many verbs have indirect objects , the argument x3 is used only when necessary , otherwise is __ommited__ . the second technique consists of rearranging the parse trees so that more complex structures are reduced to simpler ones . this decision is based on our desire to provide manageable and consistent logic representation that otherwise would be unfeasible . example 2 consider the trec9 's question : __q481__ : who shot billy the kid ? we have not noticed that these simplifications had any adverse effect on the trec questions . we have presented here a procedure to transform wordnet glosses into logic forms . the preposition predicates always have two arguments : the new qlf to be proven contains only the predicate die . predicates a predicate is generated for every noun , verb , adjective or adverb encountered in any gloss . the way of doing this was first devised in tacitus , when the predicate nn was first introduced . table 2 shows some examples of preposition predicates . conjunctions conjunctions are transformed in predicates , which enable the aggregation of several predicates under the same syntactic role . in this paper we limit the discussion to the definitions and ignore the gloss examples . this paper presents a simple but consistent logic notation suitable for representing the english texts of the wordnet glosses . the first argument corresponding to the predicate of the head of the phrase to which prepositional phrase is attached , whereas the second argument corresponds to the prepositional object . it is well understood and agreed that world knowledge is necessary for many common sense reasoning problems . then step 3 is derived using : kill cause die . consider this extra knowledge is found in wordnet glosses . complex nominals __<s>__ wordnet is a rich source of world knowledge from which formal axioms can be derived __</s>__ __<s>__ in this paper we present a method for transforming the wordnet glosses into logic forms and further into axioms __</s>__ __<s>__ the transformation of wordnet glosses into logic forms is useful for theorem proving and other applications __</s>__ __<s>__ the paper demonstrates the utility of the wordnet axioms in a question answering system to rank and extract answers __</s>__ __<s>__ wordnet is a rich source of world knowledge from which formal axioms can be derived __</s>__ __<s>__ in this paper we present a method for transforming the wordnet glosses into logic forms and further into axioms __</s>__ __<s>__ the transformation of wordnet glosses into logic forms is useful for theorem proving and other applications __</s>__ __<s>__ the paper demonstrates the utility of the wordnet axioms in a question answering system to rank and extract answers __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: wordnet is a rich source of world knowledge from which formal axioms can be derived in this paper we present a method for transforming the wordnet glosses into logic forms and further into axioms the transformation of wordnet glosses into logic forms is useful for theorem proving and other applications the paper demonstrates the utility of the wordnet axioms in a question answering system to rank and extract answers\n",
            "INFO:tensorflow:GENERATED SUMMARY: verbs have indirect objects , the argument x3 is used only when necessary , otherwise is used . the second technique consists of rearranging the parse trees so that more complex structures are reduced to simpler ones . this decision is based on our desire to provide manageable the spirit of the davidsonian treatment of the action\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "INFO:tensorflow:We've been decoding with same checkpoint for 61 seconds. Time to load new checkpoint\n",
            "INFO:tensorflow:Loading checkpoint /content/drive/My Drive/MA_colab/PG_Model2/logs/myexperiment/train/model.ckpt-15726\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/MA_colab/PG_Model2/logs/myexperiment/train/model.ckpt-15726\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  while it is possible to transcribe the continuous stream of audio data without any prior segmentation , partitioning offers several advantages over this straightforward solution . stemming is used to reduce the number of lexical items for a given word sense . the result of the partitioning process is a set of speech segments usually corresponding to speaker turns with speaker , gender and telephone a histogram of the __2096__ sections is shown in figure 4. the information retrieval system relies on a unigram model per story . the score of a combined document is set to maximum score of any one of the components . the system has two main components , the speech transcription component and the information retrieval component . finally , eliminating nonspeech segments substantially reduces the computation time . our transcription systems for french and german have comparable error rates for news broadcasts . the acoustic and language models are trained on large , representative corpora for each task and language . as shown in figure 1 the limsi broadcast news transcription system for automatic indexation consists of an audio __partitioner__ and a speech recognizer . using an a priori acoustic segmentation , the mean average precision is significantly reduced compared to a “ perfect ” manual segmentation , whereas the windowbased search engine results are much closer . the information retrieval results are given in terms of mean average precision , as is done for the trec benchmarks in table 1 with and without query expansion . of the __2096__ sections manually marked as reports , 40 % start without a manually annotated speaker change . although it is usually assumed that processing time is not a major issue since computer power has been increasing continuously , it is also known that the amount of data appearing on information channels is increasing at a close rate . since there are many stories significantly shorter than 30s in broadcast shows we __conjunctured__ that it may be of interest to use a double windowing system in order to better target short stories . transcription word error rates of about 20 % have been reported for unrestricted broadcast news data in several languages . in particular the project addresses the development of generic speech recognition technology and methods to rapidly port technology to new domains and languages with limited supervision , and to produce enriched symbolic speech transcriptions . this research has been carried out in a multilingual environment in the context of several recent and ongoing european projects . the stemming lexicon contains about __32000__ entries and was constructed using porter ’ s algorithm on the most frequent words in the collection , and then manually corrected . as proposed in , we segment the audio stream into overlapping documents of a fixed duration . as a result of optimization , we chose a 30 second window duration with a 15 second overlap . this information can be used both directly and indirectly for indexation and retrieval purposes . automatic speech recognition is a key technology for audio and video indexing . __<s>__ this paper addresses recent progress in speakerindependent large vocabulary continuous speech recognition which has opened up a wide range of near and midterm applications __</s>__ __<s>__ one rapidly expanding application area is the processing of broadcast audio for information access __</s>__ __<s>__ at limsi broadcast news transcription systems have been developed for english french german mandarin and portuguese and systems for other languages are under development __</s>__ __<s>__ audio indexation must take into account the specificities of audio data such as needing to deal with the continuous data stream and an imperfect word transcription __</s>__ __<s>__ some nearterm applications areas are audio data mining selective dissemination of information and media monitoring __</s>__ __<s>__ this paper addresses recent progress in speakerindependent large vocabulary continuous speech recognition which has opened up a wide range of near and midterm applications __</s>__ __<s>__ one rapidly expanding application area is the processing of broadcast audio for information access __</s>__ __<s>__ at limsi broadcast news transcription systems have been developed for english french german mandarin and portuguese and systems for other languages are under development __</s>__ __<s>__ audio indexation must take into account the specificities of audio data such as needing to deal with the continuous data stream and an imperfect word transcription __</s>__ __<s>__ some nearterm applications areas are audio data mining selective dissemination of information and media monitoring __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper addresses recent progress in speakerindependent large vocabulary continuous speech recognition which has opened up a wide range of near and midterm applications one rapidly expanding application area is the processing of broadcast audio for information access at limsi broadcast news transcription systems have been developed for english french german mandarin and portuguese and systems for other languages are under development audio indexation must take into account the specificities of audio data such as needing to deal with the continuous data stream and an imperfect word transcription some nearterm applications areas are audio data mining selective dissemination of information and media monitoring\n",
            "INFO:tensorflow:GENERATED SUMMARY: power manually marked as reports , 40 % start without a manually annotated speaker change . although it is usually assumed that processing time is not a major issue since computer power has been increasing continuously , it is usually assumed that processing time is not a major issue since computer power has been increasing continuously , it is possible to transcribe the continuous stream of audio data without any prior segmentation , partitioning offers several advantages over this straightforward solution . stemming is possible to transcribe the continuous stream of audio data without any prior segmentation , partitioning offers\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  their approach also raises a number of interesting followup questions , some concerned with problem detection , others with the use of machine learning techniques . the current results show that for online detection of communication problems at the utterance level it is already beneficial to pay attention only to the lexical information in the word graph and the sequence of system question types , features which are present in most spoken dialogue system and which can be obtained with little or no computational overhead . communication problems are generally easy to label since the spoken dialogue system under consideration here always provides direct feedback about what it believes the user intends . this latter feature is the one to be predicted . one important property of many other rules is that they explicitly combine pieces of information from the three main sources of information . finally , we agree with walker et al . table 1 summarizes the baselines . u this strategy has an accuracy of 58.2 % , and a recall of 0 % . rules are induced per class . in sum , it is uncertain whether other spoken dialogue systems can benefit from the findings described by walker et al . , since it is unclear which features are important and to what extent these features are available in other spoken dialogue systems . walker et al . for instance , krahmer et al . , in their descriptive analysis of dialogue problems , found that repeated material is often an indication of problems , as is the use of a marked vocabulary . this shows that spoken dialogue systems may use these features to better predict whether the ongoing dialogue is problematic . a word graph is a lattice of word hypotheses , and we conjecture that various features which have been shown to cue communication problems have correlates in the word graph . o open questions i implicit verification see the best result is obtained by taking the two most recent word graphs and the six most recent system question types as input . : this accuracy result is significantly better than the ib1ig result given in table 2 for this particular task , with .001. here , for the sake of generality , we abstract over such differences and simply represent a word graph as a bag of words , collecting all words that occur in one of the paths , irrespective of the associated acoustic confidence score . different systems determine the plausibility of paths in the word graph in different ways . the best result is obtained using all features : communication problems are detected with an accuracy of 86 % , a precision of 83 % and a recall of 75 % . these features are present in many spoken dialogue systems and do not require additional computation , which makes this a very cheap method to detect problems . a lexicon was derived of all the words and phrases that occurred in the corpus . __<s>__ we address the issue of online detection of communication problems in spoken dialogue systems __</s>__ __<s>__ the usefulness is investigated of the sequence of system question types and the word graphs corresponding to the respective user utterances __</s>__ __<s>__ by applying both ruleinduction and memorybased learning techniques to data obtained with a dutch train timetable information system the current paper demonstrates that the aforementioned features indeed lead to a method for problem detection that performs significantly above baseline __</s>__ __<s>__ the results are interesting from a dialogue perspective since they employ features that are present in the majority of spoken dialogue systems and can be obtained with little or no computational overhead __</s>__ __<s>__ the results are interesting from a machine learning perspective since they show that the rulebased method performs significantly better than the memorybased method because the former is better capable of representing interactions between features __</s>__ __<s>__ we address the issue of online detection of communication problems in spoken dialogue systems __</s>__ __<s>__ the usefulness is investigated of the sequence of system question types and the word graphs corresponding to the respective user utterances __</s>__ __<s>__ by applying both ruleinduction and memorybased learning techniques to data obtained with a dutch train timetable information system the current paper demonstrates that the aforementioned features indeed lead to a method for problem detection that performs significantly above baseline __</s>__ __<s>__ the results are interesting from a dialogue perspective since they employ features that are present in the majority of spoken dialogue systems and can be obtained with little or no computational overhead __</s>__ __<s>__ the results are interesting from a machine learning perspective since they show that the rulebased method performs significantly better than the memorybased method because the former is better capable of representing interactions between features __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we address the issue of online detection of communication problems in spoken dialogue systems the usefulness is investigated of the sequence of system question types and the word graphs corresponding to the respective user utterances by applying both ruleinduction and memorybased learning techniques to data obtained with a dutch train timetable information system the current paper demonstrates that the aforementioned features indeed lead to a method for problem detection that performs significantly above baseline the results are interesting from a dialogue perspective since they employ features that are present in the majority of spoken dialogue systems and can be obtained with little or no computational overhead the results are interesting from a machine learning perspective since they show that the rulebased method performs significantly better than the memorybased method because the former is better capable of representing interactions between features\n",
            "INFO:tensorflow:GENERATED SUMMARY: their only to the lexical information the word graph interesting followup questions , some concerned with problem detection , others with the use of machine learning techniques . the current results show that for online detection of communication problems at a number of interesting followup questions , some concerned with problem detection , others with the use of machine learning techniques . the current results show that for online detection of communication problems at the utterance level it is already beneficial to pay attention only to the lexical information\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  this is called a shared forest of parse trees , because it can represent an exponential number of possible parses using a polynomial number of nodes which are shared between alternative analyses , and can be constructed and traversed in time of the same complexity . each node in this forest is then annotated with the set of objects and intervals that it could refer to in the environment . for example , adjectives , prepositional phrases , and relative clauses , which are typically represented as __situationallydependent__ properties will be extremely large until composed with modifiers and arguments . the average reduction in number of possible parse trees due to the __environmentbased__ filtering mechanism described above was for successfully parsed and filtered __forests.7__ it incorporates ideas from type theory to represent a broad range of linguistic phenomena in a manner for which their extensions or potential referents in the environment are welldefined in every case . but since there is an interval corresponding to a __draining__ process at the root , the whole vp will still be preferred as constituent due to the other interpretation . in order to evaluate the possible interpretations of a sentence , as described in the previous section , an interface needs to define referent sets for every possible __constituent.2__ the proposed solution draws on a theory of constituent types from formal linguistic semantics , in which constituents such as nouns and verb phrases are represented as __composeable__ functions that take __entitiess__ or situations as inputs and ultimately return a truth value for the sentence . if there is a button next to an adapter , but no handle next to an adapter , the tree representing ‘ handle beside adapter ’ as a constituent may be dispreferred in disambiguation , but the np constituent at the root is still preferred because it has potential referents in the environment due to the other interpretation . in this preliminary accuracy test , forest nodes that correspond to noun phrase or modifier categories are dispreferred if they have no potential entity referents , and forest nodes corresponding to other categories are dispreferred if their arguments have no potential entity referents . a record of the derivation paths in any dynamic programming recognition algorithm or earley ) can be interpreted as a polynomial sized __andor__ graph with space complexity equal to the time complexity of recognition , whose disjunctive nodes represent possible constituents in the analysis , and whose conjunctive nodes represent binary applications of rules in the grammar . fortunately , since the logical function forest shares structure between alternative analyses , many of the sets of potential referents can be shared between analyses during evaluation as well . existing __environmentbased__ methods only calculate the referents of noun phrases , so they only consult the objects in an environment database when interpreting input sentences . this scarcity may be a result of the significant computational complexity of evaluating them in isolation . since there were no testing intervals at __3:00__ in the environment , the referent set for the np ‘ test after __3:00__ ’ is evaluated to the null set . __<s>__ the standard pipeline approach to semantic processing in which sentences are morphologically and syntactically resolved to a single tree before they are interpreted is a poor fit for applications such as natural language interfaces __</s>__ __<s>__ this is because the environment information in the form of the objects and events in the application’s runtime environment cannot be used to inform parsing decisions unless the input sentence is semantically analyzed but this does not occur until after parsing in the __singletree__ semantic architecture __</s>__ __<s>__ this paper describes the computational properties of an alternative architecture in which semantic analysis is performed on all possible interpretations during parsing in polynomial time __</s>__ __<s>__ the standard pipeline approach to semantic processing in which sentences are morphologically and syntactically resolved to a single tree before they are interpreted is a poor fit for applications such as natural language interfaces __</s>__ __<s>__ this is because the environment information in the form of the objects and events in the application’s runtime environment cannot be used to inform parsing decisions unless the input sentence is semantically analyzed but this does not occur until after parsing in the __singletree__ semantic architecture __</s>__ __<s>__ this paper describes the computational properties of an alternative architecture in which semantic analysis is performed on all possible interpretations during parsing in polynomial time __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: the standard pipeline approach to semantic processing in which sentences are morphologically and syntactically resolved to a single tree before they are interpreted is a poor fit for applications such as natural language interfaces this is because the environment information in the form of the objects and events in the application’s runtime environment cannot be used to inform parsing decisions unless the input sentence is semantically analyzed but this does not occur until after parsing in the !!__singletree__!! semantic architecture this paper describes the computational properties of an alternative architecture in which semantic analysis is performed on all possible interpretations during parsing in polynomial time\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . information a shared forest of parse trees , because it can represent an exponential number of possible parses using a polynomial number of nodes which are shared between alternative analyses , can be constructed traversed traversed time of the same complexity . each node it can represent an exponential number of possible parses using a polynomial number of nodes which are shared between alternative analyses , can be constructed traversed traversed time of the same complexity . each node it can represent an exponential number of possible parses using a polynomial number of nodes\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  while it is possible to transcribe the continuous stream of audio data without any prior segmentation , partitioning offers several advantages over this straightforward solution . stemming is used to reduce the number of lexical items for a given word sense . the result of the partitioning process is a set of speech segments usually corresponding to speaker turns with speaker , gender and telephone a histogram of the __2096__ sections is shown in figure 4. the information retrieval system relies on a unigram model per story . the score of a combined document is set to maximum score of any one of the components . the system has two main components , the speech transcription component and the information retrieval component . finally , eliminating nonspeech segments substantially reduces the computation time . our transcription systems for french and german have comparable error rates for news broadcasts . the acoustic and language models are trained on large , representative corpora for each task and language . as shown in figure 1 the limsi broadcast news transcription system for automatic indexation consists of an audio __partitioner__ and a speech recognizer . using an a priori acoustic segmentation , the mean average precision is significantly reduced compared to a “ perfect ” manual segmentation , whereas the windowbased search engine results are much closer . the information retrieval results are given in terms of mean average precision , as is done for the trec benchmarks in table 1 with and without query expansion . of the __2096__ sections manually marked as reports , 40 % start without a manually annotated speaker change . although it is usually assumed that processing time is not a major issue since computer power has been increasing continuously , it is also known that the amount of data appearing on information channels is increasing at a close rate . since there are many stories significantly shorter than 30s in broadcast shows we __conjunctured__ that it may be of interest to use a double windowing system in order to better target short stories . transcription word error rates of about 20 % have been reported for unrestricted broadcast news data in several languages . in particular the project addresses the development of generic speech recognition technology and methods to rapidly port technology to new domains and languages with limited supervision , and to produce enriched symbolic speech transcriptions . this research has been carried out in a multilingual environment in the context of several recent and ongoing european projects . the stemming lexicon contains about __32000__ entries and was constructed using porter ’ s algorithm on the most frequent words in the collection , and then manually corrected . as proposed in , we segment the audio stream into overlapping documents of a fixed duration . as a result of optimization , we chose a 30 second window duration with a 15 second overlap . this information can be used both directly and indirectly for indexation and retrieval purposes . automatic speech recognition is a key technology for audio and video indexing . __<s>__ this paper addresses recent progress in speakerindependent large vocabulary continuous speech recognition which has opened up a wide range of near and midterm applications __</s>__ __<s>__ one rapidly expanding application area is the processing of broadcast audio for information access __</s>__ __<s>__ at limsi broadcast news transcription systems have been developed for english french german mandarin and portuguese and systems for other languages are under development __</s>__ __<s>__ audio indexation must take into account the specificities of audio data such as needing to deal with the continuous data stream and an imperfect word transcription __</s>__ __<s>__ some nearterm applications areas are audio data mining selective dissemination of information and media monitoring __</s>__ __<s>__ this paper addresses recent progress in speakerindependent large vocabulary continuous speech recognition which has opened up a wide range of near and midterm applications __</s>__ __<s>__ one rapidly expanding application area is the processing of broadcast audio for information access __</s>__ __<s>__ at limsi broadcast news transcription systems have been developed for english french german mandarin and portuguese and systems for other languages are under development __</s>__ __<s>__ audio indexation must take into account the specificities of audio data such as needing to deal with the continuous data stream and an imperfect word transcription __</s>__ __<s>__ some nearterm applications areas are audio data mining selective dissemination of information and media monitoring __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper addresses recent progress in speakerindependent large vocabulary continuous speech recognition which has opened up a wide range of near and midterm applications one rapidly expanding application area is the processing of broadcast audio for information access at limsi broadcast news transcription systems have been developed for english french german mandarin and portuguese and systems for other languages are under development audio indexation must take into account the specificities of audio data such as needing to deal with the continuous data stream and an imperfect word transcription some nearterm applications areas are audio data mining selective dissemination of information and media monitoring\n",
            "INFO:tensorflow:GENERATED SUMMARY: power manually marked as reports , 40 % start without a manually annotated speaker change . although it is usually assumed that processing time is not a major issue since computer power has been increasing continuously , it is usually assumed that processing time is not a major issue since computer power has been increasing continuously , it is possible to transcribe the continuous stream of audio data without any prior segmentation , partitioning offers several advantages over this straightforward solution . stemming is possible to transcribe the continuous stream of audio data without any prior segmentation , partitioning offers\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the null element ultimately produces “ spurious ” french words . , and neither is a perfect translation . __translateandinsert__ changes this change makes decoding significantly more complex in mt ; instead of knowing the order of the input in advance , we must consider __allpermutations__ of __anword__ input sentence . even when the greedy decoder uses an __optimizedforspeed__ set of operations in which at most one word is translated , moved , or inserted at a time and at most __3wordlong__ segments are __swapped—which__ is labeled “ greedy ” in table __2—the__ translation accuracy is affected only slightly . the next step is to cast tour selection as an integer program . instead of deeply probing the search space , such greedy methods typically start out with a random , approximate solution and then try to improve it incrementally until a satisfactory solution is reached . this relative offset k a heuristic is used in a* search to estimate the cost of completing a partial hypothesis . when it starts from the gloss of the french sentence “ bien __entendu__ , il __parle__ de une __belle__ __victoire.__ ” , for example , the greedy decoder alters the initial alignment incrementally as shown in figure 2 , eventually producing the translation “ quite naturally , he talks about a great __victory.__ ” . one may also run the greedy decoder using a time threshold , as any instance of anytime algorithm . if a word has fertility greater than zero , we call it fertile . if an english word e translates into something at french position j , then the french head word of __eis__ stochastically placed in french position k with distortion probability d , class ) , where “ class ” refers to automatically determined word classes for french and english vocabulary items . with more than one stack , however , how does a multistack decoder choose which hypothesis to extend during each iteration ? if and only if travel from __hotelto__ __hotelis__ on the itinerary . , this is called a search error . we can solve ip instances with generic problemsolving software such as lp solve or __cplex.3__ in this section we explain tence f = __nullgenerated__ . if __eis__ the generic stack extend aligns an additional french word to the most recent english word , increasing its fertility . once the initial alignment is created , the greedy decoder tries to improve it , i.e. , tries to find an alignment of higher probability , by applying one of the following operations : __translateoneortwowords__ changes the translation of one or two french words , those located at positions and , from e and e into __eand__ e. how to express mt decoding in ip format . __greedyand__ __greedyare__ greedy decoders optimized for speed . first , every english word is assigned a fertility . __ebut__ a word of fertility 1 and __eis__ null , then __eis__ deleted from the translation . hotels are shown as small rectangles . the last two factors may not always coincide , as model 4 is an imperfect model of the translation __process—i.e__ . by building solutions incrementally and storing partial solutions , or hypotheses , in a “ stack ” , the decoder conducts an ordered search of the solution space . __<s>__ a good decoding algorithm is critical to the success of any statistical machine translation system __</s>__ __<s>__ the decoder’s job is to find the translation that is most likely according to set of previously learned parameters __</s>__ __<s>__ since the space of possible translations is extremely large typical decoding algorithms are only able to examine a portion of it thus risking to miss good solutions __</s>__ __<s>__ in this paper we compare the speed and output quality of a traditional stackbased decoding algorithm with two new __decoders:__ a fast greedy decoder and a slow but optimal decoder that treats decoding as an __integerprogramming__ optimization problem __</s>__ __<s>__ a good decoding algorithm is critical to the success of any statistical machine translation system __</s>__ __<s>__ the decoder’s job is to find the translation that is most likely according to set of previously learned parameters __</s>__ __<s>__ since the space of possible translations is extremely large typical decoding algorithms are only able to examine a portion of it thus risking to miss good solutions __</s>__ __<s>__ in this paper we compare the speed and output quality of a traditional stackbased decoding algorithm with two new __decoders:__ a fast greedy decoder and a slow but optimal decoder that treats decoding as an __integerprogramming__ optimization problem __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: a good decoding algorithm is critical to the success of any statistical machine translation system the decoder’s job is to find the translation that is most likely according to set of previously learned parameters since the space of possible translations is extremely large typical decoding algorithms are only able to examine a portion of it thus risking to miss good solutions in this paper we compare the speed and output quality of a traditional stackbased decoding algorithm with two new !!__decoders:__!! a fast greedy decoder and a slow but optimal decoder that treats decoding as an !!__integerprogramming__!! optimization problem\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is labeled “ greedy ” the null element ultimately produces “ spurious ” french words . , mt ; instead of knowing the order of the input mt ; instead of knowing the order of the input mt ; instead of knowing the order of the input mt ; instead of knowing the order of the input mt ; instead of knowing the order of the input mt ; instead of knowing the order of the input mt ; instead of knowing the order of the input mt ; instead of knowing the order of the input\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  we were particularly interested in those linguistic features that could aid in qualitative analysis , as we discuss in section 5. first , it had become apparent from failure analysis that mt system output tended to favor rightbranching structures over noun compounding . the first __nonselected__ feature is the presence of a word containing an extended ascii character , suggesting that general oov features were sufficient and subsume the effect of this narrower feature . the classifiers can be used for evaluating a system overall , providing feedback to aid in system development , and in evaluating individual sentences . in addition , only eight of approximately __5,200__ observed sentence templates turned out to be discriminatory . employ string edit distance between reference and output sentences to gauge output quality for mt and generation . the final five features are : we used a set of automated tools to construct decision trees based on the features extracted from the reference and mt sentences . the linguistic motivation behind this was twofold . in one visualization , shown in figure 2 , __dnetviewer__ allows the user to adjust a slider to see the order in which the features were selected during the heuristic search that guides the construction of decision trees . ➢ branching weight index for nps only ➢ modification index , i.e . the number of premodifiers minus the number of postmodifiers across all categories once an appropriate set of features has been selected and tools to automatically extract those features are in place , classifiers can be built and evaluated quickly . the models built using linguistic features , however , benefit substantially , with accuracy leveling off after 150,000 training cases . from the remainder of the corpus , we extracted 100,000 aligned sentence pairs . this yielded a set of 200,000 english sentences , one half of which were english reference sentences , and the other half of which were mt output . . we split the 200,000 english sentences __90/10__ , to yield __180,000__ sentences for training classifiers and 20,000 sentences that we used as heldout test data . in preliminary experiments , the accuracy of classifiers using support vector machines exceeded the accuracy of the decision tree classifiers by a little less than one percentage point using a linear kernel function , and by a slightly greater margin using a polynomial kernel function of degree three . secondly , subjectverb disagreement was also not predictive , validating the consistent enforcement of agreement constraints in the natural language generation component of the mt system . the features extracted fall into two broad categories : perplexity measures were extracted using the __cmucambridge__ statistical language modeling toolkit . this has been the motivation for using linguistic features in addition to perplexity measures . et al . since the training data and test data contain an even split between reference human translations and machine translations , the baseline for comparison is __50.00__ % . combining the two sets of features yields the highest accuracy , __82.89__ % . , the presence of out of vocabulary words is frequently caused by the direct transfer of source language words for which no translation could be found . __<s>__ we present a machine learning approach to evaluating the wellformedness of output of a machine translation system using classifiers that learn to distinguish human reference translations from machine translations __</s>__ __<s>__ this approach can be used to evaluate an mt system tracking improvements over time to aid in the kind of failure analysis that can help guide system development and to select among alternative output strings __</s>__ __<s>__ the method presented is fully automated and independent of source language target language and domain __</s>__ __<s>__ we present a machine learning approach to evaluating the wellformedness of output of a machine translation system using classifiers that learn to distinguish human reference translations from machine translations __</s>__ __<s>__ this approach can be used to evaluate an mt system tracking improvements over time to aid in the kind of failure analysis that can help guide system development and to select among alternative output strings __</s>__ __<s>__ the method presented is fully automated and independent of source language target language and domain __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we present a machine learning approach to evaluating the wellformedness of output of a machine translation system using classifiers that learn to distinguish human reference translations from machine translations this approach can be used to evaluate an mt system tracking improvements over time to aid in the kind of failure analysis that can help guide system development and to select among alternative output strings the method presented is fully automated and independent of source language target language and domain\n",
            "INFO:tensorflow:GENERATED SUMMARY: those linguistic features that could aid those linguistic features that could aid those linguistic features that could aid those linguistic features that could aid those linguistic features that could aid those linguistic features that could aid those linguistic features that could aid those linguistic features that could aid those linguistic features that could aid those linguistic features that could aid those linguistic features that could aid those linguistic features that could aid\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  these rule files specify : from this rule file , the compiler generates a java program , which is a __shallow−parser__ based on the rule file . future improvements of the tool will consist in adding a module to annotate syntactic functions , and complete valency information for verbs , with the help of a lexicon . this allows , for example , to obtain output for a coordination of __nps5__ . < __/rel__ contrary to the french treebank , the penn treebank contains __non−surfastic__ constructions such as empty nodes , and constituents that are not triggered by a lexical items . we present a very simple & ; toy & ; example which aims at identifying some nps in the penn treebank4 . following the linguistic tradition , we consider as function words all words associated to a pos which labels a __closed−class__ i.e . : determiners , prepositions , clitics , auxiliaries , pronouns , conjunctions , auxiliaries , * the second rule says that when a preposition is encountered , if the previous tag was also a preposition , nothing should be done . tu vu one can also compare the output of the parser to a piece of text which has been manually annotated just for the purpose of the evaluation . each rule is of the form : { preamble } disjunction of patterns then actions in this section the program takes as input a file containing rules . we then evaluated the output of the __shallow−parser__ to the test sentences . noun __adj*__ np rule 2 says that , when not inside an np , if a common noun , an adjective or a proper noun is encountered , then the current constituent should be closed and an np should be opened . we also set aside the 25 000 words that had been independently annotated in order to compare the output of the parser to a portion of the final treebank . hence our rules are declarative and unordered . to achieve this goal , we also present samples of parsed outputs we obtain , so that the reader may judge for himself/herself . > pierre < /np for closing brackets , we obtain a precision of 92.2 % and a recall of 91.4 % . for instance , contrary to english , < np we also explain why our approach is more tolerant to __pos−tagging__ errors . on the other hand , __finite−state__ techniques rely on the development of a large set of rules to capture all the ways a constituent can expend . > subordinating conjunctions are never < vn > __voit__ < __/vn__ > omitted when a subordinating clause starts . we then ran the __np−chunker__ on the output of the tagger , and still obtain a precision of 90.2 % and a recall of 92 % on the “ exact match ” np identification task : 2 presentation of the compiler our tool has been developed using javacc . since there is no backtracking , this allows an output in linear time . these lower results are normal , since the treebank contains attachments that the parser is not supposed to make . the actions __closewhenopen__ and __closewhenclose__ allow to perform some attachments . the first rule says that when a preposition is encountered , a pp should be opened . __<s>__ we present a __rule−based__ __shallow−__ parser compiler which allows to generate a robust __shallow−parser__ for any language even in the absence of training data by resorting to a very limited number of rules which aim at identifying constituent boundaries __</s>__ __<s>__ we contrast our approach to other approaches used for __shallow−parsing__ __</s>__ __<s>__ we present an evaluation of our tool for english and for french for several tasks __</s>__ __<s>__ we present a __rule−based__ __shallow−__ parser compiler which allows to generate a robust __shallow−parser__ for any language even in the absence of training data by resorting to a very limited number of rules which aim at identifying constituent boundaries __</s>__ __<s>__ we contrast our approach to other approaches used for __shallow−parsing__ __</s>__ __<s>__ we present an evaluation of our tool for english and for french for several tasks __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we present a !!__rule−based__!! !!__shallow−__!! parser compiler which allows to generate a robust __shallow−parser__ for any language even in the absence of training data by resorting to a very limited number of rules which aim at identifying constituent boundaries we contrast our approach to other approaches used for !!__shallow−parsing__!! we present an evaluation of our tool for english and for french for several tasks\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is proposed . future improvements of the tool will consist the penn treebank4 . following the linguistic tradition , we consider as function words all words associated to a pos which labels a common to annotate syntactic functions , to obtain output for a coordination of obtain output for a coordination of obtain output for a coordination of obtain output for a coordination of obtain output for a coordination of obtain output for a coordination of obtain output for a coordination of obtain output for a coordination of obtain output for a coordination of obtain output for\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  it is therefore questionable whether bod 's sampling technique can be scaled to larger domains such as the wsj portion in the penn treebank . while we have tested all subtree restrictions initially on the development set , we believe that it is interesting and instructive to report these subtree restrictions on the test set rather than reporting our best result only . however , we will not use bod 's monte carlo sampling technique from complete derivation forests , as this turned out to be prohibitive for wsj sentences . it could be the case that dop 's gain in parse accuracy with increasing subtree depth is due to the model becoming sensitive to the influence of lexical heads higher in the tree , and that this gain could also be achieved by a more compact model which associates each nonterminal with its headword , such as a __headlexicalized__ scfg . we therefore performed experiments with versions of dop1 where the base line subtree set is restricted to subtrees with a certain maximum depth . : the difference between using no and all __nonheadwords__ is 1.2 % in lp and 1.0 % in lr . let us illustrate the original dop model presented in bod , called dop1 , with a simple example . which uses statistics on __wordendings__ , hyphenation and capitalization . in this paper we will use bod 's __subtreetorule__ conversion method for studying the impact of various subtree restrictions on the wsj corpus . all trees were stripped off their semantic tags , coreference information and quotation marks . these rules are used to create a derivation forest for a sentence , and the most probable parse is computed by sampling a sufficiently large number of random derivations from the forest . there is an important question why maximal parse accuracy occurs with exactly these constraints . we see that the accuracy initially increases when the lexical context is enlarged , but that the accuracy decreases if the number of words in the subtree frontiers exceeds 12 words . we accomplished a set of experiments where unlexicalized subtrees of a certain minimal depth are deleted from the base line subtree set , while all lexicalized subtrees up to 12 words are retained . but since the relative frequency estimator has so far not been __outper__ formed by any other estimator for dop1 , we will stick to this estimator in the current paper . chiang observes that almost a quarter of all nonempty subjects in the wsj appear in such a configuration . each __corpussubtree__ t is converted into a contextfree rule r where the lefthand side of r corresponds to the root label of t and the righthand side of r corresponds to the frontier labels of t. indices link the rules to the original subtrees so as to maintain the subtree 's internal structure and probability . we also found that counts of subtrees with several __nonheadwords__ are important , resulting in improved parse accuracy over previous parsers tested on the wsj . it is not unlikely that these two opposite directions will finally converge to the same , true set of statistical dependencies for natural language parsing . __<s>__ we aim at finding the minimal set of fragments which achieves maximal parse accuracy in data oriented parsing __</s>__ __<s>__ experiments with the penn wall street journal treebank show that counts of almost arbitrary fragments within parse trees are important leading to improved parse accuracy over previous models tested on this treebank __</s>__ __<s>__ we isolate some dependency relations which previous models neglect but which contribute to higher parse accuracy __</s>__ __<s>__ we aim at finding the minimal set of fragments which achieves maximal parse accuracy in data oriented parsing __</s>__ __<s>__ experiments with the penn wall street journal treebank show that counts of almost arbitrary fragments within parse trees are important leading to improved parse accuracy over previous models tested on this treebank __</s>__ __<s>__ we isolate some dependency relations which previous models neglect but which contribute to higher parse accuracy __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we aim at finding the minimal set of fragments which achieves maximal parse accuracy in data oriented parsing experiments with the penn wall street journal treebank show that counts of almost arbitrary fragments within parse trees are important leading to improved parse accuracy over previous models tested on this treebank we isolate some dependency relations which previous models neglect but which contribute to higher parse accuracy\n",
            "INFO:tensorflow:GENERATED SUMMARY: it . let us illustrate the original dop model presented is therefore questionable whether bod 's sampling technique can be scaled to larger domains such as the wsj portion the penn treebank . let us illustrate the original dop model presented . let us illustrate the original dop model presented is therefore questionable whether bod 's sampling technique from complete derivation forests , as this turned out to be prohibitive for wsj sentences . it could be the case that dop 's gain\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  once the rule is learned , it can help identify other suffixes beginning in vowels . the input to our morphology induction system is a set of words , or lexicon , l. the remainder of this paper is organized as follows . all productive inflectional suffixes , as well as the __adjectivegenerating__ suffix __/y__ , are found by the time the input size reaches 1,000 words from the hansard english corpus and 4,000 words from the wsj corpus . it would move each stem that can be moved to good effect . for each suffix x , the loop through all nodes is restarted whenever a promotion is carried out . the empty string is considered to be a possible suffix but not a possible stem . the search algorithm explores a series of hypotheses , replacing its current hypothesis whenever a more probable one is found . for example , the suffixes discovered by the system described below could be used as one of the inputs to the yarowsky and wicentowski system . errors are underlined in table 2. gaussier reports on an explicitly probabilistic system that is based primarily on spellings , while __dejean__ describes a heuristic approach . then dem is the hypothesis such that : case of __prom__ and all properties that hold for __prom__ regardless of mi , x , or h also hold for dem . the steps are presented below , along with their probability distributions . these equations depend only on the following characteristics of the hypothesis : intuitively , the primary determinants of the probability of a hypothesis are the total number of characters in the stem and suffix sets and the number of suffixes x. one reason a given form might not be in the input is sampling error . theorem 1 let x a suffix in __psuff__ , h the 100 most common words in each corpus were also excluded , since these words tend not to have regular inflection . there are a few moderately productive derivational suffixes , such as er , which the sytem did not choose . thus , we set out to develop a system that yields very few false positives over a wide range of input sizes in multiple languages . but it is not clear whether this can be done without also outputting other common __wordendings__ that are not morphemes . we extracted input lexicons from each corpus , excluding words containing capital letters or nonalphabetic characters . one system attempted to find the set of suffixes in the language using only the spellings of the words ; another used the syntactic categories from a tagged corpus as well . we emphasize a low false positive rate because the __suffixdiscovery__ system reported here is intended as a first step toward a system for learning a complete morphological analysis , including irregular morphology , morphosyntax , and spelling adjustment rules . the system was run on the 500 , 1,000 , 2,000 , 4,000 , 8,000 , 16,000 , and 32,000 most common remaining words . the number of subsets of a given size is finite , so we can again use the uniform distribution . __<s>__ this paper describes a system for unsupervised learning of morphological affixes from texts or word lists __</s>__ __<s>__ the system is composed of a generative probability model and a search algorithm __</s>__ __<s>__ experiments on the wall street journal and the hansard corpus demonstrate the effectiveness of this approach __</s>__ __<s>__ the results suggest that more integrated systems for learning both affixes and morphographemic adjustment rules may be feasible __</s>__ __<s>__ in addition several definitions and a theorem are developed so that our search algorithm can be formalized in terms of the lattice formed by subsets of suffixes under inclusion __</s>__ __<s>__ this formalism is expected to be useful for investigating alternative search strategies over the same morphological hypothesis space __</s>__ __<s>__ this paper describes a system for unsupervised learning of morphological affixes from texts or word lists __</s>__ __<s>__ the system is composed of a generative probability model and a search algorithm __</s>__ __<s>__ experiments on the wall street journal and the hansard corpus demonstrate the effectiveness of this approach __</s>__ __<s>__ the results suggest that more integrated systems for learning both affixes and morphographemic adjustment rules may be feasible __</s>__ __<s>__ in addition several definitions and a theorem are developed so that our search algorithm can be formalized in terms of the lattice formed by subsets of suffixes under inclusion __</s>__ __<s>__ this formalism is expected to be useful for investigating alternative search strategies over the same morphological hypothesis space __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper describes a system for unsupervised learning of morphological affixes from texts or word lists the system is composed of a generative probability model and a search algorithm experiments on the wall street journal and the hansard corpus demonstrate the effectiveness of this approach the results suggest that more integrated systems for learning both affixes and morphographemic adjustment rules may be feasible in addition several definitions and a theorem are developed so that our search algorithm can be formalized in terms of the lattice formed by subsets of suffixes under inclusion this formalism is expected to be useful for investigating alternative search strategies over the same morphological hypothesis space\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is learned , it can help identify other suffixes beginning is learned , it can help identify other suffixes beginning is learned , it can help identify other suffixes beginning is learned , it can help identify other suffixes beginning is learned , it can help identify other suffixes beginning is learned , it can help identify other suffixes\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  that is , one may request a response from a listener by looking at the listener , and suppress the listener ’ s response by looking away . both turn and discourse structure exhibit an influence on posture shifts , with discourse having the most predictive value . this dialogue consists of two major segments : finding a house , and showing a house . a discourse segment is taken to be an aggregation of utterances and subsegments that convey the discourse segment purpose , which is an intention that leads to the segment initiation . when these categories are cotemporaneous with turn construction , then they are strongly predictive of gaze behavior . __ps/int__ energy __0.340__ __0.837__ __0.832__ __0.332__ __0.533__ __0.844__ __0.039__ __0.701__ __0.053__ __0.723__ thus , the system generates an end utterance posture shift 11 % of the time . combining information about discourse structure and conversation structure , the system decides on posture shifts for the beginning of the utterance and the end of the utterance . thus , the time points at which the assigned task topics were started served as segmentation points . on the other hand , during the execution phase of an utterance , speakers look more often at listeners . rea takes input from a microphone and two cameras in order to sense the user ’ s speech and gesture . in utterance , rea introduces a new discourse purpose . specifically , the beginning of themes are frequently accompanied by a __lookaway__ from the hearer , and the beginning of __rhemes__ are frequently accompanied by a __looktoward__ the hearer . in fact , the semantic and pragmatic compatibility in the gesturespeech relationship recalls the interaction of words and graphics in multimodal presentations . we used the instructions given to subjects concerning the topics to discuss as segmentation boundaries . in fact , although a number of conversational analysts and __ethnomethodologists__ have described posture shifts in conversation , their studies have been qualitative in nature , and difficult to reformulate as the basis of algorithms for the generation of language and posture . any posture shift that occurs between the end of one discourse segment and the beginning of the next is defined as an __interdiscourse__ segment posture shift . __interdseg/startturn__ __interdseg/midturn__ for this reason , we carried out our own empirical study . if rea decides to shift posture she selects a low energy posture shift using either upper or lower body . for example , if the combined factors match case , the system decides to generate a posture shift with 54 % probability for the beginning of the utterance . for relatively brief intervals normalization by number of __intersegment__ occurrences was sufficient , however , for long intervals we needed to normalize by time to obtain meaningful comparisons . in planning a next action , rea accesses the goal agenda in __collagen__ and gets the content of her next utterance . table 4.1.2 shows that turn structure does have an influence on posture shifts ; subjects were five times more likely to exhibit a shift at a boundary than within a turn . looking away from one __<s>__ this paper addresses the issue of designing embodied conversational agents that exhibit appropriate posture shifts during dialogues with human users __</s>__ __<s>__ previous research has noted the importance of hand gestures eye gaze and head nods in conversations between embodied agents and humans __</s>__ __<s>__ we present an analysis of human monologues and dialogues that suggests that postural shifts can be predicted as a function of discourse state in monologues and discourse and conversation state in dialogues __</s>__ __<s>__ on the basis of these findings we have implemented an embodied conversational agent that uses __collagen__ in such a way as to generate postural shifts __</s>__ __<s>__ this paper addresses the issue of designing embodied conversational agents that exhibit appropriate posture shifts during dialogues with human users __</s>__ __<s>__ previous research has noted the importance of hand gestures eye gaze and head nods in conversations between embodied agents and humans __</s>__ __<s>__ we present an analysis of human monologues and dialogues that suggests that postural shifts can be predicted as a function of discourse state in monologues and discourse and conversation state in dialogues __</s>__ __<s>__ on the basis of these findings we have implemented an embodied conversational agent that uses __collagen__ in such a way as to generate postural shifts __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper addresses the issue of designing embodied conversational agents that exhibit appropriate posture shifts during dialogues with human users previous research has noted the importance of hand gestures eye gaze and head nods in conversations between embodied agents and humans we present an analysis of human monologues and dialogues that suggests that postural shifts can be predicted as a function of discourse state in monologues and discourse and conversation state in dialogues on the basis of these findings we have implemented an embodied conversational agent that uses !!__collagen__!! in such a way as to generate postural shifts\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a response gaze behavior . when these categories are cotemporaneous with turn construction , then they are strongly predictive of gaze behavior is , , one may request a response from a listener by looking at the listener , suppress the listener ’ s response by looking away . both turn is , one may request a response from a listener by looking at the listener , speakers look more often at listeners . rea takes input from a microphone two cameras\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  therefore , only the words that have the same word connectivity for the following words and the preceding word belong to the same word class , and this word class definition can not represent the word connectivity attribute efficiently . let ’ s consider the condition such that only word sequence has sufficient frequency in sequence . let ’ s consider the case of different connectivity for the words that precede and follow . however , for word b , a word 2gram is used instead of the multiclass the value of word 3gram p can be used for the estimation of word c for the same reason . we have evaluated multiclass ngrams in perplexity as the next equations . based on this fact , effective word class definitions are required for high performance in class ngrams . 4. as in the previous example of ” a ” and ” an ” , following and preceding word connectivity are not always the same . better __wordclustering__ is to be considered based on word connectivity by the reflection neighboring characteristics in the corpus . however , accuracy of the word prediction capability will be lower than that of word ngrams with a sufficient size of training data , since the representation capability of the word dependent , unique connectivity attribute will be lost for the approximation base word class . as shown in this figure , multiclass 3grams result in lower perplexity than the conventional word 3gram , indicating the __reasonability__ of word clustering based on the distance2 2gram . but conventional class ngrams require a new cluster for the new word succession . multiclass composite 2grams are created in the following manner . in class ngrams , the transition probability of the next word from the previous n —1 word sequence is given in the next formula . in class ngrams with c classes , the number of estimated parameters is decreased from v n to cn . one is word 1grams of word successions as p . the performance and model size of class ngrams strongly depend on the definition of word classes . in this case , the value of word 2gram p can be used as a reliable value for the estimation of word b , as the frequency of sequence is sufficient . in this equation , the probability for word b is changed from a word 2gram to a class 3gram . both are classified by pos as an indefinite article , and are assigned to the same word class . and the number of increased parameters with the introduction of word successions is at most the number of word successions times 2. where we merge the classes whose merge cost __unew__ — __uold__ is the lowest . the vector for clustering is given in the next equation . in this paper , the multiclass assignment is proposed for effective word class definitions . in the above process , only 2 parameters are additionally used . therefore , efficient word clustering is needed to keep the reliability of 3grams after the clustering and a reasonable calculation cost . word ngrams have been widely used as a statistical language model for language processing . __<s>__ in this paper a new language model the multiclass composite ngram is proposed to avoid a data sparseness problem for spoken language in that it is difficult to collect training data __</s>__ __<s>__ the multiclass composite ngram maintains an accurate word prediction capability and reliability for sparse data with a compact model size based on multiple word clusters called __multiclasses__ __</s>__ __<s>__ in the multiclass the statistical connectivity at each position of the ngrams is regarded as word attributes and one word cluster each is created to represent the positional attributes __</s>__ __<s>__ furthermore by introducing higher order word ngrams through the grouping of frequent word successions multiclass ngrams are extended to multiclass composite ngrams __</s>__ __<s>__ in experiments the multiclass composite ngrams result in 9 5% lower perplexity and a 16% lower word error rate in speech recognition with a 40% smaller parameter size than conventional word 3grams __</s>__ __<s>__ in this paper a new language model the multiclass composite ngram is proposed to avoid a data sparseness problem for spoken language in that it is difficult to collect training data __</s>__ __<s>__ the multiclass composite ngram maintains an accurate word prediction capability and reliability for sparse data with a compact model size based on multiple word clusters called __multiclasses__ __</s>__ __<s>__ in the multiclass the statistical connectivity at each position of the ngrams is regarded as word attributes and one word cluster each is created to represent the positional attributes __</s>__ __<s>__ furthermore by introducing higher order word ngrams through the grouping of frequent word successions multiclass ngrams are extended to multiclass composite ngrams __</s>__ __<s>__ in experiments the multiclass composite ngrams result in 9 5% lower perplexity and a 16% lower word error rate in speech recognition with a 40% smaller parameter size than conventional word 3grams __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in this paper a new language model the multiclass composite ngram is proposed to avoid a data sparseness problem for spoken language in that it is difficult to collect training data the multiclass composite ngram maintains an accurate word prediction capability and reliability for sparse data with a compact model size based on multiple word clusters called !!__multiclasses__!! in the multiclass the statistical connectivity at each position of the ngrams is regarded as word attributes and one word cluster each is created to represent the positional attributes furthermore by introducing higher order word ngrams through the grouping of frequent word successions multiclass ngrams are extended to multiclass composite ngrams in experiments the multiclass composite ngrams result in 9 5% lower perplexity and a 16% lower word error rate in speech recognition with a 40% smaller parameter size than conventional word 3grams\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is not represent the word connectivity attribute efficiently . let ’ s consider the condition such that only word sequence has sufficient frequency sequence . let ’ s consider the case of different connectivity for the following words the preceding word belong to the same word class , this word class definition can not represent the word connectivity attribute efficiently . let ’ s consider the condition such that only word sequence has sufficient frequency\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the experiments were based on the fuse test suite , which is a balanced extract from four appointment scheduling dialogue corpora . we will explain each of these aspects in the following sections . note that super linear speedup is achieved with up to 12 processors , repeatedly for the same set of sentences . by using threads , we allow each parser to share the same memory space . al . the tasks are connected by directed arcs that indicate the execution dependencies . this algorithm can be seen as a wrapper around an existing sequential parser that takes care of combining the results of the individual threads . __macamba__ and __cali__ were both implemented in __objectivec__ and currently run on windows nt , linux , and __solaris__ . a thread will process the unification tasks on the agenda and , on success , will perform the resulting match task to match the new edge with the edges on its chart . in other words , as long as there is work on any queue , no thread should be idle . the first step of the analysis is to find an appropriate graph representation for parsing computations . __manousopoulou__ et the match task is also responsible for applying filtering techniques like the quick check . the parser that we will present in this paper uses the lingo grammar . grammars are readonly and can be read by all threads . our solution is based on a generation scheme . the presented model resembles a very finegrained scheme for distributing work , where each single unification tasks to be scheduled independently . as a result of step 3 , the agenda may become nonempty . for the second approach we show both the worst case , considering all possible __schedulings__ , and an average case . table 2 shows the results of these __experiments.5__ the execution times for each parse are measured in wall clock time . nevertheless , many of the presented solutions either did not yield acceptable speedup or were very specific to one application . some parsing systems require strict control over the order of evaluation of tasks . after a thread completes step 3 , it will always raise its generation . this ensures that each newly derived item needs to be visited only once by each thread . where t1 is the total work , or the execution time for the one processor case , and __t∞__ is the critical path . one obvious way to ensure that optimizations for sequential parsers can be used in a parallel parser as well is to let a parallel parser mimic a sequential parser as much as possible . in this case , __newwork__ will be set and step 2 is executed again . the overall peak is reached at 32 processors where the average speedup is __17.3.__ step 7 of __sched__ enforces that these conditions hold before any thread leaves __sched__ . it implements work __stealing__ using similar techniques . al . initially , each thread is assigned a different set of work , for example , resembling a different part of the input string . table 1 shows the results of the critical path analysis of both approaches . __<s>__ multiprocessor systems are becoming more commonplace and affordable __</s>__ __<s>__ based on analyses of actual parsings we argue that to exploit the capabilities of such machines unificationbased grammar parsers should distribute work at the level of individual unification operations __</s>__ __<s>__ we present a generic approach to parallel chart parsing that meets this requirement and show that an implementation of this technique for lingo achieves considerable speedups __</s>__ __<s>__ multiprocessor systems are becoming more commonplace and affordable __</s>__ __<s>__ based on analyses of actual parsings we argue that to exploit the capabilities of such machines unificationbased grammar parsers should distribute work at the level of individual unification operations __</s>__ __<s>__ we present a generic approach to parallel chart parsing that meets this requirement and show that an implementation of this technique for lingo achieves considerable speedups __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: multiprocessor systems are becoming more commonplace and affordable based on analyses of actual parsings we argue that to exploit the capabilities of such machines unificationbased grammar parsers should distribute work at the level of individual unification operations we present a generic approach to parallel chart parsing that meets this requirement and show that an implementation of this technique for lingo achieves considerable speedups\n",
            "INFO:tensorflow:GENERATED SUMMARY: the experiments were based on the fuse test suite , which is a balanced extract from four appointment scheduling dialogue corpora . we will explain each of these aspects the following sections . note that super linear speedup is achieved with up to 12 processors , repeatedly for the same set of . by using threads , we allow each parser to share the same memory space . al . the tasks are connected by directed arcs that indicate the execution of the individual threads were both implemented\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  an important aspect of this approach is the fact that the system learns rules that can be freely __intermixed__ with handengineered ones . the linguistic preprocessing stage involves some basic tasks : tokenisation , sentence splitting , partofspeech tagging and stemming . in reality , “ __ariane__ ii ” is a new company that should also be included in the lexicon database . the aim of this process is to recognise and classify the nes in the corpus . the system starts by generating exhaustively all candidate extraction patterns , using an earlier system called autoslog . in order to do this the knowledge base should contain information that “ __saintlouis__ ” is in missouri , and a rule should exist to interpret the __affixing__ of a parenthesis . the aim of the stemmer is to reduce the size of the lexicon as well as the size and complexity of the nerc grammar . classification involves three __substages__ : application of classification rules , __gazetteerbased__ classification , and partial matching of classified namedentities with unclassified ones . the reason for adopting this approach is that we are interested in the maintenance of a rulebased system through time . the partially supervised multilevel bootstrapping approach presented in induces a set of information extraction patterns , which can be used to identify and classify nes . __nymble__ uses statistical learning to acquire a hidden markov model that recognises nes in text . the performance of a nerc system degrades over time due to the introduction of new nes or the change in the meaning of existing ones . also in this case , the lexicon must be updated with a second entry for this entity , categorised as a company . our method bears some similarities with systems based on active learning . manual construction of nerc systems is a complicated and timeconsuming process , even for experts . it is based on a large knowledge base including 8,000 proper names that share 10,000 forms and consist of 11,000 words . the recogniser failed to separate the person name from its title , due to the last accented character of the word “ __πολιτισµού__ ” . for example , according to one of the rules , a sequence of two words , starting with capital letters , constitutes a person name if it is preceded by a definite article and the endings of these two words belong in a specific set that usually denote person names . in our present study , we control the rulebased nerc systems that have been presented in section 3 , with nerc systems constructed by the c4.5 algorithm . the learning method used in that case was a neural network . finally , section 6 concludes and presents our future plans . the exploitation of learning techniques to support this adaptation task has attracted the attention of researchers in language engineering . our method comprises two stages : the training stage , during which c4.5 constructs a new system using data generated by the rulebased system , and the deployment stage , in which the results of the two systems are compared on new data and their disagreements are used as signals for change in the rulebased system . __<s>__ this paper presents a method that assists in maintaining a rulebased namedentity recognition and classification system __</s>__ __<s>__ the underlying idea is to use a separate system constructed with the use of machine learning to monitor the performance of the rulebased system __</s>__ __<s>__ the training data for the second system is generated with the use of the rulebased system thus avoiding the need for manual tagging __</s>__ __<s>__ the disagreement of the two systems acts as a signal for updating the rulebased system __</s>__ __<s>__ the generality of the approach is illustrated by applying it to large corpora in two different languages: greek and french __</s>__ __<s>__ the results are very encouraging showing that this alternative use of machine learning can assist significantly in the maintenance of rulebased systems __</s>__ __<s>__ this paper presents a method that assists in maintaining a rulebased namedentity recognition and classification system __</s>__ __<s>__ the underlying idea is to use a separate system constructed with the use of machine learning to monitor the performance of the rulebased system __</s>__ __<s>__ the training data for the second system is generated with the use of the rulebased system thus avoiding the need for manual tagging __</s>__ __<s>__ the disagreement of the two systems acts as a signal for updating the rulebased system __</s>__ __<s>__ the generality of the approach is illustrated by applying it to large corpora in two different languages: greek and french __</s>__ __<s>__ the results are very encouraging showing that this alternative use of machine learning can assist significantly in the maintenance of rulebased systems __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper presents a method that assists in maintaining a rulebased namedentity recognition and classification system the underlying idea is to use a separate system constructed with the use of machine learning to monitor the performance of the rulebased system the training data for the second system is generated with the use of the rulebased system thus avoiding the need for manual tagging the disagreement of the two systems acts as a signal for updating the rulebased system the generality of the approach is illustrated by applying it to large corpora in two different languages: greek and french the results are very encouraging showing that this alternative use of machine learning can assist significantly in the maintenance of rulebased systems\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a learns the basic tasks : tokenisation , sentence splitting , partofspeech tagging stemming . stemming should this the knowledge base should contain information that “ the lexicon database . the linguistic preprocessing stage involves some basic tasks : tokenisation , sentence splitting , partofspeech tagging stemming . also the lexicon database . the partially supervised multilevel bootstrapping approach presented the lexicon database . the linguistic preprocessing stage involves some basic tasks : tokenisation , sentence splitting , partofspeech tagging\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  we have just launched a web questionnaire in order to have a better idea of our user community . regarding recall , we did the following partial inspection : we noted several short sentences ending in ? although sentence separation is a controversial issue , it is straightforward to dispose of sentence separation tags . the first thing we did was to check whether repeated extracts had been attributed the same classification . we therefore looked at 200 paragraphs with one single sentence ending in question or exclamation mark containing less than 8 words , and concluded that 41 cases could definitively be marked as titles , while no less than 85 of these cases where questions taken from interviews . then , we computed the distribution of the __52,665__ proper nouns identified by the program on the first million words of the corpus as shown in table 5 , and inspected manually those __1,017__ having a length larger or equal than four words . we are genuinely interested in increasing its value , and have , since corpus __release,4__ made available four patches . in the __cetempublico__ corpus , newspaper titles and subtitles , as well as author identifications , have been marked up as result of heuristic processing . and 2 ) however , we now regret this methodological flaw and would like to clean up a little more , or add back soccer results . let us first comment on the distribution options . so , instead of trying to produce other subjective criteria for evaluating a particularly delicate area , we decided to look at the amount of work needed to revise the sentence separation for a given purpose , as reported in section 4.2. a superficial comparison showed that __cetempublico__ contains a higher percentage of unrecognized words , both types and tokens , than other portuguese newspaper corpora . the asterisk & ; these extracts were randomly shuffled and numbered , and the final corpus was the ordered sequence of the extract numbers . we start by commenting on the distribution process , and then go on to analyse the corpus contents and the specific options chosen in its creation . we will not evaluate concordancing for human inspection , because we assume that this is a rather straightforward task for which __cetempublico__ is useful , especially because it requires direct scrutiny . there have also been more than four thousand queries via the web site which gives access to the corpus . or ! that should actually be tagged as titles . this is not such a good result as it seems , though , since it can be argued that subject classification is too high level to allow for definite results . second , more accompanying information in the cd would not hurt , instead of pointing to a web page as the only source : in fact , the assumption that everyone has access to the web while working with __cetempublico__ is not necessarily true in portugal or brazil . it is not obvious whether one can automatically identify which one is the revised version , or even whether it is desirable to choose that one . __<s>__ in this paper we present a thorough evaluation of a corpus resource for portuguese __cetempúblico__ a __180million__ word newspaper corpus free for r&d in portuguese processing __</s>__ __<s>__ we provide information that should be useful to those using the resource and to considerable improvement for later versions __</s>__ __<s>__ in addition we think that the procedures presented can be of interest for the larger nlp community since corpus evaluation and description is unfortunately not a common exercise __</s>__ __<s>__ in this paper we present a thorough evaluation of a corpus resource for portuguese __cetempúblico__ a __180million__ word newspaper corpus free for r&d in portuguese processing __</s>__ __<s>__ we provide information that should be useful to those using the resource and to considerable improvement for later versions __</s>__ __<s>__ in addition we think that the procedures presented can be of interest for the larger nlp community since corpus evaluation and description is unfortunately not a common exercise __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in this paper we present a thorough evaluation of a corpus resource for portuguese !!__cetempúblico__!! a !!__180million__!! word newspaper corpus free for r&d in portuguese processing we provide information that should be useful to those using the resource and to considerable improvement for later versions in addition we think that the procedures presented can be of interest for the larger nlp community since corpus evaluation and description is unfortunately not a common exercise\n",
            "INFO:tensorflow:GENERATED SUMMARY: launched a web questionnaire launched a web questionnaire launched a web questionnaire community . regarding recall , we did the following partial inspection : we noted several short had been attributed the same classification . we therefore looked at 200 paragraphs with one single sentence ending ? although sentence separation is a controversial issue , it is straightforward to dispose of sentence separation tags . the first thing we did was to check whether repeated extracts had been attributed the same classification . the first thing we did was to check whether repeated extracts had been attributed the same classification\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  subjects were 39 students , 20 native speakers of standard american english and 19 nonnative speakers ; 16 subjects were female and 23 male . where ‘ + ’ or ‘ – ’ indicates that the feature value of the first category is either significantly higher or lower than the second . to identify the different turn categories in the corpus , two authors independently labeled each turn as to whether or not it constituted a correction of a prior system failure and what turn was being corrected , and whether or not it represented an aware site for a prior failure , and , if so , the turn which the system had failed on . only once does a system feature appear , suggesting that the rules generalize beyond the experimental conditions of the data collection . it is also interesting to see that the three types of __posterror__ turns are indeed prosodically different : __awares__ are less prominent in terms of f0 and rms maximum than __corrawares__ , which , in turn , are less prominent than corrections , for example . indeed , when we predict the three classes instead of four , we do improve in predictive power — from __74.23__ % to __81.14__ % classification success . aware sites turns where a user , while interacting with a machine , first becomes aware that the system has misrecognized a previous user turn . we first calculated mean values for each prosodic feature for each of the four turn categories produced by each individual speaker . 194 of the __2320__ turns were rejected by the system . for example , it is not until turn __1161__ that the user first becomes aware of the error in date and time from __1158__ ; the user then corrects the error in __1162.__ the toot corpus was collected using an experimental sds developed for the purpose of comparing differences in dialogue strategy . a subset of the features includes the automatically computable raw prosodic features shown in table 1 , and normalized versions of these features , where normalization was done by first turn or by previous turn in a dialogue . finally , given that our previous studies showed that preceding dialogue context may affect correction behavior , we included a feature reflecting the distance of the current turn from the beginning of the dialogue , and a set of features summarizing aspects of the prior dialogue : for the latter features , we included both the number of times prior turns exhibited certain characteristics and the percentage of the prior dialogue containing one of these features . in a third set of experiments , we merged corrections with normal turns to form a 2way distinction over all between aware turns and all others . in section 3 , we present some descriptive findings on different turn categories in toot . in section 5 we summarize our conclusions . table 4 shows a confusion matrix for the four classes , produced by matrix clearly shows a tendency for the minority classes , aware , corr and __corraware__ , to be falsely classified as normal . in the current study , we focus on another turn category that is potentially useful in error handling . __<s>__ this paper focuses on the analysis and prediction of socalled aware sites defined as turns where a user of a spoken dialogue system first becomes aware that the system has made a speech recognition error __</s>__ __<s>__ we describe statistical comparisons of features of these aware sites in a train timetable spoken dialogue corpus which reveal significant prosodic differences between such turns compared with turns that ‘correct’ speech recognition errors as well as with __‘normal’__ turns that are neither aware sites nor corrections __</s>__ __<s>__ we then present machine learning results in which we show how prosodic features in combination with other automatically available features can predict whether or not a user turn was a normal turn a correction and/or an aware site __</s>__ __<s>__ this paper focuses on the analysis and prediction of socalled aware sites defined as turns where a user of a spoken dialogue system first becomes aware that the system has made a speech recognition error __</s>__ __<s>__ we describe statistical comparisons of features of these aware sites in a train timetable spoken dialogue corpus which reveal significant prosodic differences between such turns compared with turns that ‘correct’ speech recognition errors as well as with __‘normal’__ turns that are neither aware sites nor corrections __</s>__ __<s>__ we then present machine learning results in which we show how prosodic features in combination with other automatically available features can predict whether or not a user turn was a normal turn a correction and/or an aware site __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper focuses on the analysis and prediction of socalled aware sites defined as turns where a user of a spoken dialogue system first becomes aware that the system has made a speech recognition error we describe statistical comparisons of features of these aware sites in a train timetable spoken dialogue corpus which reveal significant prosodic differences between such turns compared with turns that ‘correct’ speech recognition errors as well as with !!__‘normal’__!! turns that are neither aware sites nor corrections we then present machine learning results in which we show how prosodic features in combination with other automatically available features can predict whether or not a user turn was a normal turn a correction and/or an aware site\n",
            "INFO:tensorflow:GENERATED SUMMARY: subjects , 20 native speakers of standard american english 19 nonnative speakers ; 16 subjects were female 23 male . where ‘ + ’ or ‘ – ’ indicates that the feature value of the first category is either significantly higher or lower than the second . to identify the different turn categories the corpus , two authors independently labeled each turn as to whether or not it represented an aware site for a prior failure , was , if so , the turn which example . indeed , when we predict the three classes instead of four , we\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  thus , we use as the retrieval module “ google , ” which is one of the major search engines and does not conduct query __expansion1__ . we found that our webbased encyclopedia improved the system coverage obtained solely with an existing dictionary . in the following two sections , we explain methods to realize the domain and description models , respectively . since past examinations and answers are open to the public , we can evaluate the performance of our qa system with minimal cost . these results indicate that our method for generating encyclopedias is of operational quality . to avoid this problem , we normalize p by the number of words contained in d. encyclopedias generated through our webbased method can be used in a number of applications , including human usage , thesaurus production and natural language understanding in general . in a typical case , a term in question is highlighted as a heading with tags such as < dt > , < b > and < hx > , followed by its description . among a number of existing categorization methods , we experimentally used one proposed by __iwayama__ and tokunaga , which formulates p as in equation . in our case , a question and four possible answers correspond to query and document collection , respectively . however , words that are representative for a domain tend to be frequently used in compound word entries associated with the domain , and thus our method is a practical approximation . the retrieval module searches the web for pages containing an input term , for which existing web search engines can be used , and those with broad coverage are desirable . the second rule is based on html layout . thus , as performed in the first experiment , we used the __nichigai__ computer dictionary as a baseline encyclopedia . although clustering is optionally performed , resultant clusters are not necessarily related to explicit criteria , such as word senses and domains . for example , different senses for “ pipeline ” are associated with the computer and construction domains , respectively . for each test term , our method first computed p using equation and discarded domains whose p was below 0.05. thus , in practice the description model is approximated solely with the language model as in equation . to sum up , in principle we select d while coverage can be estimated objectively and systematically , estimating accuracy relies on human subjects , and thus is expensive . we analyzed those descriptions from different perspectives . accuracy is the ratio between the number of correct answers and the total number of answers made by the system . a reasonable method is that while the system periodically updates the encyclopedia offline , terms unindexed in the encyclopedia are dynamically processed in realtime usage . in handcrafted encyclopedias , term descriptions are carefully organized based on domains and word senses , which are especially effective for human usage . in view of this problem , we targeted information technology engineers __examinations4__ , which are __biannual__ examinations necessary for candidates to qualify to be it engineers in japan . __<s>__ we propose a method to generate largescale encyclopedic knowledge which is valuable for much nlp research based on the web __</s>__ __<s>__ we first search the web for pages containing a term in question __</s>__ __<s>__ then we use linguistic patterns and html structures to extract text fragments describing the term __</s>__ __<s>__ finally we organize extracted term descriptions based on word senses and domains __</s>__ __<s>__ in addition we apply an automatically generated encyclopedia to a question answering system targeting the japanese __informationtechnology__ engineers examination __</s>__ __<s>__ we propose a method to generate largescale encyclopedic knowledge which is valuable for much nlp research based on the web __</s>__ __<s>__ we first search the web for pages containing a term in question __</s>__ __<s>__ then we use linguistic patterns and html structures to extract text fragments describing the term __</s>__ __<s>__ finally we organize extracted term descriptions based on word senses and domains __</s>__ __<s>__ in addition we apply an automatically generated encyclopedia to a question answering system targeting the japanese __informationtechnology__ engineers examination __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we propose a method to generate largescale encyclopedic knowledge which is valuable for much nlp research based on the web we first search the web for pages containing a term in question then we use linguistic patterns and html structures to extract text fragments describing the term finally we organize extracted term descriptions based on word senses and domains in addition we apply an automatically generated encyclopedia to a question answering system targeting the japanese !!__informationtechnology__!! engineers examination\n",
            "INFO:tensorflow:GENERATED SUMMARY: generated through our webbased method our our webbased encyclopedia improved the system coverage obtained solely with an existing dictionary . since past examinations the following two sections , we explain methods to use as the retrieval module “ google , ” which is one of the major search engines does not conduct query . we found that our webbased encyclopedia improved the system coverage obtained solely with an existing dictionary . since past examinations the following two sections , we explain methods to use as the retrieval module “ google , ” which is one of the major search engines\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the second set consists of __pnv__ triples extracted from an 8 million word portion of the frankfurter rundschau corpus4 , in which partofspeech tags and minimal pps were __identified.5__ examining the precision and recall graphs in more detail , we find that for the adjn data , loglikelihood and ttest lead to the best results , with loglikelihood giving an overall better result than the ttest . manual annotation was performed for adjn pairs with frequency and __pnv__ triples with only . loglikelihood is second best but achieves the best results for highfrequency adjn data . here ttest outperforms loglikelihood , and even precision gained by frequency is better than or at least comparable to loglikelihood . the major drawback of an approach where all lowfrequency candidates are excluded is that a large part of the data is lost for collocation extraction . samples comprising particular frequency strata are examined . the dotted horizontal line represents the percentage of true collocations in the base set . general statistics for the adjn and __pnv__ base sets are given in table 1. while precision measures the quality of the best lists produced , recall measures their coverage , i.e. , how many of all true collocations in the corpus were identified . , lemmatized adjn pairs extracted from a small corpus of freely available german law __texts.3__ due to the extraction strategy , the data are homogeneous and grammatically correct , i.e. , there is always a grammatical dependency between adjacent adjectives and nouns in running text . for instance , there is a widely held belief we apply a onetailed statistical test based on the probabilities to our samples in order to obtain an upper estimate for the actual proportion of collocations among the lowfrequency data : the estimate is accepted at a given significance level if . ) was much lower as the full curves show , loglikelihood is obviously the best measure . the base sets also differ with respect to syntactic homogeneity and grammatical correctness . the first set comprises bigrams of adjacent as the frequency distribution of word combinations in texts is characterised by a large number of rare events , lowfrequency data are a serious challenge for ams . see section 3 for a description of the base data . the picture differs slightly for the __pnv__ data . in contrast to our expectation stated at the beginning of this section , the performance of and relative to the other ams is not better for highfrequency data than for lowfrequency data . only main verbs were considered and full forms were reduced to __bases.6__ the __pnv__ data are partially inhomogeneous and not fully grammatically correct , because they include combinations with no grammatical relation between pn and the axis represents the percentage of data processed in the respective sl , while the axis represents the precision values achieved . we have also shown that the evaluation results and the ranking of ams differ depending on the kind of collocations to be identified , and the proportion of hapaxes in the candidate sets . __<s>__ this paper presents methods for a qualitative unbiased comparison of lexical association measures and the results we have obtained for adjectivenoun pairs and __prepositionnounverb__ triples extracted from german corpora __</s>__ __<s>__ in our approach we compare the entire list of candidates sorted according to the particular measures to a reference set of manually identified __“true__ __positives”__ __</s>__ __<s>__ we also show how estimates for the very large number of __hapaxlegomena__ and double occurrences can be inferred from random samples __</s>__ __<s>__ this paper presents methods for a qualitative unbiased comparison of lexical association measures and the results we have obtained for adjectivenoun pairs and __prepositionnounverb__ triples extracted from german corpora __</s>__ __<s>__ in our approach we compare the entire list of candidates sorted according to the particular measures to a reference set of manually identified __“true__ __positives”__ __</s>__ __<s>__ we also show how estimates for the very large number of __hapaxlegomena__ and double occurrences can be inferred from random samples __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper presents methods for a qualitative unbiased comparison of lexical association measures and the results we have obtained for adjectivenoun pairs and !!__prepositionnounverb__!! triples extracted from german corpora in our approach we compare the entire list of candidates sorted according to the particular measures to a reference set of manually identified !!__“true__!! !!__positives”__!! we also show how estimates for the very large number of !!__hapaxlegomena__!! and double occurrences can be inferred from random samples\n",
            "INFO:tensorflow:GENERATED SUMMARY: the second set consists of triples extracted from an 8 million word portion of the frankfurter rundschau corpus4 , base set . general statistics for the adjn base sets are given the corpus were identified . , lemmatized adjn pairs extracted from a small corpus of freely available german law extracted from an 8 million word portion of the frankfurter rundschau corpus4 , which is that a large part of the data is a widely held belief we apply a onetailed statistical test based on the probabilities to our samples partofspeech precision measures the quality of the best lists produced\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  dagan et the classbased approach , on the other hand , relies on wordnet , a lexical taxonomy that can be expected to cover most senses of a given lexical item . in the case of compound nouns , we only included sequences of two nouns , and considered the rightmost occurring noun as the head . __distanceweighted__ averaging induces classes of similar words from word cooccurrences without making reference to a taxonomy . we showed that the recreated frequencies are significantly correlated with plausibility judgements . al . we use the weighted average of the evidence provided by the similar words , where the weight given to a word __w1__ depends on its similarity to w1 and ) . we evaluated the recreated frequencies by comparing them with plausibility judgements elicited from human subjects . several measures of distributional similarity have been proposed in the literature . in this way , each subject can establish their own rating scale , thus yielding maximally __finegraded__ data and avoiding the known problems with the conventional ordinal scales for linguistic data . classes can be induced directly from the corpus or taken from a manually crafted taxonomy . our results provide independent evidence for the validity of the smoothing techniques we employed . taxonomic classbased smoothing and __distanceweighted__ __averaging.1__ computation of the jensenshannon divergence depends only on the linguistic contexts w2 which the two words w1 and __w1__ have in common . we also examined the effect of varying one further parameter . this means that predicates which impose few restrictions on their arguments have low selectional association values , whereas predicates selecting for a restricted number of arguments have high selectional association values . as a predictor of plausibility , cooccurrence frequency has the obvious limitation that it can not be applied to adjectivenoun pairs that never occur in the corpus . we applied two smoothing techniques in order to recreate cooccurrence frequency and found that the classbased smoothing method was the best predictor of plausibility . these adjectives were chosen to be minimally ambiguous : each adjective had exactly two senses according to wordnet and was unambiguously tagged as ‘ adjective ’ 98.6 % of the time , measured as the number of different partofspeech tags assigned to the word in the bnc . this simplification is necessary unless we have a corpus of adjectivenoun pairs labelled explicitly with taxonomic __information.2__ consider the pair proud chief which is not attested in the british national corpus . al . . to summarise , it appears that __distanceweighted__ averaging smoothing is only partially successful in reproducing the linguistic dependencies that characterise and constrain the formation of adjectivenoun combinations . intersubject agreement gives an upper bound for the task and allows us to interpret how well the smoothing techniques are doing in relation to the human judges . we can estimate the conditional probability p of the unseen word pair __w2w1__ by combining estimates for cooccurrences involving similar words : where s is the set of words most similar to w1 , w is the similarity function between w1 and __w1__ , and n is a normalising factor n = __∑w1__ __<s>__ previous research has shown that the plausibility of an adjectivenoun combination is correlated with its corpus cooccurrence frequency __</s>__ __<s>__ in this paper we estimate the cooccurrence frequencies of adjectivenoun pairs that fail to occur in a 100 million word corpus using smoothing techniques and compare them to human plausibility ratings __</s>__ __<s>__ both classbased smoothing and __distanceweighted__ averaging yield frequency estimates that are significant predictors of rated plausibility which provides independent evidence for the validity of these smoothing techniques __</s>__ __<s>__ previous research has shown that the plausibility of an adjectivenoun combination is correlated with its corpus cooccurrence frequency __</s>__ __<s>__ in this paper we estimate the cooccurrence frequencies of adjectivenoun pairs that fail to occur in a 100 million word corpus using smoothing techniques and compare them to human plausibility ratings __</s>__ __<s>__ both classbased smoothing and __distanceweighted__ averaging yield frequency estimates that are significant predictors of rated plausibility which provides independent evidence for the validity of these smoothing techniques __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: previous research has shown that the plausibility of an adjectivenoun combination is correlated with its corpus cooccurrence frequency in this paper we estimate the cooccurrence frequencies of adjectivenoun pairs that fail to occur in a 100 million word corpus using smoothing techniques and compare them to human plausibility ratings both classbased smoothing and __distanceweighted__ averaging yield frequency estimates that are significant predictors of rated plausibility which provides independent evidence for the validity of these smoothing techniques\n",
            "INFO:tensorflow:GENERATED SUMMARY: on can be expected to cover most senses of a given lexical item the literature . we also examined the effect of varying one further parameter . this means that predicates which impose few restrictions on their arguments have low selectional association values , whereas predicates selecting for a restricted number of arguments have high selectional association values . as a predictor of plausibility , cooccurrence frequency has the obvious limitation that it\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  as a logical notation quickly becomes unwieldy , we use graphics instead . figure 3 shows an example parsing matrix for the string intuitively , a negative node variable can be thought of as an open valency which must be filled exactly once by a positive node variable while a neutral node variable is a variable that may not be identified with any other node variable . the marking is arbitrary : it does not matter which proposition is the head as long as each semantic representation has exactly one head . for instance , a neodavidsonian semantics would associate e.g . with the verb “ run ” or with the past tensed “ ran ” . the __fierce__ little brown cat likes a tame yellow fox 3 __8.0s__ topdown guidance from the tag trees to avoid typical bottomup shortcomings such as the requirement for grammar semantic monotonicity and by implementing an axiomatic view of grammar , it supports a __neardeterministic__ treatment of intersective modifiers . this ensures strong constraint propagation and thereby reduces nondeterminism . node variables can have positive , negative or neutral polarity which are represented by black , white and gray nodes respectively . shows that constraint programming can be used to implement a model generator for tree logic . on the one hand , they require that the grammar be semantically monotonic that is , that the semantics of each daughter in a rule subsumes some portion of the mother semantics . the first solution yields the sentence a description grammar is a set of lexical entries of the form where is a tree description and is the semantic representation associated with . if we restrict our attention to grammars where every lexical tree description has exactly one anchor and assuming that each word is associated with exactly one lexical entry , then parsing a sentence consists in finding the saturated model with yield first , the polarities are used in a systematic way as follows . and similarly , whether the description based treatment of discourse parsing sketched in could be used to generate discourse . we first show that these restrictions are too strong . specifically , the global constraint stating that the semantics of a solution tree must be identical with it would be better for the decomposition of the input semantics to be specified by the lexical lookup phase , rather than by means of a language independent partitioning procedure . like all these frameworks , dg uses tree descriptions and thereby benefits first , from the extended domain of locality which makes tag particularly suitable for generation ) and second , from the monotonicity which differentiates descriptions from trees with respect to adjunction ) . on the theoretical side , the problem is that the partitioning is made independent of grammatical knowledge . lexical lookup only returns these categories in the grammar whose semantics subsumes some portion of the input semantics . moreover , dg differs from interaction grammars in that it uses a flat rather than a montague style recursive semantics thereby permitting a simple syntax/semantics interface . __<s>__ while the generative view of language processing builds bigger units out of smaller ones by means of rewriting steps the axiomatic view eliminates invalid linguistic structures out of a set of possible structures by means of wellformedness principles __</s>__ __<s>__ we present a generator based on the axiomatic view and argue that when combined with a __taglike__ grammar and a flat semantics this axiomatic view permits avoiding drawbacks known to hold either of topdown or of bottomup generators __</s>__ __<s>__ while the generative view of language processing builds bigger units out of smaller ones by means of rewriting steps the axiomatic view eliminates invalid linguistic structures out of a set of possible structures by means of wellformedness principles __</s>__ __<s>__ we present a generator based on the axiomatic view and argue that when combined with a __taglike__ grammar and a flat semantics this axiomatic view permits avoiding drawbacks known to hold either of topdown or of bottomup generators __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: while the generative view of language processing builds bigger units out of smaller ones by means of rewriting steps the axiomatic view eliminates invalid linguistic structures out of a set of possible structures by means of wellformedness principles we present a generator based on the axiomatic view and argue that when combined with a !!__taglike__!! grammar and a flat semantics this axiomatic view permits avoiding drawbacks known to hold either of topdown or of bottomup generators\n",
            "INFO:tensorflow:GENERATED SUMMARY: is the semantic representation associated with . if we restrict our attention to grammars where every a logical notation quickly becomes unwieldy , we use graphics instead . figure 3 shows an example parsing matrix for the string intuitively , a negative node variable can be thought of as an open valency which must be filled exactly once by a positive node variable by implementing an axiomatic view of grammar , it supports a parsing matrix for the string intuitively , a negative node variable can be thought of as an open valency which must be filled exactly once by\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  we found several different bracketing patterns and searched for these patterns , but one can not be certain that no other bracketing patterns were used in the ptb . one possible explanation is that there are in fact other features , which we have not yet identified , and for which the surfaceoriented features are __standins__ . discourse structure : are the discourse segments containing the antecedent and candidate directly related in the discourse structure ? the question arises where in this architecture the decision about vpe should be made . finally , we consider architecture option tp , in which the vpe decision is made right after text planning , and only semantic and discourse features are available . we call this corpus balanced ; clearly balanced does not reflect the distribution of vpe in naturally occurring text , as does __sections5+6__ ; we therefore use it only in examining factors affecting vpe in section 4 , and we do not use it in algorithm evaluation in section 5. we see that vpe does not often occur across quotes , and that it occurs unusually frequently within quotes , suggesting that it is more common in spoken language than in written language . “ directly related ” means that the two vps are in the same segment , the segments are directly related to each other , or the segments are both directly related to the same third discourse segment . therefore , when summarizing the data , we report three figures : for the negative cases , all from __sections5+6__ ; for the positive cases in __sections5+6__ ; and for the positive cases in balanced . these parameter settings are of two types : first , parameters internal to ripper , such as the number of optimization passes ; and second , the specification of which attributes are used . the result is a corpus with 111 negative examples – those from __sections5+6__ – and 121 positive examples . the positive examples were taken from the corpus collected in previous work . otherwise default to no vpe . in this paper , we identify factors which govern the decision to elide vps . measures distance between possible antecedent and candidate , in words . these two features can be annotated highly reliably . the frequency distributions are shown in figure 2 . __=com__ . this rule accounts for 6 cases correctly and misclassified none . however , before we can use ripper , we must discuss the issue of how our new trainable vpe module fits into the architecture of generation . for this reason we use the machine learning system ripper . first , candidate examples were identified automatically if there were two occurrences of the same verb , separated by fewer than 10 intervening verbs . in this example , the italicized vp could be elided , since it has a nearby antecedent with the same meaning . tasks in the generation process have been divided into three stages : the text planner has access only to information about communicative goals , the discourse context , and semantics , and generates a nonlinguistic representation of text structure and content . __<s>__ we present conditions under which verb phrases are elided based on a corpus of positive and negative examples __</s>__ __<s>__ factor that affect verb phrase ellipsis include: the distance between antecedent and ellipsis site the syntactic relation between antecedent and ellipsis site and the presence or absence of adjuncts __</s>__ __<s>__ building on these results we examine where in the generation architecture a trainable algorithm for vp ellipsis should be located __</s>__ __<s>__ we show that the best performance is achieved when the trainable module is located after the realizer and has access to surfaceoriented features __</s>__ __<s>__ we present conditions under which verb phrases are elided based on a corpus of positive and negative examples __</s>__ __<s>__ factor that affect verb phrase ellipsis include: the distance between antecedent and ellipsis site the syntactic relation between antecedent and ellipsis site and the presence or absence of adjuncts __</s>__ __<s>__ building on these results we examine where in the generation architecture a trainable algorithm for vp ellipsis should be located __</s>__ __<s>__ we show that the best performance is achieved when the trainable module is located after the realizer and has access to surfaceoriented features __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we present conditions under which verb phrases are elided based on a corpus of positive and negative examples factor that affect verb phrase ellipsis include: the distance between antecedent and ellipsis site the syntactic relation between antecedent and ellipsis site and the presence or absence of adjuncts building on these results we examine where in the generation architecture a trainable algorithm for vp ellipsis should be located we show that the best performance is achieved when the trainable module is located after the realizer and has access to surfaceoriented features\n",
            "INFO:tensorflow:GENERATED SUMMARY: patterns , but one , but one , but one , but one , but one , but one , but one , but one architecture several different bracketing patterns were used the ptb . one possible explanation the ptb . one possible explanation the ptb . one possible explanation the ptb . one possible explanation the ptb . one possible explanation the ptb . one possible explanation the ptb . one possible explanation the ptb . one possible explanation the ptb . one possible explanation the ptb . one possible explanation the ptb . one possible explanation is that\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  we use the usual __equalweight__ formula for fmeasure : we have used an hmm tagger in the usual sourcechannel setting , finetuned to perfection using a 3gram tag language model , a __tagtoword__ lexical model using bigram histories instead of just __samethus__ the hmm tagger outputs a sequence of tags according to the usual equation the tagger has been trained in the usual way , using part of the training data as heldout data for smoothing of the two models employed . thus , the rules are assigned reliabilities which divide the rules into reliability classes , with the most reliable group of rules applied first and less reliable groups of rules being applied in subsequent steps . having performed this , the whole system is restarted , which means that the next rule is applied on the changed input . however , in cases like this , it is more appropriate to add another condition to the context of such a rule rather than discard the rule as a whole . smoothing has been done first without using buckets , and then with them to show the difference . from languages we are acquainted with , the method has been applied on a larger scale only to english , , and french . this knowledge is primarily based on the study of types of morphological ambiguity occurring in czech . the overall strategy of the rule system is to keep the highest recall possible and gradually improve precision . table 5 contains the final evaluation of the main contribution of this paper . that way , the ambiguity of the input __text4__ decreases . thus , the rules use as wide context as possible with no context limitations being imposed in advance . when the two systems are coupled together , the manual rules are run first , and then the hmm tagger runs as usual , except it selects from only those tags retained at individual tokens by the manual rule component , instead of from all tags as produced by the morphological analyzer : the morphological analyzer is run on the test data set . however , also in the rulebased approach , there is the usual tradeoff between precision and recall . we believe that the key to this success is both the increased data size and the meticulous implementation of smoothing with bucketing together with using all possible tag trigrams , which has never been done before . we have decided to go for the “ perfect ” solution : to keep 100 % recall , or very close to it , and gradually improve precision by writing rules which eliminate more and more incorrect tags . the work described herein has been supported by the following grants : __mˇsmt__ __ln00a063__ , __mˇsmt__ me 293 , and __gaˇcr__ __405/96/k214__ . the casual morphological ambiguity is lexically specific and hence can not be investigated via __paradigmatics__ . there is no threshold being applied for low counts . the system of handwritten rules for czech has a twofold objective : the improvements obtained beat the pure statistical classifier combination , which obtained only 3 % relative improvement . currently , the system consists of 80 rules . __<s>__ a hybrid system is described which combines the strength of manual __rulewriting__ and statistical learning obtaining results superior to both methods if applied separately __</s>__ __<s>__ the combination of a rulebased system and a statistical one is not parallel but __serial:__ the rulebased system performing partial disambiguation with recall close to 100% is applied first and a trigram hmm tagger runs on its results __</s>__ __<s>__ an experiment in czech tagging has been performed with encouraging results __</s>__ __<s>__ a hybrid system is described which combines the strength of manual __rulewriting__ and statistical learning obtaining results superior to both methods if applied separately __</s>__ __<s>__ the combination of a rulebased system and a statistical one is not parallel but __serial:__ the rulebased system performing partial disambiguation with recall close to 100% is applied first and a trigram hmm tagger runs on its results __</s>__ __<s>__ an experiment in czech tagging has been performed with encouraging results __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: a hybrid system is described which combines the strength of manual !!__rulewriting__!! and statistical learning obtaining results superior to both methods if applied separately the combination of a rulebased system and a statistical one is not parallel but !!__serial:__!! the rulebased system performing partial disambiguation with recall close to 100% is applied first and a trigram hmm tagger runs on its results an experiment in czech tagging has been performed with encouraging results\n",
            "INFO:tensorflow:GENERATED SUMMARY: formula for fmeasure : we have used an hmm tagger the usual sourcechannel setting , finetuned to perfection using a 3gram tag language model , a 3gram tag language model the usual sourcechannel setting , finetuned to perfection using a 3gram tag language model the usual sourcechannel setting , finetuned to perfection using a 3gram tag language model the usual sourcechannel setting , finetuned to the usual equation the tagger has been trained the usual sourcechannel setting , finetuned to perfection using a 3gram tag language model the usual sourcechannel setting , finetuned to perfection using a 3gram tag language\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  in this paper , we present a document compression system that uses hierarchical models of discourse and syntax in order to simultaneously manage all these conflicting goals . the source model assigns to a string the probability , the probability that the summary is good english . because the mitre data contained mostly short sentences , the syntax and discourse parsers made fewer errors , which allowed for better compressions to be generated . for estimating the parameters for the discourse models , we used an rst corpus of 385 wall street journal articles from the penn treebank , which we obtained from ldc . their system compressed sentences by dropping syntactic constituents , but could be applied to entire documents only on a sentencebysentence basis . through a sequence of discourse expansions , we can expand upon this summary to reach the original text . this models the extent to which is a good expansion of . that is , the source model should measure how good english a summary is . finally , we can compress the root deriving into __sat=background__ __nuc=span__ by dropping the __sat=background__ constituent . in order to achieve this goal , we extent the noisychannel model proposed by knight & marcu . the mayor is now looking for __reelection__ . the “ document compression ” entry in table 1 shows a grammatical , coherent summary of text , which was generated by a hypothetical document compression system that preserves the most important information in a text while deleting sentences , phrases , and words that are subsidiary to the main message of the text . choosing the manner in which the discourse and syntactic representations of texts are mixed should be influenced by the genre of the texts one is interested to compress . for example , the first sentence in text provides background information for interpreting the information in sentences 2 and 3 , which are in a contrast relation . the noise added in our model consists of words , phrases and discourse units . a __constituentbut__ without the support of the __governer__ , for expand operation could insert a syntactic constituent , such as “ this year ” anywhere in the syntactic tree of . experimentally , we found a reasonable metric was to , for a compression of length , divide each logprobability by . we call this set the mitre corpus . this expansion adds the probability of performing the expansion . we are now able to perform the second operation , __dexpand__ , with which we expand on the core message contained in by adding a satellite which evaluates the information presented in . __goals1__ . when summarizing text , some sentences should be dropped altogether . the rest of figure 2 shows some of the remaining steps to produce the original document , each step labeled with the appropriate probability factors . the deletion of certain words and phrases may also lead to ungrammaticality and information loss . we also consider methods for scaling up the decoder to handling documents of more realistic length . we assume that we are given the discourse structure of each document and the syntactic structures of each of its edus . __<s>__ we present a document compression system that uses a hierarchical noisychannel model of text production __</s>__ __<s>__ our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input __</s>__ __<s>__ the system then uses a statistical hierarchical model of text production in order to drop __nonimportant__ syntactic and discourse constituents so as to generate coherent grammatical document compressions of arbitrary length __</s>__ __<s>__ the system outperforms both a baseline and a sentencebased compression system that operates by simplifying sequentially all sentences in a text __</s>__ __<s>__ our results support the claim that discourse knowledge plays an important role in document summarization __</s>__ __<s>__ we present a document compression system that uses a hierarchical noisychannel model of text production __</s>__ __<s>__ our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input __</s>__ __<s>__ the system then uses a statistical hierarchical model of text production in order to drop __nonimportant__ syntactic and discourse constituents so as to generate coherent grammatical document compressions of arbitrary length __</s>__ __<s>__ the system outperforms both a baseline and a sentencebased compression system that operates by simplifying sequentially all sentences in a text __</s>__ __<s>__ our results support the claim that discourse knowledge plays an important role in document summarization __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we present a document compression system that uses a hierarchical noisychannel model of text production our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input the system then uses a statistical hierarchical model of text production in order to drop !!__nonimportant__!! syntactic and discourse constituents so as to generate coherent grammatical document compressions of arbitrary length the system outperforms both a baseline and a sentencebased compression system that operates by simplifying sequentially all sentences in a text our results support the claim that discourse knowledge plays an important role in document summarization\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a compress the root deriving into of order to simultaneously manage all these a grammatical , coherent summary of text , which was generated by a hypothetical document compression system that preserves the most important information table 1 shows a grammatical , coherent summary of text , which was generated by a hypothetical document compression system that preserves the most important information table 1 shows a grammatical , coherent summary of text , which was generated by a hypothetical document compression system that preserves the most important information\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the model tables , , and are called the __rtable__ , __ntable__ , and ttable , respectively . to see if it was really a corpus problem , we selected a good portion of the corpus and retrained the __rtable__ . the decoding parser is modified to allow inserting __zerofertility__ words , but unlimited insertion easily __blows__ up the memory space . then , we obtain an english parse tree by removing the leaf nodes from the __backreordered__ tree . for example , the best syntactic structure is different if a verb requires a noun phrase as object than it is if it does not . surprisingly , even though the decoded sentence is passable english , the tree structure is totally unnatural . in addition , only limited partofspeech labels are considered to reduce the number of possible __decodedtree__ structures . as we need to parse sentences on the channel input side only , many __xtoenglish__ translation systems can be developed with an english parser alone . the use of an lm needs consideration . the frequency count is essentially a joint probability p , while the tm uses a conditional probability p . using only trigrams obtains the best result for the bleu score . the english word he under the same node is translated into a foreign word kare as seen in the fourth tree . usually a bilingual corpus comes as pairs of translation sentences , so we need to parse the corpus . in our experiment , there are about 4m nonzero entries in the trained table . table 2 shows the result . section 2 briefly reviews the syntaxbased tm , and section 3 describes phrasal translation as an extension . in , the __translationis__ a 1to1 lexical translation from an english __wordto__ a foreign word , i.e. , . to allow non 1to1 translation , such as for idiomatic phrases or compound nouns , we extend the model as follows . we also noticed that too many unary rules were used . in fact , among a total of __138,662__ __reorderexpanded__ rules , the most likely 875 rules contribute 95 % of the probability mass , so discarding the rules which contribute the lower 5 % of the probability mass efficiently eliminates more than 99 % of the total rules . here we need to build an english parse tree from a string of foreign words . the rightmost tree in figure 4 is the decoded tree using the retrained tm . because only a limited number of reorderings are used in actual translation , a small number of rules are highly probable . when it was further relaxed to 100 % , only 20 sentences were decoded . beam search : we give up optimal decoding by using a standard dynamicprogramming parser with beam search , which is similar to the parser used in . figure 1 shows an example . __zerofertility__ words their models are based on mechanisms that generate two languages at the same time , so an english tree is obtained as a __subproduct__ of parsing . , the obtained gain was less than what we expected . this may cause the lm to prefer noun phrases too much . __<s>__ this paper describes a decoding algorithm for a syntaxbased translation model __</s>__ __<s>__ the model has been extended to incorporate phrasal translations as presented here __</s>__ __<s>__ in contrast to a conventional wordtoword statistical model a decoder for the syntaxbased model builds up an english parse tree given a sentence in a foreign language __</s>__ __<s>__ as the model size becomes huge in a practical setting and the decoder considers multiple syntactic structures for each word alignment several pruning techniques are necessary __</s>__ __<s>__ we tested our decoder in a chinesetoenglish translation system and obtained better results than ibm model 4 we also discuss issues concerning the relation between this decoder and a language model __</s>__ __<s>__ this paper describes a decoding algorithm for a syntaxbased translation model __</s>__ __<s>__ the model has been extended to incorporate phrasal translations as presented here __</s>__ __<s>__ in contrast to a conventional wordtoword statistical model a decoder for the syntaxbased model builds up an english parse tree given a sentence in a foreign language __</s>__ __<s>__ as the model size becomes huge in a practical setting and the decoder considers multiple syntactic structures for each word alignment several pruning techniques are necessary __</s>__ __<s>__ we tested our decoder in a chinesetoenglish translation system and obtained better results than ibm model 4 we also discuss issues concerning the relation between this decoder and a language model __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper describes a decoding algorithm for a syntaxbased translation model the model has been extended to incorporate phrasal translations as presented here in contrast to a conventional wordtoword statistical model a decoder for the syntaxbased model builds up an english parse tree given a sentence in a foreign language as the model size becomes huge in a practical setting and the decoder considers multiple syntactic structures for each word alignment several pruning techniques are necessary we tested our decoder in a chinesetoenglish translation system and obtained better results than ibm model 4 we also discuss issues concerning the relation between this decoder and a language model\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is if it does not . surprisingly , even though the decoded , is passable english , the tree structure is totally unnatural . the english word he under the fourth tree . usually a bilingual corpus comes as pairs of translation translation as 4m nonzero entries the trained table . table 2 shows the result . section 2 briefly reviews the syntaxbased tm , there are about 4m nonzero entries the trained table . table 2 shows the result . section 2 briefly reviews the syntaxbased tm , there are about 4m nonzero entries\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the subcategorisation frame descriptions were formally evaluated by comparing the automatically generated verb frames against manual definitions in the german dictionary __duden__ – das __stilwörterbuch__ . the task of evaluating the result of a cluster analysis against the known gold standard of handconstructed verb classes requires us to assess the similarity between two sets of equivalence relations . they are consistent with the german verb classification in as far as the relevant verbs appear in his less extensive semantic ‘ fields ’ . from the methodological point of view , the clustering evaluation gave interesting insights into kmeans ’ behaviour on the syntactic frame data . this is similar to the perspective of , who present , in the context of the muc coreference evaluation scheme , a modeltheoretic measure of the similarity between equivalence classes . the cluster analysis was obtained by running kmeans on a random cluster initialisation , with information radius as distance measure ; the verb description contained condition 2 subcategorisation frame types with pp information . our long term goal is to support the development of highquality and largescale lexical resources . equation , and skew divergence , recently shown as an effective measure for distributional similarity , cf . equation . the semantic aspects and majority of verbs are closely related to levin ’ s english classes . as an extreme example , the semantic class support contains the verb __unterstützen__ , which syntactically requires a direct object , together with the three verbs __dienen__ , __folgen__ , __helfen__ which dominantly subcategorise an indirect object . the resulting lexical subcategorisation for __reden__ and the frame confusion has been caused by parsing mistakes for the infrequent verb ; ni is not among the frames possibly subcategorised by __rudern__ . for measuring the quality of an individual cluster , the cluster purity of each cluster is defined by its largest , the number of members that are projected into the same class . number of correct pairs in number of verbs in we also created a more delicate version of subcategorisation frames that discriminates between different kinds of __pparguments__ . but initialisation using ward ’ s method , which produces tighter clusters and a narrower range of cluster sizes does outperform random cluster initialisation . prepositional phrases are referred to by case and preposition , such as ‘ __dat.mit__ ’ , ‘ __akk.für__ ’ . in both conditions verbs were clustered using kmeans , an iterative , unsupervised , hard clustering method with wellknown properties , cf . . the linguistic investigation gives some insight into the reasons for the success of our clustering technique . hierarchies imposing a strong structure on the clustering are hardly improved by kmeans . as noted by , it is useful to have an evaluation measure that does not depend on the choice of similarity measure or on the original dimensionality of the input data , since that allows meaningful comparison of results for which these parameters vary . their evaluation results are noticeably below those for random clusters . semantic verb classes have been defined for several languages , with dominant examples concerning english and spanish . standard choices include the cosine , euclidean distance , manhattan metric , and variants of the kullbackleibler divergence . __<s>__ the paper describes the application of kmeans a standard clustering technique to the task of inducing semantic classes for german verbs __</s>__ __<s>__ using probability distributions over verb subcategorisation frames we obtained an intuitively plausible clustering of 57 verbs into 14 classes __</s>__ __<s>__ the automatic clustering was evaluated against independently motivated handconstructed semantic verb classes __</s>__ __<s>__ a series of posthoc cluster analyses explored the influence of specific frames and frame groups on the coherence of the verb classes and supported the tight connection between the syntactic behaviour of the verbs and their lexical meaning components __</s>__ __<s>__ the paper describes the application of kmeans a standard clustering technique to the task of inducing semantic classes for german verbs __</s>__ __<s>__ using probability distributions over verb subcategorisation frames we obtained an intuitively plausible clustering of 57 verbs into 14 classes __</s>__ __<s>__ the automatic clustering was evaluated against independently motivated handconstructed semantic verb classes __</s>__ __<s>__ a series of posthoc cluster analyses explored the influence of specific frames and frame groups on the coherence of the verb classes and supported the tight connection between the syntactic behaviour of the verbs and their lexical meaning components __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: the paper describes the application of kmeans a standard clustering technique to the task of inducing semantic classes for german verbs using probability distributions over verb subcategorisation frames we obtained an intuitively plausible clustering of 57 verbs into 14 classes the automatic clustering was evaluated against independently motivated handconstructed semantic verb classes a series of posthoc cluster analyses explored the influence of specific frames and frame groups on the coherence of the verb classes and supported the tight connection between the syntactic behaviour of the verbs and their lexical meaning components\n",
            "INFO:tensorflow:GENERATED SUMMARY: the subcategorisation frame descriptions were formally evaluated by comparing the automatically generated verb frames against manual definitions the subcategorisation frame descriptions were formally evaluated by comparing the automatically generated verb frames against manual definitions the subcategorisation frame descriptions were formally evaluated by comparing the automatically generated verb frames against manual definitions the subcategorisation frame descriptions were formally evaluated by comparing the automatically generated verb frames against manual definitions the subcategorisation frame descriptions were formally evaluated by comparing the automatically generated verb frames against manual definitions the subcategorisation frame descriptions were formally evaluated by comparing the automatically generated verb frames against\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  however , quite a few techniques were integrated to construct a chinese language modeling system , and the contribution of using the acm alone was by no means completely investigated . current products make about 510 % errors in conversion of real data in a wide variety of domains . unfortunately , the acm contains four submodels and this makes it difficult to be pruned to a specific size . we __have2__ furthermore , although we may never have seen an example of “ party on __weekday__ tuesday ” , after we backoff or interpolate with a lower order model , we may able to accurately estimate p . all in which case p = p , or __k=14__ , 16 , which is very similar . and tw are the pruning thresholds . using the trigram approximation , we have p p , assuming that the next word depends only on the two preceding words . it 2 thanks to lillian lee for suggesting this justification of predictive clusters . we then performed a series of experiments to investigate the impact of different types of clusters on the acm . gao et al. , presented a fairly thorough empirical study of clustering techniques for asian language modeling . we first constructed several test sets with different backoff __rates4__ . the lm predicts the next word wi given its history h by estimating the conditional probability p . finally , we can also perform iterations of swapping all words between all clusters until convergence the performance is measured in terms of character error rate . it turns out that predictive cluster models achieve the best perplexity results at about __2^6__ or __2^8__ clusters . as the product of the following two probabilities : the acm consists of two submodels : the cluster submodel p , and the word submodel p . another important finding here is that for most of these settings , the unpruned model is actually larger than a normal trigram model – whenever __k=all__ or 14 , 16 , the unpruned model p × p is actually larger than an unpruned model p . japanese kanakanji conversion is the standard method of inputting japanese text by converting a __syllabarybased__ kana string into the appropriate combination of ideographic kanji and kana . in what follows , we will investigate the impact of each of the factors . if we put the word “ tuesday ” into the cluster __weekday__ , we decompose the probability when each word belongs to one class , simple math shows that this decomposition is a strict equality . we used stolcke ’ s pruning method to produce many __acms__ with different model sizes . based on the pilot experiment results using conditional and predictive cluster models , we tried combined cluster models for values le , j , ke . this allows us to relatively quickly search through what would otherwise be an overwhelmingly large search space . the first similar cluster model was presented by goodman and gao in which the clustering techniques were combined with stolcke ’ s pruning to reduce the language model size effectively . __<s>__ the ngram model is a stochastic model which predicts the next word given the previous words in a word sequence __</s>__ __<s>__ the cluster ngram model is a variant of the ngram model in which similar words are classified in the same cluster __</s>__ __<s>__ it has been demonstrated that using different clusters for predicted and conditional words leads to cluster models that are superior to classical cluster models which use the same clusters for both words __</s>__ __<s>__ this is the basis of the asymmetric cluster model discussed in our study __</s>__ __<s>__ in this paper we first present a formal definition of the acm __</s>__ __<s>__ we then describe in detail the methodology of constructing the acm __</s>__ __<s>__ the effectiveness of the acm is evaluated on a realistic application namely japanese kanakanji conversion __</s>__ __<s>__ experimental results show substantial improvements of the acm in comparison with classical cluster models and word ngram models at the same model size __</s>__ __<s>__ our analysis shows that the highperformance of the acm lies in the asymmetry of the model __</s>__ __<s>__ the ngram model is a stochastic model which predicts the next word given the previous words in a word sequence __</s>__ __<s>__ the cluster ngram model is a variant of the ngram model in which similar words are classified in the same cluster __</s>__ __<s>__ it has been demonstrated that using different clusters for predicted and conditional words leads to cluster models that are superior to classical cluster models which use the same clusters for both words __</s>__ __<s>__ this is the basis of the asymmetric cluster model discussed in our study __</s>__ __<s>__ in this paper we first present a formal definition of the acm __</s>__ __<s>__ we then describe in detail the methodology of constructing the acm __</s>__ __<s>__ the effectiveness of the acm is evaluated on a realistic application namely japanese kanakanji conversion __</s>__ __<s>__ experimental results show substantial improvements of the acm in comparison with classical cluster models and word ngram models at the same model size __</s>__ __<s>__ our analysis shows that the highperformance of the acm lies in the asymmetry of the model __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: the ngram model is a stochastic model which predicts the next word given the previous words in a word sequence the cluster ngram model is a variant of the ngram model in which similar words are classified in the same cluster it has been demonstrated that using different clusters for predicted and conditional words leads to cluster models that are superior to classical cluster models which use the same clusters for both words this is the basis of the asymmetric cluster model discussed in our study in this paper we first present a formal definition of the acm we then describe in detail the methodology of constructing the acm the effectiveness of the acm is evaluated on a realistic application namely japanese kanakanji conversion experimental results show substantial improvements of the acm in comparison with classical cluster models and word ngram models at the same model size our analysis shows that the highperformance of the acm lies in the asymmetry of the model\n",
            "INFO:tensorflow:GENERATED SUMMARY: . it turns out that predictive cluster models achieve the best perplexity results at about that predictive cluster models achieve the best perplexity results at about that predictive cluster models achieve the best perplexity results at about that predictive cluster models achieve the best perplexity results at about that predictive cluster models achieve the best perplexity results at about that predictive cluster models achieve the best perplexity results at about about . as the product of the following two probabilities : the acm consists of two submodels\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  by a similar line of reasoning , we would label the relation between __2.a__ and 2.b as evidence . and we extracted automatically 1,000,000 examples of what we hypothesize to be crossdocument nonrelations , by randomly selecting two sentences from distinct documents . current nlp techniques do not enable us to reliably infer from sentence 1.a that “ can not buy arms legally ” and do not give us access to general purpose knowledge bases that assert that “ similar ” . as expected , the __blipp__ corpus yielded much fewer learning cases : we hypothesize that we can determine that a contrast relation holds between the sentences in even if we can not semantically interpret the two sentences , simply because our background knowledge tells us that good and fails are good indicators of contrastive statements . also , since the learning curve for the __blipp__ corpus is steeper than the learning curve for the raw corpus , this suggests that discourse relation classifiers trained on most representative word pairs and millions of training examples can achieve higher levels of performance than classifiers trained on all word pairs . the discourse relation definitions proposed by others are not easier to apply either because they assume the ability to automatically derive , in addition to the semantics of the text spans , the intentions and __illocutions__ associated with them as well . second , given the complexity of the definitions these theories propose , it is clear why it is difficult to build programs that recognize such relations in unrestricted texts . in order to collect training cases , we mined in an unsupervised manner two corpora . in other words , in our approach , we do not distinguish between contrasts of semantic and pragmatic nature , contrasts specific to violated expectations , etc . in lascarides and asher ’ s theory , we would label the relation between __2.a__ and 2.b as explanation because the event in 2.b explains why the event in __2.a__ happened . table 5 displays in general , a word pair can “ signal ” any relation . table 3 shows the performance of all discourse relation classifiers . in reality though , associating a discourse relation with a text span pair is a choice that is clearly influenced by the theoretical framework one is willing to adopt . table 4 summarizes the results . in the same framework , the relation between clauses __2.a__ and 2.b will be labeled as __causalsemanticpositivenonbasic__ . this means that our classifier contributes to an increase in accuracy from to ! ! ! if we had access to robust semantic interpreters , we could , for example , infer from sentence 1.a that “ can not buy arms legally ” , infer from sentence 1.b that “ can buy arms legally ” , use our background knowledge in order to infer that “ similar ” , and apply hobbs ’ s definitions of discourse relations to arrive at the conclusion that a contrast relation holds between the sentences in . __<s>__ we present an unsupervised approach to recognizing discourse relations of contrast __explanationevidence__ condition and elaboration that hold between arbitrary spans of texts __</s>__ __<s>__ we show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93% even when the relations are not explicitly marked by cue phrases __</s>__ __<s>__ we present an unsupervised approach to recognizing discourse relations of contrast __explanationevidence__ condition and elaboration that hold between arbitrary spans of texts __</s>__ __<s>__ we show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93% even when the relations are not explicitly marked by cue phrases __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we present an unsupervised approach to recognizing discourse relations of contrast !!__explanationevidence__!! condition and elaboration that hold between arbitrary spans of texts we show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93% even when the relations are not explicitly marked by cue phrases\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is clear why it is difficult to build programs an unsupervised manner two corpora . also , a similar line of reasoning , we would label the relation between a similar line of reasoning , we would label the relation between a similar line of reasoning , we would label the relation between addition a similar line of reasoning , we would label the relation between 2.b texts . current nlp techniques do not enable us to reliably infer from sentence 1.a that “ 2.b as evidence . second , given the complexity of the definitions these\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  this section lays out a generalized version of ot ’ s theory of production , introducing some notational and representational conventions that may be useful to others and will be important below . since pron is regular , it follows that produce = for example , if x = __abdip__ and z * ’ s from those with k + 1 finitestate ot is a restriction of the formalism discussed above . §6 considers other harmony orderings , a possibility recognized by prince and smolensky . __y¯__ can be made by looking at the non there is an alternative treatment of the lexicon . production under such a grammar is a matter of successive filtering by the constraints c1 , ... in this case smolensky if x contains fewer a ’ s than b ’ s , then produce = . d then for any regular ot grammar , produce = __ib*__ ] . while the formulas above are almost identical , comprehension is in a sense more complex because it varies both the underlying and surface forms . the notation that we have been using so far for candidates is actually misleading , since in fact the candidates y that are compared encode more than just x and z. they also encode a particular alignment or correspondence between x and z. in general , an ot grammar consists of 4 components : a constraint ranking , a harmony ordering , and generating and pronouncing functions . by removing symbols of σ. ellison exploited such properties to give a production algorithm for finitestate * . e __yi−1__ this unconventional formulation is needed for new approaches that care about the exact location of the ? the surface tones indicate the total number of a ’ s and b ’ s in the underlying form , comprehend is actually a finite set in this version , hence regular . y. gen looks up each abstract morpheme ’ s to justify ⇔ we must show when __y¯__ ∈ __¯yi__ that __y¯∈__ h ) as we will see , they identify the problem as the harmony ordering > , rather than the space of constraints or the potential __infinitude__ of the answer set . indeed , for any phonology , it is trivial to design a harmony measure that both production and comprehension optimize . in addition to having the right kind of power linguistically , regular relations are closed under various relevant operations and allow parallel processing of regular sets of strings . for whom that __outfit__ is optimal , i.e. , is at least as __flattering__ as any other __outfit__ z0 in general > may be a partial order : two competing candidates may be equally harmonic or incomparable , and candidates with different underlying forms never compete at all . ot . __aab__ ? 0c ? 0 , inserting a ? but we are really only interested in harmony measures that are defined by __otstyle__ grammars . the construction is inductive . it is the counterpart to gen : just as gen fleshes out x e their empirical adequacy has been __defended__ by eisner . that much was previously shown by markus __hiller__ and paul smolensky , using similar examples . the trouble is that inputs may be arbitrarily long , and so may __accrue__ arbitrarily large numbers of violations . __<s>__ this paper ties up some loose ends in finitestate optimality theory __</s>__ __<s>__ first it discusses how to perform comprehension under optimality theory grammars consisting of finitestate constraints __</s>__ __<s>__ comprehension has not been much studied in ot we show that unlike production it does not always yield a regular set making finitestate methods inapplicable __</s>__ __<s>__ however after giving a suitably flexible presentation of ot we show carefully how to treat comprehension under recent variants of ot in which grammars can be compiled into finitestate transducers __</s>__ __<s>__ we then unify these variants showing that compilation is possible if all components of the grammar are regular relations including the harmony ordering on scored candidates __</s>__ __<s>__ a side benefit of our construction is a far simpler implementation of directional ot __</s>__ __<s>__ this paper ties up some loose ends in finitestate optimality theory __</s>__ __<s>__ first it discusses how to perform comprehension under optimality theory grammars consisting of finitestate constraints __</s>__ __<s>__ comprehension has not been much studied in ot we show that unlike production it does not always yield a regular set making finitestate methods inapplicable __</s>__ __<s>__ however after giving a suitably flexible presentation of ot we show carefully how to treat comprehension under recent variants of ot in which grammars can be compiled into finitestate transducers __</s>__ __<s>__ we then unify these variants showing that compilation is possible if all components of the grammar are regular relations including the harmony ordering on scored candidates __</s>__ __<s>__ a side benefit of our construction is a far simpler implementation of directional ot __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper ties up some loose ends in finitestate optimality theory first it discusses how to perform comprehension under optimality theory grammars consisting of finitestate constraints comprehension has not been much studied in ot we show that unlike production it does not always yield a regular set making finitestate methods inapplicable however after giving a suitably flexible presentation of ot we show carefully how to treat comprehension under recent variants of ot in which grammars can be compiled into finitestate transducers we then unify these variants showing that compilation is possible if all components of the grammar are regular relations including the harmony ordering on scored candidates a side benefit of our construction is a far simpler implementation of directional ot\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a sense more complex varies both the non y that are compared encode more than just x z. they also encode a particular alignment or correspondence between x x the candidates y that are compared encode more than just x z. they also encode a particular alignment or correspondence between x x conventions that may be useful to others will be important below . since pron is a restriction of the formalism discussed above . §6 considers other harmony orderings , a possibility recognized by prince from those with k + 1\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  an nbest em variant is then employed to jointly reestimate the model parameters such that the ppl on training data is decreased — the likelihood of the training data under our model is increased . in other words , if the two exposed heads bear different information about their parents , they can never be adjoined . the choice among the two schemes is made according to a list of rules based on the identity of the label on the lefthandside of a cf rewrite rule . the same setup was also used in , and . we finally put the decoding results of the two parts together to get the final decoding output . since the slm parses sentences bottomup while the parsers used in , and are topdown , it ’ s not clear how to find a direct correspondence between our schemes of enriching the dependency structure and the ones employed above . an extensive presentation of the slm can be found in . the intermediate nodes created by the above binarization schemes receive the nt label 1. em training can be started . for these reasons , we prefer enriching the syntactic dependencies by information from the left context . one choice is a synchronous multistack search algorithm which is very similar to a beam search . for example , the parent and __opposite+parent__ schemes are worse than baseline in the first iteration when __=0.0__ , the __h2+parent__ and __h2+opposite+parent__ schemes are also worse than h2 scheme in the first iteration when __=0.0__ . as we mentioned in section 3 , the __nt/pos__ vocabularies for the seven models are different because of the enrichment of __nt/pos__ tags . however , the parent and __opposite+parent__ schemes are both worse than the baseline , especially before the em training and with __=0.0__ . the sequence of constructor operations at position grows the wordparse prefix into a wordparse prefix . the results are shown in table 4. since the number of parses for a given word prefix grows exponentially with , , the state space of our model is huge even for relatively short sentences , so we have to use a search strategy that prunes it . the slm , on the other hand , always uses the same dependency structure that is decided beforehand . the results show that an improved parser is indeed helpful in reducing the ppl and wer . because slm training on the __20m__ words of wsj text is very expensive , especially after enriching the __nt/pos__ tags , we only evaluated the wer performance of the seven models with initial statistics from binarized and enriched parse trees . since the wer results in are based on less training data , we do not have a fair comparison between our best model and roark ’ s model . since this restriction of adjoin movement is very tight , pruning may delete some or all the good parsing hypotheses early and the net result may be later development of inadequate parses which lead to poor language modeling and poor parsing performance . __<s>__ we study the impact of richer syntactic dependencies on the performance of the structured language model along three dimensions: parsing accuracy perplexity and worderrorrate __</s>__ __<s>__ we show that our models achieve an improvement in __lp/lr__ ppl and/or wer over the reported baseline results using the slm on the upenn treebank and wall street journal corpora respectively __</s>__ __<s>__ analysis of parsing performance shows correlation between the quality of the parser and the language model performance __</s>__ __<s>__ a remarkable fact is that the enriched slm outperforms the baseline 3gram model in terms of wer by 10% when used in isolation as a second pass language model __</s>__ __<s>__ we study the impact of richer syntactic dependencies on the performance of the structured language model along three dimensions: parsing accuracy perplexity and worderrorrate __</s>__ __<s>__ we show that our models achieve an improvement in __lp/lr__ ppl and/or wer over the reported baseline results using the slm on the upenn treebank and wall street journal corpora respectively __</s>__ __<s>__ analysis of parsing performance shows correlation between the quality of the parser and the language model performance __</s>__ __<s>__ a remarkable fact is that the enriched slm outperforms the baseline 3gram model in terms of wer by 10% when used in isolation as a second pass language model __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we study the impact of richer syntactic dependencies on the performance of the structured language model along three dimensions: parsing accuracy perplexity and worderrorrate we show that our models achieve an improvement in !!__lp/lr__!! ppl and/or wer over the reported baseline results using the slm on the upenn treebank and wall street journal corpora respectively analysis of parsing performance shows correlation between the quality of the parser and the language model performance a remarkable fact is that the enriched slm outperforms the baseline 3gram model in terms of wer by 10% when used in isolation as a second pass language model\n",
            "INFO:tensorflow:GENERATED SUMMARY: schemes are also worse than h2 to on can is increased . the intermediate nodes created by the two exposed heads bear different information about their parents , they the two exposed heads bear different information about their parents , they the two exposed heads bear different information about their parents , they the two exposed heads bear different information about their parents , they the two exposed heads bear different information about their parents , they the two exposed heads bear different information about their parents , they the two exposed heads bear different information about their parents ,\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  this section describes the patternmatching algorithm in detail . pattern matching and substitution can be defined more rigorously using tree automata , but for reasons of space these definitions are not given here . the algorithm has two phases . this blocks the matching of shallower patterns , reducing their match values and hence raising their success probability . . it goes to some lengths to handle complex cases such as adjunction and where two or more empty nodes ’ paths cross . let g be the set of such empty node representations derived from the “ gold standard ” evaluation corpus and t this paper presents an algorithm that takes as its input a tree without empty nodes of the kind shown in figure 2 and modifies it by inserting empty nodes and coindexation to produce a the tree shown in figure 1. any indices occuring on nodes in the pattern are systematically __renumbered__ beginning with 1. it can also be regarded as a kind of tree transformation , so the overall system architecture is an instance of the “ __transformdetransform__ ” approach advocated by johnson . the relabelling of auxiliary verbs was performed primarily because charniak ’ s parser produces trees with such labels ; experiments show that auxiliary relabelling has little effect on the algorithm ’ s performance . the entry with pos sbar and no label refers to a “ compound ” type of empty structure labelled sbar consisting of an empty complementizer and an empty s ; a typical example is shown in figure 3. on the other hand , since shallower patterns contain less structure they are likely to match a greater variety of trees than the deeper patterns , they still have ample opportunity to apply . in fact this headbased antecedent representation yields scores very __similiar__ to those obtained using the phrasebased representation . this information is shown under the “ match ” column in table 2 , and is used to filter patterns which would most often be incorrect to apply even though they match . we used sections 2–21 of the penn treebank as the training corpus ; section 24 was used as the development corpus for experimentation and tuning , while the test corpus was used exactly once . table 1 contains summary statistics on the distribution of empty nodes in the penn treebank . the previous section described an algorithm for restoring empty nodes and coindexing their antecedents . rather , they produce trees of the kind shown in figure 2. in fact , the actual implementation of pattern matching and substitution used here is considerably more complex than just described . * we experimented with specialized classifiers for determining if an np the “ label ” column gives the terminal label of the empty node , the “ pos ” column gives its preterminal label and the “ antecedent ” column there are many different possible ways of realizing this intuition , but all of the ones tried gave approximately similar results so we present the simplest one here . if a pattern p matches a tree t , then it is possible to substitute p for the fragment of t that it matches . __<s>__ this paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their coindexed antecedents in phrase structure trees that do not contain this information __</s>__ __<s>__ the patterns are minimal connected tree fragments containing an empty node and all other nodes coindexed with it __</s>__ __<s>__ this paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus __</s>__ __<s>__ evaluating the algorithm on the output of charniak’s parser and the penn treebank shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity __</s>__ __<s>__ this paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their coindexed antecedents in phrase structure trees that do not contain this information __</s>__ __<s>__ the patterns are minimal connected tree fragments containing an empty node and all other nodes coindexed with it __</s>__ __<s>__ this paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus __</s>__ __<s>__ evaluating the algorithm on the output of charniak’s parser and the penn treebank shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their coindexed antecedents in phrase structure trees that do not contain this information the patterns are minimal connected tree fragments containing an empty node and all other nodes coindexed with it this paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus evaluating the algorithm on the output of charniak’s parser and the penn treebank shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity\n",
            "INFO:tensorflow:GENERATED SUMMARY: figure 1. any indices it by inserting empty nodes coindexation to produce a the tree shown figure 1. any indices occuring on nodes the patternmatching algorithm detail it by inserting empty nodes coindexation to produce a the tree shown figure 1. any indices occuring on nodes the patternmatching algorithm detail it by inserting empty nodes coindexation to produce a the tree shown figure 1. any indices occuring on nodes the patternmatching algorithm detail it by inserting empty nodes coindexation to produce a the tree shown figure 1. any indices occuring on nodes the patternmatching algorithm figure 1. any indices occuring\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  x : f = � } , or for that set ’ s characteristic function . for example , consider bruce kaplan , president of __metals__ inc. add a special final round to boost recall , yielding __91.2/80.0/85.2__ for the yarowsky algorithm and __91.3/80.1/85.3__ for their version of the original cotraining algorithm . + , and the area of its upper left quadrant represents pr . = we first derive an expression for the precision of f in terms of g. note that the second line is derived from the first by rule independence . in what follows , f represents an atomic rule under consideration , and g represents the current classifier . i give here an algorithm , the greedy agreement algorithm , that constructs paired rules that agree on unlabeled data , and i examine its performance . in the special case in which rule independence is satisfied , both horizontal and vertical lines are __unbroken__ , as in figure 1. f = + } . = the first assumption is precision independence . precision independence and view independence are distinct assumptions ; neither implies the __other.2__ to sum up , we have refined previous work on the analysis of cotraining , and given a new cotraining algorithm that is theoretically justified and has good empirical performance . by contrast , in the expression p , f is the set of instances for which f i extend this work in two ways . the areas of minority values are marked in figure 1b . x2 such that x1 = x1 and x2 the task is to classify names in text as person , location , or organization . we define the conditional dependence of f and g given y blum and mitchell an atomic rule __f`__ is chosen only if its precision , pr , exceeds at each iteration , each possible extension to one of the rules is considered and scored . one possibility is that minority values are opposite to error values . = + . the empirical investigations described here and below use the data set of . + if f __abstains__ on input x . “ classifier ” is synonymous with “ rule ” . in recent work , prove that a classifier has low generalization error if it agrees on unlabeled data with a second classifier based on a different “ view ” of the data . = __a/nt__ . there is an unlabeled training set containing __89,305__ instances , and a labeled test set containing 289 persons , 186 locations , 402 organizations , and 123 “ other ” , for a total of 1,000 instances . typically , in such a diagram , either the horizontal or vertical line is broken , as in figure 2. a binary rule f can be thought of as the characteristic function of the set of instances { x : the precision of the new classifier is __119/120__ = note that r = 0 __<s>__ this paper refines the analysis of cotraining defines and evaluates a new cotraining algorithm that has theoretical justification gives a theoretical justification for the yarowsky algorithm and shows that cotraining and the yarowsky algorithm are based on different independence assumptions __</s>__ __<s>__ this paper refines the analysis of cotraining defines and evaluates a new cotraining algorithm that has theoretical justification gives a theoretical justification for the yarowsky algorithm and shows that cotraining and the yarowsky algorithm are based on different independence assumptions __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper refines the analysis of cotraining defines and evaluates a new cotraining algorithm that has theoretical justification gives a theoretical justification for the yarowsky algorithm and shows that cotraining and the yarowsky algorithm are based on different independence assumptions\n",
            "INFO:tensorflow:GENERATED SUMMARY: terms of g. note that the second line is derived from the first by rule independence . i give here an algorithm , the greedy agreement algorithm , that constructs paired rules inc. add a special final round to boost recall , yielding for their version of the original cotraining algorithm that the second line is derived from the first by rule independence . i give here an algorithm , the greedy agreement algorithm , that constructs paired rules inc. add a special final round to boost\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  a discussion of results is in section 5. a final capability of xle that increases coverage of the __standardplusfragment__ grammar is a skimming technique . the grammar uses several lexicons and two guessers : one guesser for words recognized by the morphological analyzer but not in the lexicons and one for those not recognized . furthermore , only sentences 2an alternative numerical method would be a combination of iterative scaling techniques with a conditional em algorithm . both the dr and lfg annotations broadly agree in their measure of error reduction . let { __imj=1__ be a set of training data , consisting of pairs of sentences y and partial annotations z , let x be the set of parses for sentence y consistent with annotation z , and let x be the set of all parses produced by the grammar for sentence y. furthermore , let p denote the expectation of function f under distribution when the amount of time or memory spent on a sentence exceeds a __threshhold__ the grammar has 314 rules with regular expression righthand sides which compile into a collection of finitestate machines with a total of __8,759__ states and __19,695__ arcs . section 2 describes the lexicalfunctional grammar , the constraintbased parser , and the robustness techniques employed in this work . according to their dependency relations __scheme.3__ annotating the wsj test set was bootstrapped by parsing the test sentences using the lfg grammar and also checking for consistency with the penn treebank annotation . x ir for i = 1 , ... , n on the set of parses x , and we define discriminative or conditional criteria with respect to the set of grammar parses consistent with the treebank annotations . in this version of the corpus , all wsj labels with sbj are retained and are restricted to phrases corresponding to subj in the lfg grammar ; in addition , it contains np under vp , all lgs tags , all prd tags , vp under vp , sbar , and verb pos tags under vp . this grammar parses the sentence as wellformed chunks specified by the grammar , in particular as ss , nps , pps , and vps . to our knowledge , so far the only direct point of comparison is the parser of carroll et al . furthermore , properties refering to lexical elements based on an auxiliary distribution approach as presented in riezler et al . are included in the model . the effect of the quality of the parses on disambiguation performance can be illustrated by breaking down the fscores according to whether the parser yields full parses , fragment , skimmed , or __skimmed+fragment__ parses for the test sentences . the gradient takes the form : the derivatives in the gradient vector intuitively are again just a difference of two expectations note also that this expression shares many common terms with the likelihood function , suggesting an efficient implementation of the optimization routine . ’ s system , we computed an fscore that does not distinguish different types of dependency relations . the third column reports fscores for examples which receive only __nonfull__ parses , i.e . fragment or skimmed parses or __skimmed+fragment__ parses . __<s>__ we present a stochastic parsing system consisting of a lexicalfunctional grammar a constraintbased parser and a stochastic disambiguation model __</s>__ __<s>__ we report on the results of applying this system to parsing the upenn wall street journal treebank __</s>__ __<s>__ the model combines full and partial parsing techniques to reach full grammar coverage on unseen data __</s>__ __<s>__ the treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models __</s>__ __<s>__ disambiguation performance is evaluated by measuring matches of predicateargument relations on two distinct test sets __</s>__ __<s>__ on a gold standard of manually annotated fstructures for a subset of the wsj treebank this evaluation reaches 79% fscore __</s>__ __<s>__ an evaluation on a gold standard of dependency relations for brown corpus data achieves 76% fscore __</s>__ __<s>__ we present a stochastic parsing system consisting of a lexicalfunctional grammar a constraintbased parser and a stochastic disambiguation model __</s>__ __<s>__ we report on the results of applying this system to parsing the upenn wall street journal treebank __</s>__ __<s>__ the model combines full and partial parsing techniques to reach full grammar coverage on unseen data __</s>__ __<s>__ the treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models __</s>__ __<s>__ disambiguation performance is evaluated by measuring matches of predicateargument relations on two distinct test sets __</s>__ __<s>__ on a gold standard of manually annotated fstructures for a subset of the wsj treebank this evaluation reaches 79% fscore __</s>__ __<s>__ an evaluation on a gold standard of dependency relations for brown corpus data achieves 76% fscore __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we present a stochastic parsing system consisting of a lexicalfunctional grammar a constraintbased parser and a stochastic disambiguation model we report on the results of applying this system to parsing the upenn wall street journal treebank the model combines full and partial parsing techniques to reach full grammar coverage on unseen data the treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models disambiguation performance is evaluated by measuring matches of predicateargument relations on two distinct test sets on a gold standard of manually annotated fstructures for a subset of the wsj treebank this evaluation reaches 79% fscore an evaluation on a gold standard of dependency relations for brown corpus data achieves 76% fscore\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is a skimming technique . the grammar uses several lexicons two guessers : one guesser for words recognized by the morphological analyzer but not is a skimming technique . the grammar uses several lexicons two guessers : one guesser for words recognized by the morphological analyzer but not is a skimming technique . the grammar uses several lexicons two guessers : one guesser for words recognized by the morphological analyzer but not is a skimming technique . the grammar uses several lexicons two guessers : one guesser for words recognized by the morphological analyzer but not\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "INFO:tensorflow:We've been decoding with same checkpoint for 60 seconds. Time to load new checkpoint\n",
            "INFO:tensorflow:Loading checkpoint /content/drive/My Drive/MA_colab/PG_Model2/logs/myexperiment/train/model.ckpt-15726\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/MA_colab/PG_Model2/logs/myexperiment/train/model.ckpt-15726\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  a model parameter am , m = 1 , ... , m. the feature functions have then not only a dependence on fj1 and ei1 but also on __zk1__ , ak1 . = a2 = 1. table 1 shows the corpus statistics of this task . this is a very convenient approach to improve the quality of a baseline system . for each of them , if the intended information is conveyed and there are no syntactic errors , the sentence is counted as correct . instead of modeling one probability distribution , we obtain two different knowledge sources that are trained independently . = aj . interestingly , this framework contains as special case the source channel approach if we use and set a1 we even can use both features log pr and log pr , obtaining a more symmetric translation model . as alternative to the sourcechannel approach , we directly model the posterior probability pr . yet , the criterion as it is described in eq . 11 , we use the gis algorithm . we do not observe significant overfitting . : here , we replaced __pˆ�__ by __pˆ�__ . yet , if both decision rules yield the same translation quality , we can use that decision rule which is better suited for efficient search . each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0 . here , pr is the language model of the target language , whereas pr is the translation model . this measure compares the words in the two sentences ignoring the word order . so far , we use the logarithm of the components of a translation model as feature functions . : this approach has been suggested by for a natural language understanding task . the alignment of the alignment templates : the alignment mapping is j → i = aj from source position j to target position here , we omit a detailed description of modeling , training and search , as this is not relevant for the subsequent exposition . obviously , we can perform the same step for translation models with an even richer structure of hidden variables than only the alignment aj1 . the overall architecture of the sourcechannel approach is summarized in figure 1. we are given a source sentence fj1 = f1 , ... , fj , ... , more detailed analysis , subjective judgments by test persons are necessary . to simplify the notation , we shall omit in the following the dependence on the hidden variables of the model . for further details , see . in general , as shown in this figure , there may be additional transformations to make the translation task simpler for the algorithm . • we can use numerous additional features that deal with specific problems of the baseline statistical mt system . ei1 = e1 , ... , ei , ... , ei . to use these three component models in a direct maximum entropy approach , we define three different feature functions for each component of the translation model instead of one feature function for the whole translation model p . 1 perform the following maximization __<s>__ we present a framework for statistical machine translation of natural languages based on direct maximum entropy models which contains the widely used sourcechannel approach as a special case __</s>__ __<s>__ all knowledge sources are treated as feature functions which depend on the source language sentence the target language sentence and possible hidden variables __</s>__ __<s>__ this approach allows a baseline machine translation system to be extended easily by adding new feature functions __</s>__ __<s>__ we show that a baseline statistical machine translation system is significantly improved using this approach __</s>__ __<s>__ we present a framework for statistical machine translation of natural languages based on direct maximum entropy models which contains the widely used sourcechannel approach as a special case __</s>__ __<s>__ all knowledge sources are treated as feature functions which depend on the source language sentence the target language sentence and possible hidden variables __</s>__ __<s>__ this approach allows a baseline machine translation system to be extended easily by adding new feature functions __</s>__ __<s>__ we show that a baseline statistical machine translation system is significantly improved using this approach __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we present a framework for statistical machine translation of natural languages based on direct maximum entropy models which contains the widely used sourcechannel approach as a special case all knowledge sources are treated as feature functions which depend on the source language sentence the target language sentence and possible hidden variables this approach allows a baseline machine translation system to be extended easily by adding new feature functions we show that a baseline statistical machine translation system is significantly improved using this approach\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a the model of eq pr , whereas pr is the translation model . this measure compares the words the model parameter am , m = 1 , ... , m. the feature functions have then not only a dependence on fj1 ei1 but also on model parameter am , m = 1 , ... , m. the feature functions have then not only a dependence on fj1\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the ranking algorithm rewards most specific concepts first ; for example , a sentence containing “ milan __kucan__ ” has a higher score than a sentence contains only either milan or __kucan__ . each sentence is marked with its publication date and a reference date is inserted after every date expression . mckeown but no system or human scored perfect in grammaticality . we apply a simple sentence filter that only retains the lead 10 sentences . unit __s1.1__ says “ thousands of people are feared dead ” and unit __m2.2__ says “ 3,000 and perhaps ... duc is a new evaluation series supported by nist under tides , to further progress in summarization and enable researchers to participate in largescale experiments . the second baseline , coverage baseline , took the first sentence in the first document , the first sentence in the second document and so on until it had a summary of 50 , 100 , 200 , or 400 words . borrowed from information retrieval research , precision is used to measure how effectively a system generates good summary sentences . it was also used as a baseline in a preliminary multidocument summarization study by marcu and gerber with relatively good results . clusters are formed through strict lexical connection . when it came to the measure for cohesion the results are confusing . we conclude with future directions . neats so far only considers features pertaining to individual sentences . in duc2001 we simply used the first sentence of its document . nist did not define any official performance metric in duc2001 . we present the performance of neats in duc2001 in content and quality measures . neats ’ s performance for averaged pseudo precision equals human ’ s at about 58 % . this example highlights the difficulty of judging the content coverage of system summaries against model summaries and the inadequacy of using recall as defined . are these two equivalent ? figure 1 shows the top 5 concepts with their relevancy scores for the topic “ __slovenia__ __secession__ from __yugoslavia__ ” in the duc2001 test collection . section 3 gives a brief overview of the evaluation procedure used in duc2001 . as we mentioned earlier , nist assessors not only marked the sharing relations among system units and model units , they also indicated the degree of match , i.e. , all , most , some , hardly any , or none . with the individual key concepts available , we proceed to cluster these concepts in order to identify major subtopics within the main topic . we would like to apply some compression techniques or use linguistic units smaller than sentences to improve our retention score . to remedy this problem , we introduce a __buddy__ system to improve cohesion and coherence . therefore , time disambiguation and normalization are very important in multidocument summarization . 6 nist assessors wrote two separate summaries per topic . before we present our results , we describe the corpus and evaluation procedures of the document understanding conference 2001 . this ranking algorithm performs relatively well , but it also results in many ties . a total of 12 systems participated in that task . “ the quake was centered in a remote __mountainous__ area ” . __<s>__ neats is a multidocument summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order __</s>__ __<s>__ neats is among the best performers in the large scale summarization evaluation duc 2001 __</s>__ __<s>__ neats is a multidocument summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order __</s>__ __<s>__ neats is among the best performers in the large scale summarization evaluation duc 2001 __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: neats is a multidocument summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order neats is among the best performers in the large scale summarization evaluation duc 2001\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is marked the such of scored perfect a reference date expression date expression . mckeown but no system or human scored perfect largescale experiments . the second baseline , coverage baseline , took the first document , the first sentence ” has a higher score than a sentence contains only either milan or a sentence contains only either milan or a sentence contains only either milan or a sentence contains only either milan or a sentence contains only either milan or a sentence contains only either milan\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the wordbased shallow parser displays an apparently loglinear increase in performance , and surpasses the flatter posbased curve at about 50,000 sentences of training data . to provide __differentlysized__ training sets for learning curve experiments , each training set was also clipped at the following sizes : 100 sentences , 500 , 1000 , 2000 , 5000 , 10,000 , 20,000 and __50,000.__ we review related research in section 5 , and formulate our conclusions in section 6. a more general shortcoming is that the word form of an unknown word often contains useful information that is not available in the present setup . the addition of pos also improves performance . the last paragraph describes results with input consisting of words and pos tags . for the total data set , this yields __1,637,268__ instances , one for each word or punctuation mark . section 4 contains a comparison of the effects with goldstandard and automatically assigned pos . finally , words and pos are combined . as shown in figure 1 , the “ attenuated word + goldstandard pos ” curve starts close to the goldstandard pos curve , attains breakeven with this curve at about 500 sentences , and ends close to but higher than all other curves , including the “ attenuated word ” curve . his chunker works on the basis of pos information alone , whereas the second module , the attacher , also uses lexical information . in our experiments , we used a variant of the ib1 memorybased learner and classifier as implemented in timbl . : nodes in the tree are labeled with a syntactic category and up to four function tags that specify grammatical relations , subtypes of adverbials , discrepancies between syntactic form and syntactic function and notions like topicalization . for creating the posbased task , all words are replaced by the goldstandard pos tags associated with them in the penn treebank . with realistic pos the improvement is much smaller . ’ s vote by the inverse of its distance to the test example . this is clearly __disadvantageous__ and specific to this choice of __alnation__ of words and pos . the first two articles mention that words and pos together perform better than pos alone . in one experiment , it has to be performed on the basis of the “ goldstandard ” , __assumedperfect__ pos taken directly from the training data , the penn treebank , so as to abstract from a particular pos tagger and to provide an upper bound . the types and definitions of chunks are identical to the ones used here . if __tribl__ encounters an unknown word in the test material , it stops already at the decision tree stage and returns the default class without even using the information provided by the context . abney ’ s chunking parser consists of two modules : a chunker and an attacher . in a third , a special encoding of lowfrequency words is used . the yaxis represents f on combined chunking and function assignment . for the chunk part of the code , we adopt the “ inside ” , “ outside ” , and “ between ” encoding originating from . all data was converted to instances as illustrated in table 2.1. comparative experiments with a real pos tagger produce lower results . __<s>__ we describe a case study in which a memorybased learning algorithm is trained to simultaneously chunk sentences and assign grammatical function tags to these chunks __</s>__ __<s>__ we compare the algorithm’s performance on this parsing task with varying training set sizes and different input representations __</s>__ __<s>__ in particular we compare input consisting of words only a variant that includes word form information for lowfrequency words goldstandard pos only and combinations of these __</s>__ __<s>__ the wordbased shallow parser displays an apparently loglinear increase in performance and surpasses the flatter posbased curve at about 50 000 sentences of training data __</s>__ __<s>__ the lowfrequency variant performs even better and the combinations is best __</s>__ __<s>__ comparative experiments with a real pos tagger produce lower results __</s>__ __<s>__ we argue that we might not need an explicit intermediate postagging step for parsing when a sufficient amount of training material is available and word form information is used for lowfrequency words __</s>__ __<s>__ we describe a case study in which a memorybased learning algorithm is trained to simultaneously chunk sentences and assign grammatical function tags to these chunks __</s>__ __<s>__ we compare the algorithm’s performance on this parsing task with varying training set sizes and different input representations __</s>__ __<s>__ in particular we compare input consisting of words only a variant that includes word form information for lowfrequency words goldstandard pos only and combinations of these __</s>__ __<s>__ the wordbased shallow parser displays an apparently loglinear increase in performance and surpasses the flatter posbased curve at about 50 000 sentences of training data __</s>__ __<s>__ the lowfrequency variant performs even better and the combinations is best __</s>__ __<s>__ comparative experiments with a real pos tagger produce lower results __</s>__ __<s>__ we argue that we might not need an explicit intermediate postagging step for parsing when a sufficient amount of training material is available and word form information is used for lowfrequency words __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we describe a case study in which a memorybased learning algorithm is trained to simultaneously chunk sentences and assign grammatical function tags to these chunks we compare the algorithm’s performance on this parsing task with varying training set sizes and different input representations in particular we compare input consisting of words only a variant that includes word form information for lowfrequency words goldstandard pos only and combinations of these the wordbased shallow parser displays an apparently loglinear increase in performance and surpasses the flatter posbased curve at about 50 000 sentences of training data the lowfrequency variant performs even better and the combinations is best comparative experiments with a real pos tagger produce lower results we argue that we might not need an explicit intermediate postagging step for parsing when a sufficient amount of training material is available and word form information is used for lowfrequency words\n",
            "INFO:tensorflow:GENERATED SUMMARY: the wordbased shallow parser displays an apparently loglinear increase , surpasses the wordbased shallow parser displays an apparently loglinear increase , surpasses the flatter posbased curve at about 50,000 of training data . : nodes shallow parser displays an apparently loglinear increase , surpasses the flatter posbased curve at about 50,000 of training data . : nodes shallow parser an apparently loglinear increase , performance , 2000 , 5000 , 10,000 , 20,000\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  for this case , the domaindependent features may have been particularly important , making it difficult to compare the results of this approach to others working on less restricted domains . i or until all the unlabeled data has been labeled . it seems that cotraining is useful in rather specialized constellations only . in addition , this kind of test set approximates the decisions made by a simple resolution algorithm that cause in a realworld setting , information about a pronoun ’ s semantic class obviously is not available prior to its resolution . cardie and wagstaff describe an unsupervised clustering approach to noun phrase coreference resolution in which features are assigned to single noun phrases only . we regard these cases as irrelevant because they do not contribute any knowledge for the classifier . a number of requirements for these views are mentioned in the literature , e.g. , that they have to be disjoint or even conditionally independent ) . in our annotation , coreference is represented in terms of a member attribute on markables . from this set , no instances were removed because no knowledge whatsoever about the data can be assumed in a realistic setting . they only used pairs of anaphors and their closest antecedents as positive examples in training , but evaluated according to vilain et al . . it was criticized that the features used by mccarthy and lehnert are highly idiosyncratic and applicable only to one particular domain . driven by the necessity to provide robust systems for the muc system evaluations , researchers began to look for those features which were particular important for the task of reference resolution . in strube et al . in the literature on reference resolution it is claimed that the antecedent ’ s grammatical function and its realization are important . the med is computed from these editing operations and the length of the potential antecedent m or the length of the anaphor n. cotraining is a metalearning algorithm which exploits unlabeled in addition to labeled training data for classifier learning . we favour this definition because it strengthens the predictive power of the word distance between potential anaphor and potential antecedent . in figure 1 , three curves and three baselines are plotted : for 20 , 20 __0its__ is the baseline , i.e . the initial result obtained by just combining the two initial classifiers . this produced 250 data sets with a total of __92750__ instances of potential antecedentanaphor pairs . then we ran the cotraining experiment with the np form pairs of anaphors and __nonantecedents__ were labeled dn if at least one true antecedent occurred in between . they mention that it was important for the training data to contain transitive positives , i.e. , all possible coreference relations within an anaphoric chain . then we briefly introduce the cotraining paradigm , which is followed by a description of the corpus we use , the corpus annotation , and the way we prepared the data for using a binary classifier in the cotraining algorithm . they distinguish between features which focus on individual noun phrases and features which focus on the anaphoric relation . __<s>__ in this paper we investigate the practical applicability of cotraining for the task of building a classifier for reference resolution __</s>__ __<s>__ we are concerned with the question if cotraining can significantly reduce the amount of manual labeling work and still produce a classifier with an acceptable performance __</s>__ __<s>__ in this paper we investigate the practical applicability of cotraining for the task of building a classifier for reference resolution __</s>__ __<s>__ we are concerned with the question if cotraining can significantly reduce the amount of manual labeling work and still produce a classifier with an acceptable performance __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in this paper we investigate the practical applicability of cotraining for the task of building a classifier for reference resolution we are concerned with the question if cotraining can significantly reduce the amount of manual labeling work and still produce a classifier with an acceptable performance\n",
            "INFO:tensorflow:GENERATED SUMMARY: has been labeled . it seems that cotraining rather specialized constellations only . we regard these only . it or until all the unlabeled data has been labeled . it seems that cotraining rather specialized constellations only . it or until all the unlabeled data has been labeled . it seems that cotraining rather specialized constellations only . it or until all the unlabeled data has been labeled . it seems that cotraining rather specialized constellations only . it or until all the unlabeled data has been labeled . it seems that cotraining\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  as i increases , so does the set ci . let s = we will combine random variables x1 ... thus , if lexical effects are present , we expect the model that uses caching to provide lower entropy estimates . wn } be the test sentence . we claim , however , that the entropy of these random variables is on average the __same2__ . the probability model for sentence s with parse tree t is : where parents are words which are parents of node x in the the tree t. our claim is that the entropy of yi , h stays constant for all i. by the definition of relative mutual information between xi and ci , where the last term is the mutual information between the word and context given the sentence . it has been shown by kuhn and mohri that lexical effects can be easily captured by caching . the random variable we are interested in is yi , a random variable that has the same distribution as __xi|x1__ = w1 , ... , we are only interested in the mean value of the h for wj e si , where si is the ith sentence . wi−1 for some fixed words these causes may be split into two categories : lexical and nonlexical . this suggests that such measurements may be able to pick up more obviously semantic contextual influences than simply the repeating words captured by caching models . w1 ... { w1 ... the first , which we call ci , contains x1 through __xj−1__ , i.e . all the words from the preceding sentences . the same corpus , training and testing sets were used . we have shown that entropy of the sentences taken without context increases with the sentence number , which is in agreement with the above principle . finally we compute the entropy using the estimator described in . the results are reported on figure 1 . the trend is fairly obvious , especially for small sentence numbers : sentences get harder as sentence number increases , i.e . the probability of the sentence given the model decreases . we have as many random variables as we have words in a text . there has been work in the speech community inspired by this constancy rate principle . it is wellknown from information theory that the most efficient way to send information through noisy channels is at a constant rate . the communication medium we examine in this paper is text , and we present some evidence that this principle holds here . entropy is the highest when all values are equally probable , and is lowest when one of the choices has probability of 1 , i.e . deterministically known in advance . these results are interesting in their own right , and may have practical implications as well . let t be our training corpus . since we have proven the presence of the nonlexical effects in the previous experiment , we can see that both lexical and nonlexical effects are present . __<s>__ we present a constancy rate principle governing language generation __</s>__ __<s>__ we show that this principle implies that local measures of entropy should increase with the sentence number __</s>__ __<s>__ we demonstrate that this is indeed the case by measuring entropy in three different ways __</s>__ __<s>__ we also show that this effect has both lexical and nonlexical causes __</s>__ __<s>__ we present a constancy rate principle governing language generation __</s>__ __<s>__ we show that this principle implies that local measures of entropy should increase with the sentence number __</s>__ __<s>__ we demonstrate that this is indeed the case by measuring entropy in three different ways __</s>__ __<s>__ we also show that this effect has both lexical and nonlexical causes __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we present a constancy rate principle governing language generation we show that this principle implies that local measures of entropy should increase with the sentence number we demonstrate that this is indeed the case by measuring entropy in three different ways we also show that this effect has both lexical and nonlexical causes\n",
            "INFO:tensorflow:GENERATED SUMMARY: up more obviously semantic contextual influences than simply the repeating words captured by caching models . w1 ... { w1 ... { w1 ... the first , which we call ci , contains x1 through than simply the repeating words captured by caching models . w1 ... { w1 ... { w1 ... the first , which we call ci , contains x1 through , contains x1 through , contains x1 through , contains x1 through , contains x1 through , contains x1 through , contains x1 through , contains x1 sets were used is : where parents are words\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the next section describes unification grammars and maxwell and kaplan packed representation . the algorithm rests on the observation that so long as features are local to the parse fragments used in the packed representations , the statistics required for parsing and estimation are the kinds of quantities that dynamic programming algorithms for graphical models can perform . that does not involve this enumeration . that is , we require that : finally , a packed representation r represents the set of parses q that are identified by values that satisfy the __nogoods__ , i.e. , q let i < j iff there is a path from mi to mj in this tree . it seems that the maxwell and kaplan packed representation may permit more compact representations than the disjunctive representations used by miyao et al. , but this does not imply that the algorithms proposed here are more efficient . in parsing applications it is important to be able to extract the most probable parse __ˆ__ of string y with respect to a __subg__ . this characterisation omits many details about unification grammars and the algorithm by which the packed representations are actually constructed ; see maxwell iii and kaplan for details . maxwell iii and kaplan describes a parsing algorithm for unificationbased grammars that takes as input a string y and returns a packed representation r then we can calculate the partial derivatives required by the conjugate gradient algorithm as well . miyao and tsujii is the closest related work we know of . this section characterises the properties of unification grammars and the maxwell and kaplan packed parse representations that will be important for what follows . however , the dynamic programming algorithms presented here require the information encoded in properties to be local with respect to the features f used in the packed parse representation . note that except for trivial grammars f and q are infinite . for most linguistically realistic grammars this set is finite , and for moderate sized grammars and training corpora by the same arguments as above , d only contains variables ordered after xi , so vn if f is a function whose domain is x , we say that f depends on the set of variables d = { __xj|ix__ , x ' e x , x while for uniformity we write conditions as functions on the entire vector x , in practice maxwell and kaplan if yi is the yield of the parse __i__ , the conditional likelihood of the parses given their yields is : then the maximum conditional likelihood estimate thus , while properties are required to be local relative to features , we can use the ability of the underlying unification grammar to encode essentially arbitrary nonlocal information in features to introduce properties that also encode nonlocal information . is the subset of h that do not depend on any variables at all .3 because such parsers prune the search space they can not guarantee correct results , unlike the algorithms proposed here . __<s>__ stochastic unificationbased grammars define exponential distributions over the parses generated by a unificationbased grammar __</s>__ __<s>__ existing algorithms for parsing and estimation require the enumeration of all of the parses of a string in order to determine the most likely one or in order to calculate the statistics needed to estimate a grammar from a training corpus __</s>__ __<s>__ this paper describes a graphbased dynamic programming algorithm for calculating these statistics from the packed ubg parse representations of maxwell and kaplan which does not require enumerating all parses __</s>__ __<s>__ like many graphical algorithms the dynamic programming algorithm’s complexity is worstcase exponential but is often polynomial __</s>__ __<s>__ the key observation is that by using maxwell and kaplan packed representations the required statistics can be rewritten as either the max or the sum of a product of functions __</s>__ __<s>__ this is exactly the kind of problem which can be solved by dynamic programming over graphical models __</s>__ __<s>__ stochastic unificationbased grammars define exponential distributions over the parses generated by a unificationbased grammar __</s>__ __<s>__ existing algorithms for parsing and estimation require the enumeration of all of the parses of a string in order to determine the most likely one or in order to calculate the statistics needed to estimate a grammar from a training corpus __</s>__ __<s>__ this paper describes a graphbased dynamic programming algorithm for calculating these statistics from the packed ubg parse representations of maxwell and kaplan which does not require enumerating all parses __</s>__ __<s>__ like many graphical algorithms the dynamic programming algorithm’s complexity is worstcase exponential but is often polynomial __</s>__ __<s>__ the key observation is that by using maxwell and kaplan packed representations the required statistics can be rewritten as either the max or the sum of a product of functions __</s>__ __<s>__ this is exactly the kind of problem which can be solved by dynamic programming over graphical models __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: stochastic unificationbased grammars define exponential distributions over the parses generated by a unificationbased grammar existing algorithms for parsing and estimation require the enumeration of all of the parses of a string in order to determine the most likely one or in order to calculate the statistics needed to estimate a grammar from a training corpus this paper describes a graphbased dynamic programming algorithm for calculating these statistics from the packed ubg parse representations of maxwell and kaplan which does not require enumerating all parses like many graphical algorithms the dynamic programming algorithm’s complexity is worstcase exponential but is often polynomial the key observation is that by using maxwell and kaplan packed representations the required statistics can be rewritten as either the max or the sum of a product of functions this is exactly the kind of problem which can be solved by dynamic programming over graphical models\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is proposed . it we the parse fragments used to be local with respect to the parse fragments used the packed representations , the statistics required for parsing estimation are the kinds of quantities that dynamic programming algorithms for graphical models tree . it seems that the maxwell as features are local to the parse fragments used\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the system will only discuss restaurants that rank highly according to the user ’ s dining preferences , and will only describe attributes of those restaurants the user considers important . the ink stream and m the meaning stream . the __browserand__ componentbased architecture of the multimodal ui facilitated its reuse in a log viewer that reads multimodal log files , replays interactions between the user and system , and allows analysis and annotation of the data . number and type indicate the number of entities in a selection and their type , theatre ) . __mcube__ messages are encoded in xml , providing a general mechanism for message parsing and facilitating logging . we employ the approach proposed in in which the ink meaning lattice is converted to a transducer we also discovered that speech recognition performance was initially hindered by placement of the ‘ __clicktospeak__ ’ button and the recognition feedback box on the bottomright side of the device , leading many users to speak ‘ to ’ this area , rather than toward the microphone on the upper left side . for example , in order to make a comparison between the set of restaurants shown in figure 6 , the text planner first ranks the restaurants within the set according to the predicted ranking of the user model . when multiple selection gestures are present an aggregation technique is employed to overcome the problems with deictic plurals and numerals described in johnston . it starts by zooming in on the first station and then gradually zooms out , graphically presenting each stage of the route along with a series of synchronized tts prompts . the g path in the result is used to __reestablish__ the connection between sem symbols and their specific contents in i : g . after initial openended piloting trials , more structured user tests were conducted , for which we developed a set of six scenarios ordered by increasing level of difficulty . the latticebased finitestate approach to multimodal understanding enables both multimodal integration and dialogue context to compensate for recognition errors . agents may reside either on the client device or elsewhere on the network and can be implemented in multiple different languages . multimodal integrator __mmfst__ receives the speech lattice and the ink meaning lattice and builds a multimodal meaning lattice which captures the potential joint interpretations of the speech and ink inputs . they are free to provide input using speech , by drawing on the display with a stylus , or by using synchronous multimodal combinations of the two modes . when the user draws on the map their ink is captured and any objects potentially selected , such as currently displayed restaurants , are identified . this grammar captures not just multimodal integration patterns but also the parsing of speech and gesture , and the assignment of meaning . the generation process performs templatebased generation for simple responses and updates the system ’ s model of the user ’ s intentions after generation . gesture and handwriting recognition gesture and handwriting recognition agents provide possible classifications of electronic ink for the ui . testing match in nyc this iterative development approach highlighted several important problems early on . __<s>__ mobile interfaces need to allow the user and system to adapt their choice of communication modes according to user preferences the task at hand and the physical and social environment __</s>__ __<s>__ we describe a multimodal application architecture which combines finitestate multimodal language processing a speechact based multimodal dialogue manager dynamic multimodal output generation and usertailored text planning to enable rapid prototyping of multimodal interfaces with flexible input and adaptive output __</s>__ __<s>__ our testbed application match provides a mobile multimodal __speechpen__ interface to restaurant and subway information for new york city __</s>__ __<s>__ mobile interfaces need to allow the user and system to adapt their choice of communication modes according to user preferences the task at hand and the physical and social environment __</s>__ __<s>__ we describe a multimodal application architecture which combines finitestate multimodal language processing a speechact based multimodal dialogue manager dynamic multimodal output generation and usertailored text planning to enable rapid prototyping of multimodal interfaces with flexible input and adaptive output __</s>__ __<s>__ our testbed application match provides a mobile multimodal __speechpen__ interface to restaurant and subway information for new york city __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: mobile interfaces need to allow the user and system to adapt their choice of communication modes according to user preferences the task at hand and the physical and social environment we describe a multimodal application architecture which combines finitestate multimodal language processing a speechact based multimodal dialogue manager dynamic multimodal output generation and usertailored text planning to enable rapid prototyping of multimodal interfaces with flexible input and adaptive output our testbed application match provides a mobile multimodal !!__speechpen__!! interface to restaurant and subway information for new york city\n",
            "INFO:tensorflow:GENERATED SUMMARY: their type , theatre ) . it starts by zooming , providing a will only discuss restaurants that rank highly according to the data . number type type indicate the number of entities a log viewer that reads multimodal log files , replays interactions between the meaning stream . the ink stream architecture m the meaning stream . the ink stream architecture m the meaning stream . the ink stream architecture m the meaning stream . the ink stream architecture m the meaning stream . the ink stream architecture m the meaning stream . the ink stream architecture\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the __bulm__ derivation can be best explained by an example in figure 1. a sketch of the algorithm is as follows . we define an event as a parse action together with its context . we have __20951__ unlabeled sentences for the active learner to select samples . given a sentence , the existing model could generate the top most likely parses , __eachhaving__ a probability : __whereis__ the possible parse __andis__ its associated score . improving speed of sentence clustering is also worthwhile . gorithm converges . right , as the tag wd is the leftmost child of the constituent s ; and the third action is tagging the second word from given the sentence and the two proceeding actions , and so on and so forth . to this end , a modelbased structural distance is defined to quantify how “ far ” two sentences are apart , and with the help of this distance , the active training set is clustered so that we can define and compute the “ density ” of a sample ; second , we propose and test several entropybased measures to quantify the uncertainty of a sample in the active training set using an existing model , as it makes sense to ask human beings to annotate the portion of data for which the existing model is not doing well . most uncertain per cluster : in our implementation , we cluster the active training set so that the number of clusters equals the batch size . the current model ’ s uncertainty about a sentence could be because similar sentences are underrepresented in the training set , or similar sentences are intrinsically difficult . there is a corresponding vocabulary for tag or label , and there are four extension directions : right , left , up and unique . we have shown in section 2.1 that a parse tree can be represented by a sequence of events , each of which can in turn be represented as bitstrings through answering questions . in our implementation , contexts are internally represented as bitstrings through a set of predesigned questions . this work is partially supported by darpa under __spawar__ contract number __n660019928916__ . the tree can then be represented by a sequence of events , which can be “ poured ” down the grown trees , and the count can be updated accordingly – denote the updated count as . __tering__ and the word entropy . as explained above , longer sentences also have larger sentence entropy . since the space of the entire parses is too large and can not be modeled directly , a parse tree is decomposed as a series of individual actions . as a result , longer sentences would have higher change of entropy , in other words , larger impact on models . the horizontal line on the graph is the performance if all 20k sentences are used . in the __bulm__ derivation , there are three types of parse actions : tag , label and extension . our efforts are devoted to two aspects : first , we believe that the selected samples should reflect the underlying distribution of the training corpus . __<s>__ it is necessary to have a annotated corpus to build a statistical parser __</s>__ __<s>__ acquisition of such a corpus is costly and timeconsuming __</s>__ __<s>__ this paper presents a method to reduce this demand using active learning which selects what samples to annotate instead of annotating blindly the whole training corpus __</s>__ __<s>__ sample selection for annotation is based upon __“representativeness”__ and “usefulness” __</s>__ __<s>__ a modelbased distance is proposed to measure the difference of two sentences and their most likely parse trees __</s>__ __<s>__ based on this distance the active learning process analyzes the sample distribution by clustering and calculates the density of each sample to quantify its representativeness __</s>__ __<s>__ further more a sentence is deemed as useful if the existing model is highly uncertain about its parses where uncertainty is measured by various entropybased scores __</s>__ __<s>__ experiments are carried out in the shallow semantic parser of an air travel dialog system __</s>__ __<s>__ our result shows that for about the same parsing accuracy we only need to annotate a third of the samples as compared to the usual random selection method __</s>__ __<s>__ it is necessary to have a annotated corpus to build a statistical parser __</s>__ __<s>__ acquisition of such a corpus is costly and timeconsuming __</s>__ __<s>__ this paper presents a method to reduce this demand using active learning which selects what samples to annotate instead of annotating blindly the whole training corpus __</s>__ __<s>__ sample selection for annotation is based upon __“representativeness”__ and “usefulness” __</s>__ __<s>__ a modelbased distance is proposed to measure the difference of two sentences and their most likely parse trees __</s>__ __<s>__ based on this distance the active learning process analyzes the sample distribution by clustering and calculates the density of each sample to quantify its representativeness __</s>__ __<s>__ further more a sentence is deemed as useful if the existing model is highly uncertain about its parses where uncertainty is measured by various entropybased scores __</s>__ __<s>__ experiments are carried out in the shallow semantic parser of an air travel dialog system __</s>__ __<s>__ our result shows that for about the same parsing accuracy we only need to annotate a third of the samples as compared to the usual random selection method __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: it is necessary to have a annotated corpus to build a statistical parser acquisition of such a corpus is costly and timeconsuming this paper presents a method to reduce this demand using active learning which selects what samples to annotate instead of annotating blindly the whole training corpus sample selection for annotation is based upon !!__“representativeness”__!! and “usefulness” a modelbased distance is proposed to measure the difference of two sentences and their most likely parse trees based on this distance the active learning process analyzes the sample distribution by clustering and calculates the density of each sample to quantify its representativeness further more a sentence is deemed as useful if the existing model is highly uncertain about its parses where uncertainty is measured by various entropybased scores experiments are carried out in the shallow semantic parser of an air travel dialog system our result shows that for about the same parsing accuracy we only need to annotate a third of the samples as compared to the usual random selection method\n",
            "INFO:tensorflow:GENERATED SUMMARY: on can be best explained by an existing as a parse action together with its context . the current model ’ the “ density ” of a sample ; second , we propose test several entropybased measures to quantify the uncertainty of a sample the active training set using an existing model , as it makes sense to ask human beings to annotate the portion of data for which the existing model is as follows . we define an event as a parse action together with its context . the current model ’ s uncertainty about a sentence could be\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the question term could appear in the documents obtained from the web in various ways . is answered by “ denver ’ s new airport , __topped__ with white __fiberglass__ __cones__ in imitation of the rocky mountains in the background , continues to lie empty ” , because the system picked the answer “ the background ” using the pattern co = the performance of the system depends significantly on there being only one anchor word , which allows a single word match between the question and the candidate answer sentence . using a named entity tagger and/or an ontology would enable the system to use the knowledge that “ background ” is not a location . these techniques are greatly aided by the fact that there is no need to handtag a corpus , while the abundance of data on the web makes it easier to determine reliable statistical estimates . in the first case , the trec corpus was used as the input source and ir was performed by the ir component of our qa system . the rather long list of patterns obtained would have been very difficult for any human to come up with manually . it fails to perform under certain conditions as exemplified by the question “ when was __lyndon__ b. johnson born ? ” . ” we check the presence of the following strings in the answer sentence all the answers need to be __enlisted__ to ensure a high confidence in the precision score of each pattern , in the present framework . as will be seen later , this allows us to differentiate patterns such as d from its more general substring c . the system can not locate the answer in “ london , which has one of the most __busiest__ airports in the world , lies on the banks of the river __thames__ ” due to the explosive danger of unrestricted wildcard matching , as would be required in the pattern “ < question > , * i ) mozart was born in < __any_word__ > ii ) mozart was born in __1756__ calculate the precision of each pattern by the formula p = ca / co since the output from the web contains many correct answers among the top ones , a simple word count could help in eliminating many unlikely answers . “ the < name > in < answer > , ” . a similar result for qa was obtained by brill et al . . retain only the patterns matching a sufficient number of examples . canonicalization of words is also an issue . no external knowledge has been added to these patterns . these may include named entity taggers , wordnet , parsers , handtagged corpora , and ontology lists . our method uses the machine learning technique of bootstrapping to build a large tagged corpus starting with only a few examples of qa pairs . the trec corpus does not have enough candidate answers with high precision score and has to settle for answers extracted from sentences matched by low precision patterns . the apparent power of such patterns surprised many . these questions were run through the testing phase of the algorithm . __<s>__ in this paper we explore the power of surface text patterns for opendomain question answering systems __</s>__ __<s>__ in order to obtain an optimal set of patterns we have developed a method for learning such patterns automatically __</s>__ __<s>__ a tagged corpus is built from the internet in a bootstrapping process by providing a few handcrafted examples of each question type to altavista __</s>__ __<s>__ patterns are then automatically extracted from the returned documents and standardized __</s>__ __<s>__ we calculate the precision of each pattern and the average precision for each question type __</s>__ __<s>__ these patterns are then applied to find answers to new questions __</s>__ __<s>__ using the trec10 question set we report results for two cases: answers determined from the trec10 corpus and from the web __</s>__ __<s>__ in this paper we explore the power of surface text patterns for opendomain question answering systems __</s>__ __<s>__ in order to obtain an optimal set of patterns we have developed a method for learning such patterns automatically __</s>__ __<s>__ a tagged corpus is built from the internet in a bootstrapping process by providing a few handcrafted examples of each question type to altavista __</s>__ __<s>__ patterns are then automatically extracted from the returned documents and standardized __</s>__ __<s>__ we calculate the precision of each pattern and the average precision for each question type __</s>__ __<s>__ these patterns are then applied to find answers to new questions __</s>__ __<s>__ using the trec10 question set we report results for two cases: answers determined from the trec10 corpus and from the web __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in this paper we explore the power of surface text patterns for opendomain question answering systems in order to obtain an optimal set of patterns we have developed a method for learning such patterns automatically a tagged corpus is built from the internet in a bootstrapping process by providing a few handcrafted examples of each question type to altavista patterns are then automatically extracted from the returned documents and standardized we calculate the precision of each pattern and the average precision for each question type these patterns are then applied to find answers to new questions using the trec10 question set we report results for two cases: answers determined from the trec10 corpus and from the web\n",
            "INFO:tensorflow:GENERATED SUMMARY: the question term could appear the documents obtained from the web various various ways . as will be seen later , this allows us to differentiate the documents obtained from the web various ways . is answered by “ denver ’ s new airport the documents obtained from the web various ways . is answered by “ denver ’ s new airport the question term could appear the documents obtained from the web various ways . is answered by “ denver ’ s new airport , topped\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  and we define the forward probabilities as the probability that it will start from and output on the left stream , and on the right stream and be in state , and the backward probabilities as the probability that starting from state it will output on the right and on the left and then terminate , ie end in state . for a parametric model with free parameters , the set of all these models will form a smooth dimensional manifold in the space of all distributions . this is equivalent to the task of finding the most likely string generated by a hmm , which is nphard , but it is possible to sample from the conditional distribution , which allows an efficient stochastic computation . we store all of the observed pairs of strings . __nakisa__ it is a tensor because it transforms properly when the parametrization is changed . the techniques presented here operate directly on sequences of atomic symbols , using a much less articulated representation , and much less input information . given a fst , and a string , we often need to find the string that maximizes . given a generative model for a string , one can use the sufficient statistics of those generative models as features . there are a number of reasons why this simple approach will not work : first , for many languages the inflected form is lexically not phonologically specified and thus the model will not be able to identify the correct form ; secondly , modelling all of the irregular exceptions in a single transduction is computationally intractable at the moment . this algorithm will maximize the joint probability of the training data . the data is classified according to the type of the plural , and is mapped onto a syllabic skeleton , with each phoneme represented as a bundle of phonological features . the second term in equation 2 corresponding to the normalization can be neglected . otherwise we sample repeatedly from the conditional distribution of each of the submodels . the fst then defines a joint probability distribution on pairs of strings from the alphabet . the test data consists of all the remaining verbs . it is possible to modify the normal dynamicprogramming training algorithm for hmms , the baumwelch algorithm to work with fsts as well . this was taken as evidence for the presence of two qualitatively distinct modules in human morphological processing . however these approaches are not a complete solution to the problem of learning morphology , since they do not directly produce the transduction . the training data consists of all of the verbs with a nonzero lemma spoken frequency in the 1.3 million word cobuild corpus . these results are presented in table 5. the curvature of this manifold can be described by a __riemannian__ tensor – this tensor is just the expected fisher information for that model . __ept__ in sampa transcription . __<s>__ this paper discusses the supervised learning of morphology using stochastic transducers trained using the expectationmaximization algorithm __</s>__ __<s>__ two approaches are presented: first using the transducers directly to model the process and secondly using them to define a similarity measure related to the fisher kernel method and then using a memorybased learning technique __</s>__ __<s>__ these are evaluated and compared on data sets from english german slovene and arabic __</s>__ __<s>__ this paper discusses the supervised learning of morphology using stochastic transducers trained using the expectationmaximization algorithm __</s>__ __<s>__ two approaches are presented: first using the transducers directly to model the process and secondly using them to define a similarity measure related to the fisher kernel method and then using a memorybased learning technique __</s>__ __<s>__ these are evaluated and compared on data sets from english german slovene and arabic __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper discusses the supervised learning of morphology using stochastic transducers trained using the expectationmaximization algorithm two approaches are presented: first using the transducers directly to model the process and secondly using them to define a similarity measure related to the fisher kernel method and then using a memorybased learning technique these are evaluated and compared on data sets from english german slovene and arabic\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . the techniques presented here operate directly on sequences of atomic symbols , using a much less for the backward probabilities as the probability that starting from state it will output on the left stream , the probability that starting from state it will output on the left stream , the probability that starting from state it will output on the left stream , the probability that starting from state it will output on the left stream , the probability that starting from state it will output on the left stream , the probability that\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  section 6 describes a way to model the interaction of ellipsis resolution and scope resolution in an underspecified structure . in __udrt__ , scope decisions are modelled as subordination constraints . a __udrs__ is disambiguated by adding subordination constraints . the elliptical clause can also be contained in the source , cf . the indefinite np gets narrow scope under “ before ” . whenever an occurrence of ellipsis is recognized , a counter is incremented . remember that in each process of ellipsis resolution the parallelism module returns a bijective function which expresses the parallelism between labels and discourse referents in source and target . a __udrs__ is a triple consisting of a top label , a set of labelled conditions or discourse referents , and a set of subordination constraints . section 3 formulates the general setup of ellipsis resolution assumed in the rest of the paper . __crouch__ considers __ordersensitivity__ of interpretation a serious drawback . understood in this sense , underspecification often obviates the need for complete disambiguation . in prolog , class membership is most efficiently tested via unification . to bind an anaphor to some antecedent expression , a binding constraint and an equality constraint between two discourse referents are introduced . it also provides a bijective function associating the parallel labels and discourse referents in source and target . to this end , a method to encode “ phantom conditions ” has been presented , i.e . subformulae whose presence depends on the scope configuration . for reasons that will become clear only in section 7 discourse referents also have contexts . john went to the station , and every student did too , on a bike . if the subsequent discourse contains a plural anaphoric np such as “ both solutions ” , two or more discourse referents designating solutions are looked for . but none of the approaches mentioned can ascertain this fact without complete scope resolution . every professor found a solution before most students did , and every assistant did too . the example is problematic for all approaches which assume source and target scope order to be identical . constraints are only evaluated when the underspecified representation is finally interpreted . section 2 gives a short introduction to __udrt__ . erk and koller go on to propose an extension of clls that permits the reading . rather , some “ parallelism ” module is assumed to take care of task 1. consequently , sentence shows that subordination constraints may affect more than one pair of labels . section 8 concludes . erk and koller discuss sentence which has a reading in which each student went to the station on a different bike . the qlf approach gives an interesting answer to this question : it uses reentrancy to propagate scope decisions among parallel structures . another consequence is , however , that the strategy of postponing disambiguation steps is in some cases insufficient . now we can model the parallelism effects by stipulating that a subordination constraint connects two equivalence classes and rather than two individual labels and . if the decisions on scope order have not yet been taken , how can they be guaranteed to be the same in source and target ? __<s>__ the paper presents an approach to ellipsis resolution in a framework of scope underspecification __</s>__ __<s>__ it is argued that the approach improves on previous proposals to integrate ellipsis resolution and scope underspecification in that application processes like anaphora resolution do not require full disambiguation but can work directly on the underspecified representation __</s>__ __<s>__ furthermore it is shown that the approach presented can cope with the examples discussed by dalrymple et al __</s>__ __<s>__ as well as a problem noted recently by erk and koller __</s>__ __<s>__ the paper presents an approach to ellipsis resolution in a framework of scope underspecification __</s>__ __<s>__ it is argued that the approach improves on previous proposals to integrate ellipsis resolution and scope underspecification in that application processes like anaphora resolution do not require full disambiguation but can work directly on the underspecified representation __</s>__ __<s>__ furthermore it is shown that the approach presented can cope with the examples discussed by dalrymple et al __</s>__ __<s>__ as well as a problem noted recently by erk and koller __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: the paper presents an approach to ellipsis resolution in a framework of scope underspecification it is argued that the approach improves on previous proposals to integrate ellipsis resolution and scope underspecification in that application processes like anaphora resolution do not require full disambiguation but can work directly on the underspecified representation furthermore it is shown that the approach presented can cope with the examples discussed by dalrymple et al as well as a problem noted recently by erk and koller\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is disambiguated by adding subordination constraints . the elliptical clause this underspecified . the elliptical clause this sense , underspecification often obviates the need for complete disambiguation . remember that will a way to model the interaction of ellipsis resolution scope resolution an underspecified structure . it also provides a bijective function associating the interaction of ellipsis resolution scope resolution an underspecified structure . it also provides a bijective function associating the interaction of ellipsis resolution scope resolution\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  suppose we have five candidate classes a , b , c , d and e , and the true class of x is b. figure 1 shows the created training examples . morphological analysis is conducted by choosing the most likely path on it . the computational cost in testing is also large , because all the classifiers have to work on each test example . one advantage of revision learning is its small computational cost . for any subpaths from the beginning of the sentence in the lattice , its generative probability can be calculated using hmms . to solve this problem , we propose a revision learning method which combines a model with high generalization capacity and a model with small computational cost to achieve high performance with small computational cost . if the classifier answers the example as incorrect , the next highest ranked class becomes the next candidate for checking . the __oneversusrest__ method is not used because it is not applicable to morphological analysis of nonsegmented languages directly . however , this method has the problem of being computationally costly in training , because the negative examples are created for all the classes other than the true class , and the total number of the training examples becomes large . revision learning uses a binary classifier with higher capacity to revise the errors made by the stochastic model with lower capacity as follows : during the training phase , a ranking is assigned to each class by the stochastic model for a training example , that is , the candidate classes are sorted in descending order of its conditional probability given the example . however , revision learning could not surpass the __oneversusrest__ . the dictionary for hmms is constructed from all the words in the training data . the __oneversusrest__ method is known as one of such methods . many tasks in natural language processing such as pos tagging are regarded as a multiclass classification problem . recently , combinations of multiple learners have been studied to achieve high performance . we assume a binary classifier f that returns positive or negative real value for the class of x , where the absolute value |f | reflects the confidence of the classification . the corpus is randomly separated into training data of __33,831__ sentences and test data of __3,758__ sentences . using linear kernel , the accuracy decreases a little , but the computational cost is much lower than the second order polynomial kernel . we adopt hmms as the stochastic model and svms as the binary classifier . learning models with higher capacity may not be of practical use because of their prohibitive computational cost . then a binary classifier for each class is trained using the examples , and five classifiers are created for this problem . during the test phase , first the ranking of the candidate classes for a given example is assigned by the stochastic model as in the training . __<s>__ this paper presents a revision learning method that achieves high performance with small computational cost by combining a model with high generalization capacity and a model with small computational cost __</s>__ __<s>__ this method uses a high capacity model to revise the output of a small cost model __</s>__ __<s>__ we apply this method to english partofspeech tagging and japanese morphological analysis and show that the method performs well __</s>__ __<s>__ this paper presents a revision learning method that achieves high performance with small computational cost by combining a model with high generalization capacity and a model with small computational cost __</s>__ __<s>__ this method uses a high capacity model to revise the output of a small cost model __</s>__ __<s>__ we apply this method to english partofspeech tagging and japanese morphological analysis and show that the method performs well __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper presents a revision learning method that achieves high performance with small computational cost by combining a model with high generalization capacity and a model with small computational cost this method uses a high capacity model to revise the output of a small cost model we apply this method to english partofspeech tagging and japanese morphological analysis and show that the method performs well\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . information a model applicable to morphological is also large , because the classifiers have to work on each test example . one advantage of revision learning is also large , because all the classifiers have to work on each test example . one advantage of revision learning is its small computational cost . for any subpaths from the beginning of the sentence testing is also large , because all the classifiers have to work on each test example . one advantage of revision learning is its small computational cost . for any subpaths from\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the type can be grammatical or the position of w� in a context window : the relation indicates that the term dog , was the direct object of the verb walk . the next section describes the method of evaluating each thesaurus created by the combination of a given context extraction system and corpus size . is the number of different words aj appears in relations with . since minipar performs morphological analysis on the context relations we have added an existing morphological analyser to the other extractors . to avoid sample bias , the words were randomly selected from wordnet such that they covered a range of values for the following word properties : the larger windows with low correlation between the thesaurus term and context , extract a massive context representation but the results are about 10 % worse than the syntactic extractors . more recently , semantic resources have also been used in collocation discovery , smoothing and model estimation and text classification . the times reported below include the naive bayes pos tagging time . the jaccard measure is the cardinality ratio of the intersection and union of attribute sets is the attribute set for wn ) : the generalised jaccard measure allows each relation to have a significance weight associated with it : where f is the frequency of the relation and n extractors marked with an asterisk , for example w , do not distinguish between different positions of the word w ' in the window . vectorspace thesaurus extraction can be separated into two independent processes . the estimate indicates that minipar will continue to be the best performer on direct matching . the macquarie consists of 812 large topics , each of which is separated into __21174__ small synonym sets . to help overcome the problems of coarsegrained direct comparisons we use three different types of measure to evaluate thesaurus quality : a match is an extracted synonym that appears in the corresponding gold standard synonym list . thesaurus extraction is a good task to use to experiment with scaling context spaces . however , sextant runs about 28 times faster than minipar . but also , thesaurus extraction is a task where success has been limited when using small corpora ; corpora of the order of 300 million words have already been shown to be more successful at this task . we trained a thesaurus extraction system with a range of __contextextracting__ frontends to demonstrate the interaction between context quality , extraction time and representation size . we have demonstrated the behaviour of a simple learning algorithm on much more complicated contextual information on very large corpora . the first thing to note is the time spent extracting contextual information : minipar takes significantly longer to run than the other extractors . for the purposes of evaluation , we selected 70 single word noun terms for thesaurus extraction . secondly , sextant and minipar have quite similar results overall , but minipar is slightly better across most measures . the second step in thesaurus extraction performs clustering or nearestneighbour analysis to determine which terms are similar based on their context vectors . __<s>__ context is used in many nlp systems as an indicator of a term’s syntactic and semantic function __</s>__ __<s>__ the accuracy of the system is dependent on the quality and quantity of contextual information available to describe each term __</s>__ __<s>__ however the quantity variable is no longer fixed by limited corpus resources __</s>__ __<s>__ given fixed training time and computational resources it makes sense for systems to invest time in extracting high quality contextual information from a fixed corpus __</s>__ __<s>__ however with an effectively __limitless__ quantity of text available extraction rate and representation size need to be considered __</s>__ __<s>__ we use thesaurus extraction with a range of context extracting tools to demonstrate the interaction between context quantity time and size on a corpus of 300 million words __</s>__ __<s>__ context is used in many nlp systems as an indicator of a term’s syntactic and semantic function __</s>__ __<s>__ the accuracy of the system is dependent on the quality and quantity of contextual information available to describe each term __</s>__ __<s>__ however the quantity variable is no longer fixed by limited corpus resources __</s>__ __<s>__ given fixed training time and computational resources it makes sense for systems to invest time in extracting high quality contextual information from a fixed corpus __</s>__ __<s>__ however with an effectively __limitless__ quantity of text available extraction rate and representation size need to be considered __</s>__ __<s>__ we use thesaurus extraction with a range of context extracting tools to demonstrate the interaction between context quantity time and size on a corpus of 300 million words __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: context is used in many nlp systems as an indicator of a term’s syntactic and semantic function the accuracy of the system is dependent on the quality and quantity of contextual information available to describe each term however the quantity variable is no longer fixed by limited corpus resources given fixed training time and computational resources it makes sense for systems to invest time in extracting high quality contextual information from a fixed corpus however with an effectively !!__limitless__!! quantity of text available extraction rate and representation size need to be considered we use thesaurus extraction with a range of context extracting tools to demonstrate the interaction between context quantity time and size on a corpus of 300 million words\n",
            "INFO:tensorflow:GENERATED SUMMARY: the type can be grammatical or the position of w� a context window : the relation indicates that the term dog , was the direct object of the verb walk . the times reported below include the naive bayes pos tagging time . the times reported below include the naive bayes pos tagging time . the times reported below include the naive bayes pos tagging time . the times reported below include the naive bayes pos tagging time . the times reported below include the naive bayes pos tagging time . the times reported below include the naive bayes pos\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  always guessing the majority class would yield an accuracy of 59 % . to eliminate any possible influence from the testing data , i added “ and ” to every query , which tells altavista not to include the epinions web site in its searches . past work has demonstrated that adjectives are good indicators of subjective , evaluative sentences . a natural question , given the preceding results , is what makes movie reviews hard to classify ? the directionality of a document is determined by its deep argumentative structure , rather than a shallow analysis of its adjectives . the simplicity of __pmiir__ may encourage further work with semantic orientation . table 2 shows an example for a recommended review and table 3 the first step of the algorithm is to extract phrases containing adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations and a negative semantic orientation when it has bad associations . another area for future work is to empirically compare __pmiir__ and the algorithm of hatzivassiloglou and mckeown . although they do not consider the task of classifying reviews , it seems their algorithm could be plugged into the classification algorithm presented in section 2 , where it would replace __pmiir__ and equation in the second step . reviews at epinions are not written by professional writers ; any person with a web browser can become a member of epinions and contribute a review . so is positive when phrase is more strongly associated with “ excellent ” and negative when phrase is more strongly associated with “ poor ” . the second pattern , for example , means that two consecutive words are extracted if the first word is an adverb and the second word is an adjective , but the third word can not be a noun . the latter difficulty might be addressed by using semantic orientation combined with other features in a supervised classification algorithm . the company __mindfuleye7__ offers a technology called __lexanttm__ that appears similar to tong however , i cached all query results , and since there is no need to recalculate hits and hits for every phrase , each phrase requires an average of slightly less than two queries . except for the travel reviews , there is surprisingly little variation in the accuracy within a domain . their algorithm performs well , but it is designed for isolated adjectives , rather than phrases containing adjectives or adverbs . table 5 shows the experimental results . as mentioned in the introduction , one application is to provide summary statistics for search engines . the classification algorithm is evaluated on 410 reviews from __epinions2__ , randomly sampled from four different domains : reviews of automobiles , banks , movies , and travel destinations . the first step is to use a partofspeech tagger to identify phrases in the input text that contain adjectives or adverbs . the log of this ratio is the amount of information that we acquire about the presence of one of the words when we observe the other . in addition to recommended and not recommended , epinions reviews are classified using the five star rating system . __<s>__ this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended or not recommended __</s>__ __<s>__ the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs __</s>__ __<s>__ a phrase has a positive semantic orientation when it has good associations and a negative semantic orientation when it has bad associations __</s>__ __<s>__ in this paper the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word __“excellent”__ minus the mutual information between the given phrase and the word __“poor”__ __</s>__ __<s>__ a review is classified as recommended if the average semantic orientation of its phrases is positive __</s>__ __<s>__ the algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from epinions sampled from four different domains __</s>__ __<s>__ the accuracy ranges from 84% for automobile reviews to 66% for movie reviews __</s>__ __<s>__ this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended or not recommended __</s>__ __<s>__ the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs __</s>__ __<s>__ a phrase has a positive semantic orientation when it has good associations and a negative semantic orientation when it has bad associations __</s>__ __<s>__ in this paper the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word __“excellent”__ minus the mutual information between the given phrase and the word __“poor”__ __</s>__ __<s>__ a review is classified as recommended if the average semantic orientation of its phrases is positive __</s>__ __<s>__ the algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from epinions sampled from four different domains __</s>__ __<s>__ the accuracy ranges from 84% for automobile reviews to 66% for movie reviews __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended or not recommended the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs a phrase has a positive semantic orientation when it has good associations and a negative semantic orientation when it has bad associations in this paper the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word !!__“excellent”__!! minus the mutual information between the given phrase and the word !!__“poor”__!! a review is classified as recommended if the average semantic orientation of its phrases is positive the algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from epinions sampled from four different domains the accuracy ranges from 84% for automobile reviews to 66% for movie reviews\n",
            "INFO:tensorflow:GENERATED SUMMARY: its searches . past work has demonstrated that adjectives are good indicators of subjective , evaluative are good indicators of subjective , evaluative a negative semantic orientation . table 2 shows an example for tells altavista not to include the epinions web site its searches . past work has demonstrated that adjectives are good indicators of subjective , evaluative a negative semantic orientation . table 2 shows an example for tells altavista not to include the epinions web site its searches . past work has demonstrated that adjectives are good indicators of subjective , evaluative are good indicators of subjective\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  text appearing wordforword to express the same information ; in our problem , we consider how much newspaper text can be maximally covered by words from pa copy . these texts cover 265 different stories from july 1999 to june 2000 and all newspaper stories have been manually classified at the documentlevel . __+63__ a naive bayes classifier was used because of its success in previous classification tasks , however we are aware of its naive assumptions that attributes are assumed independent and data to be normally distributed . figure 1 shows the result of gst for the example in section 2 . 772 of these texts are pa copy and __944__ from the nine newspapers . for example consider the following : original a __drinkdriver__ who ran into the queen mother 's official daimler was fined $ 700 and banned from driving for two years . in other words we measure the proportion of unique ngrams in b that are found in a. we have concentrated on just three : ngram overlap measures , greedy string tiling , and sentence alignment . at the lexical or word sequence level , individual words and phrases within a newspaper story are classified as to whether they are used to express the same information as words in news agency copy and or used to express information not found in agency copy . here cognates are defined as pairs of terms that are identical , share the same stems , or are substitutable in the given context . table 1 shows the results of the single ternary classifiers . if no such candidate is found , the dt sentence is assumed to be independent of the st. based on individual dt sentence alignments , the overall possibility of derivation for the dt is estimated with a score ranging between 0 and 1. at the document level , newspaper stories are assigned to one of three possible categories coarsely reflecting the amount of text reused from the news agency and the dependency of the newspaper story upon news agency copy for the provision of & ; facts & ; . these results were ordered according to each set to create the same 10 datasets for each approach thereby enabling comparison . they are : 1 ) __whollyderived__ : all text in the newspaper article is rewritten only from news agency copy ; 2 ) __partiallyderived__ : some text is derived from the news agency , but other sources have also been used ; and 3 ) nonderived : news agency has not been used as the source of the article ; although words may still cooccur between the newspaper article and news agency copy on the same topic , the journalist is confident the news agency has not been used . note the high misclassification of pd as both wd and nd , reflecting the difficulty of separating this class . often __newsworkers__ , such as the reporter and editor , will rely upon news agency copy as the basis of a news story or to verify facts and assess the importance of a story in the context of all those appearing on the newswire . __<s>__ in this paper we present results from the meter project whose aim is to explore issues pertaining to text reuse and derivation especially in the context of newspapers using newswire sources __</s>__ __<s>__ although the reuse of text by journalists has been studied in linguistics we are not aware of any investigation using existing computational methods for this particular task __</s>__ __<s>__ we investigate the classification of newspaper articles according to their degree of dependence upon or derivation from a newswire source using a simple 3level scheme designed by journalists __</s>__ __<s>__ three approaches to measuring text similarity are considered: ngram overlap greedy string tiling and sentence alignment __</s>__ __<s>__ measured against a manually annotated corpus of source and derived news text we show that a combined classifier with features automatically selected performs best overall for the ternary classification achieving an average flmeasure score of 0 664 across all three categories __</s>__ __<s>__ in this paper we present results from the meter project whose aim is to explore issues pertaining to text reuse and derivation especially in the context of newspapers using newswire sources __</s>__ __<s>__ although the reuse of text by journalists has been studied in linguistics we are not aware of any investigation using existing computational methods for this particular task __</s>__ __<s>__ we investigate the classification of newspaper articles according to their degree of dependence upon or derivation from a newswire source using a simple 3level scheme designed by journalists __</s>__ __<s>__ three approaches to measuring text similarity are considered: ngram overlap greedy string tiling and sentence alignment __</s>__ __<s>__ measured against a manually annotated corpus of source and derived news text we show that a combined classifier with features automatically selected performs best overall for the ternary classification achieving an average flmeasure score of 0 664 across all three categories __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in this paper we present results from the meter project whose aim is to explore issues pertaining to text reuse and derivation especially in the context of newspapers using newswire sources although the reuse of text by journalists has been studied in linguistics we are not aware of any investigation using existing computational methods for this particular task we investigate the classification of newspaper articles according to their degree of dependence upon or derivation from a newswire source using a simple 3level scheme designed by journalists three approaches to measuring text similarity are considered: ngram overlap greedy string tiling and sentence alignment measured against a manually annotated corpus of source and derived news text we show that a combined classifier with features automatically selected performs best overall for the ternary classification achieving an average flmeasure score of 0 664 across all three categories\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a the same information ; we express the same information ; this the same information ; this the same information ; this the same information ; the appearing wordforword to express the same information ; the appearing wordforword to express the same information ; our problem , we consider how much newspaper text can be maximally covered by words from pa copy . these texts cover 265 different stories from july 1999 to be normally distributed . figure 1 shows the result found wordforword to express the same information ;\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  in this paper , we present a document compression system that uses hierarchical models of discourse and syntax in order to simultaneously manage all these conflicting goals . the source model assigns to a string the probability , the probability that the summary is good english . because the mitre data contained mostly short sentences , the syntax and discourse parsers made fewer errors , which allowed for better compressions to be generated . for estimating the parameters for the discourse models , we used an rst corpus of 385 wall street journal articles from the penn treebank , which we obtained from ldc . their system compressed sentences by dropping syntactic constituents , but could be applied to entire documents only on a sentencebysentence basis . through a sequence of discourse expansions , we can expand upon this summary to reach the original text . this models the extent to which is a good expansion of . that is , the source model should measure how good english a summary is . finally , we can compress the root deriving into __sat=background__ __nuc=span__ by dropping the __sat=background__ constituent . in order to achieve this goal , we extent the noisychannel model proposed by knight & marcu . the mayor is now looking for __reelection__ . the “ document compression ” entry in table 1 shows a grammatical , coherent summary of text , which was generated by a hypothetical document compression system that preserves the most important information in a text while deleting sentences , phrases , and words that are subsidiary to the main message of the text . choosing the manner in which the discourse and syntactic representations of texts are mixed should be influenced by the genre of the texts one is interested to compress . for example , the first sentence in text provides background information for interpreting the information in sentences 2 and 3 , which are in a contrast relation . the noise added in our model consists of words , phrases and discourse units . a __constituentbut__ without the support of the __governer__ , for expand operation could insert a syntactic constituent , such as “ this year ” anywhere in the syntactic tree of . experimentally , we found a reasonable metric was to , for a compression of length , divide each logprobability by . we call this set the mitre corpus . this expansion adds the probability of performing the expansion . we are now able to perform the second operation , __dexpand__ , with which we expand on the core message contained in by adding a satellite which evaluates the information presented in . __goals1__ . when summarizing text , some sentences should be dropped altogether . the rest of figure 2 shows some of the remaining steps to produce the original document , each step labeled with the appropriate probability factors . the deletion of certain words and phrases may also lead to ungrammaticality and information loss . we also consider methods for scaling up the decoder to handling documents of more realistic length . we assume that we are given the discourse structure of each document and the syntactic structures of each of its edus . __<s>__ we present a document compression system that uses a hierarchical noisychannel model of text production __</s>__ __<s>__ our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input __</s>__ __<s>__ the system then uses a statistical hierarchical model of text production in order to drop __nonimportant__ syntactic and discourse constituents so as to generate coherent grammatical document compressions of arbitrary length __</s>__ __<s>__ the system outperforms both a baseline and a sentencebased compression system that operates by simplifying sequentially all sentences in a text __</s>__ __<s>__ our results support the claim that discourse knowledge plays an important role in document summarization __</s>__ __<s>__ we present a document compression system that uses a hierarchical noisychannel model of text production __</s>__ __<s>__ our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input __</s>__ __<s>__ the system then uses a statistical hierarchical model of text production in order to drop __nonimportant__ syntactic and discourse constituents so as to generate coherent grammatical document compressions of arbitrary length __</s>__ __<s>__ the system outperforms both a baseline and a sentencebased compression system that operates by simplifying sequentially all sentences in a text __</s>__ __<s>__ our results support the claim that discourse knowledge plays an important role in document summarization __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we present a document compression system that uses a hierarchical noisychannel model of text production our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input the system then uses a statistical hierarchical model of text production in order to drop !!__nonimportant__!! syntactic and discourse constituents so as to generate coherent grammatical document compressions of arbitrary length the system outperforms both a baseline and a sentencebased compression system that operates by simplifying sequentially all sentences in a text our results support the claim that discourse knowledge plays an important role in document summarization\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a compress the root deriving into of order to simultaneously manage all these a grammatical , coherent summary of text , which was generated by a hypothetical document compression system that preserves the most important information table 1 shows a grammatical , coherent summary of text , which was generated by a hypothetical document compression system that preserves the most important information table 1 shows a grammatical , coherent summary of text , which was generated by a hypothetical document compression system that preserves the most important information\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the subcategorisation frame descriptions were formally evaluated by comparing the automatically generated verb frames against manual definitions in the german dictionary __duden__ – das __stilwörterbuch__ . the task of evaluating the result of a cluster analysis against the known gold standard of handconstructed verb classes requires us to assess the similarity between two sets of equivalence relations . they are consistent with the german verb classification in as far as the relevant verbs appear in his less extensive semantic ‘ fields ’ . from the methodological point of view , the clustering evaluation gave interesting insights into kmeans ’ behaviour on the syntactic frame data . this is similar to the perspective of , who present , in the context of the muc coreference evaluation scheme , a modeltheoretic measure of the similarity between equivalence classes . the cluster analysis was obtained by running kmeans on a random cluster initialisation , with information radius as distance measure ; the verb description contained condition 2 subcategorisation frame types with pp information . our long term goal is to support the development of highquality and largescale lexical resources . equation , and skew divergence , recently shown as an effective measure for distributional similarity , cf . equation . the semantic aspects and majority of verbs are closely related to levin ’ s english classes . as an extreme example , the semantic class support contains the verb __unterstützen__ , which syntactically requires a direct object , together with the three verbs __dienen__ , __folgen__ , __helfen__ which dominantly subcategorise an indirect object . the resulting lexical subcategorisation for __reden__ and the frame confusion has been caused by parsing mistakes for the infrequent verb ; ni is not among the frames possibly subcategorised by __rudern__ . for measuring the quality of an individual cluster , the cluster purity of each cluster is defined by its largest , the number of members that are projected into the same class . number of correct pairs in number of verbs in we also created a more delicate version of subcategorisation frames that discriminates between different kinds of __pparguments__ . but initialisation using ward ’ s method , which produces tighter clusters and a narrower range of cluster sizes does outperform random cluster initialisation . prepositional phrases are referred to by case and preposition , such as ‘ __dat.mit__ ’ , ‘ __akk.für__ ’ . in both conditions verbs were clustered using kmeans , an iterative , unsupervised , hard clustering method with wellknown properties , cf . . the linguistic investigation gives some insight into the reasons for the success of our clustering technique . hierarchies imposing a strong structure on the clustering are hardly improved by kmeans . as noted by , it is useful to have an evaluation measure that does not depend on the choice of similarity measure or on the original dimensionality of the input data , since that allows meaningful comparison of results for which these parameters vary . their evaluation results are noticeably below those for random clusters . semantic verb classes have been defined for several languages , with dominant examples concerning english and spanish . standard choices include the cosine , euclidean distance , manhattan metric , and variants of the kullbackleibler divergence . __<s>__ the paper describes the application of kmeans a standard clustering technique to the task of inducing semantic classes for german verbs __</s>__ __<s>__ using probability distributions over verb subcategorisation frames we obtained an intuitively plausible clustering of 57 verbs into 14 classes __</s>__ __<s>__ the automatic clustering was evaluated against independently motivated handconstructed semantic verb classes __</s>__ __<s>__ a series of posthoc cluster analyses explored the influence of specific frames and frame groups on the coherence of the verb classes and supported the tight connection between the syntactic behaviour of the verbs and their lexical meaning components __</s>__ __<s>__ the paper describes the application of kmeans a standard clustering technique to the task of inducing semantic classes for german verbs __</s>__ __<s>__ using probability distributions over verb subcategorisation frames we obtained an intuitively plausible clustering of 57 verbs into 14 classes __</s>__ __<s>__ the automatic clustering was evaluated against independently motivated handconstructed semantic verb classes __</s>__ __<s>__ a series of posthoc cluster analyses explored the influence of specific frames and frame groups on the coherence of the verb classes and supported the tight connection between the syntactic behaviour of the verbs and their lexical meaning components __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: the paper describes the application of kmeans a standard clustering technique to the task of inducing semantic classes for german verbs using probability distributions over verb subcategorisation frames we obtained an intuitively plausible clustering of 57 verbs into 14 classes the automatic clustering was evaluated against independently motivated handconstructed semantic verb classes a series of posthoc cluster analyses explored the influence of specific frames and frame groups on the coherence of the verb classes and supported the tight connection between the syntactic behaviour of the verbs and their lexical meaning components\n",
            "INFO:tensorflow:GENERATED SUMMARY: the subcategorisation frame descriptions were formally evaluated by comparing the automatically generated verb frames against manual definitions the subcategorisation frame descriptions were formally evaluated by comparing the automatically generated verb frames against manual definitions the subcategorisation frame descriptions were formally evaluated by comparing the automatically generated verb frames against manual definitions the subcategorisation frame descriptions were formally evaluated by comparing the automatically generated verb frames against manual definitions the subcategorisation frame descriptions were formally evaluated by comparing the automatically generated verb frames against manual definitions the subcategorisation frame descriptions were formally evaluated by comparing the automatically generated verb frames against\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the model tables , , and are called the __rtable__ , __ntable__ , and ttable , respectively . to see if it was really a corpus problem , we selected a good portion of the corpus and retrained the __rtable__ . the decoding parser is modified to allow inserting __zerofertility__ words , but unlimited insertion easily __blows__ up the memory space . then , we obtain an english parse tree by removing the leaf nodes from the __backreordered__ tree . for example , the best syntactic structure is different if a verb requires a noun phrase as object than it is if it does not . surprisingly , even though the decoded sentence is passable english , the tree structure is totally unnatural . in addition , only limited partofspeech labels are considered to reduce the number of possible __decodedtree__ structures . as we need to parse sentences on the channel input side only , many __xtoenglish__ translation systems can be developed with an english parser alone . the use of an lm needs consideration . the frequency count is essentially a joint probability p , while the tm uses a conditional probability p . using only trigrams obtains the best result for the bleu score . the english word he under the same node is translated into a foreign word kare as seen in the fourth tree . usually a bilingual corpus comes as pairs of translation sentences , so we need to parse the corpus . in our experiment , there are about 4m nonzero entries in the trained table . table 2 shows the result . section 2 briefly reviews the syntaxbased tm , and section 3 describes phrasal translation as an extension . in , the __translationis__ a 1to1 lexical translation from an english __wordto__ a foreign word , i.e. , . to allow non 1to1 translation , such as for idiomatic phrases or compound nouns , we extend the model as follows . we also noticed that too many unary rules were used . in fact , among a total of __138,662__ __reorderexpanded__ rules , the most likely 875 rules contribute 95 % of the probability mass , so discarding the rules which contribute the lower 5 % of the probability mass efficiently eliminates more than 99 % of the total rules . here we need to build an english parse tree from a string of foreign words . the rightmost tree in figure 4 is the decoded tree using the retrained tm . because only a limited number of reorderings are used in actual translation , a small number of rules are highly probable . when it was further relaxed to 100 % , only 20 sentences were decoded . beam search : we give up optimal decoding by using a standard dynamicprogramming parser with beam search , which is similar to the parser used in . figure 1 shows an example . __zerofertility__ words their models are based on mechanisms that generate two languages at the same time , so an english tree is obtained as a __subproduct__ of parsing . , the obtained gain was less than what we expected . this may cause the lm to prefer noun phrases too much . __<s>__ this paper describes a decoding algorithm for a syntaxbased translation model __</s>__ __<s>__ the model has been extended to incorporate phrasal translations as presented here __</s>__ __<s>__ in contrast to a conventional wordtoword statistical model a decoder for the syntaxbased model builds up an english parse tree given a sentence in a foreign language __</s>__ __<s>__ as the model size becomes huge in a practical setting and the decoder considers multiple syntactic structures for each word alignment several pruning techniques are necessary __</s>__ __<s>__ we tested our decoder in a chinesetoenglish translation system and obtained better results than ibm model 4 we also discuss issues concerning the relation between this decoder and a language model __</s>__ __<s>__ this paper describes a decoding algorithm for a syntaxbased translation model __</s>__ __<s>__ the model has been extended to incorporate phrasal translations as presented here __</s>__ __<s>__ in contrast to a conventional wordtoword statistical model a decoder for the syntaxbased model builds up an english parse tree given a sentence in a foreign language __</s>__ __<s>__ as the model size becomes huge in a practical setting and the decoder considers multiple syntactic structures for each word alignment several pruning techniques are necessary __</s>__ __<s>__ we tested our decoder in a chinesetoenglish translation system and obtained better results than ibm model 4 we also discuss issues concerning the relation between this decoder and a language model __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper describes a decoding algorithm for a syntaxbased translation model the model has been extended to incorporate phrasal translations as presented here in contrast to a conventional wordtoword statistical model a decoder for the syntaxbased model builds up an english parse tree given a sentence in a foreign language as the model size becomes huge in a practical setting and the decoder considers multiple syntactic structures for each word alignment several pruning techniques are necessary we tested our decoder in a chinesetoenglish translation system and obtained better results than ibm model 4 we also discuss issues concerning the relation between this decoder and a language model\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is if it does not . surprisingly , even though the decoded , is passable english , the tree structure is totally unnatural . the english word he under the fourth tree . usually a bilingual corpus comes as pairs of translation translation as 4m nonzero entries the trained table . table 2 shows the result . section 2 briefly reviews the syntaxbased tm , there are about 4m nonzero entries the trained table . table 2 shows the result . section 2 briefly reviews the syntaxbased tm , there are about 4m nonzero entries\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  however , quite a few techniques were integrated to construct a chinese language modeling system , and the contribution of using the acm alone was by no means completely investigated . current products make about 510 % errors in conversion of real data in a wide variety of domains . unfortunately , the acm contains four submodels and this makes it difficult to be pruned to a specific size . we __have2__ furthermore , although we may never have seen an example of “ party on __weekday__ tuesday ” , after we backoff or interpolate with a lower order model , we may able to accurately estimate p . all in which case p = p , or __k=14__ , 16 , which is very similar . and tw are the pruning thresholds . using the trigram approximation , we have p p , assuming that the next word depends only on the two preceding words . it 2 thanks to lillian lee for suggesting this justification of predictive clusters . we then performed a series of experiments to investigate the impact of different types of clusters on the acm . gao et al. , presented a fairly thorough empirical study of clustering techniques for asian language modeling . we first constructed several test sets with different backoff __rates4__ . the lm predicts the next word wi given its history h by estimating the conditional probability p . finally , we can also perform iterations of swapping all words between all clusters until convergence the performance is measured in terms of character error rate . it turns out that predictive cluster models achieve the best perplexity results at about __2^6__ or __2^8__ clusters . as the product of the following two probabilities : the acm consists of two submodels : the cluster submodel p , and the word submodel p . another important finding here is that for most of these settings , the unpruned model is actually larger than a normal trigram model – whenever __k=all__ or 14 , 16 , the unpruned model p × p is actually larger than an unpruned model p . japanese kanakanji conversion is the standard method of inputting japanese text by converting a __syllabarybased__ kana string into the appropriate combination of ideographic kanji and kana . in what follows , we will investigate the impact of each of the factors . if we put the word “ tuesday ” into the cluster __weekday__ , we decompose the probability when each word belongs to one class , simple math shows that this decomposition is a strict equality . we used stolcke ’ s pruning method to produce many __acms__ with different model sizes . based on the pilot experiment results using conditional and predictive cluster models , we tried combined cluster models for values le , j , ke . this allows us to relatively quickly search through what would otherwise be an overwhelmingly large search space . the first similar cluster model was presented by goodman and gao in which the clustering techniques were combined with stolcke ’ s pruning to reduce the language model size effectively . __<s>__ the ngram model is a stochastic model which predicts the next word given the previous words in a word sequence __</s>__ __<s>__ the cluster ngram model is a variant of the ngram model in which similar words are classified in the same cluster __</s>__ __<s>__ it has been demonstrated that using different clusters for predicted and conditional words leads to cluster models that are superior to classical cluster models which use the same clusters for both words __</s>__ __<s>__ this is the basis of the asymmetric cluster model discussed in our study __</s>__ __<s>__ in this paper we first present a formal definition of the acm __</s>__ __<s>__ we then describe in detail the methodology of constructing the acm __</s>__ __<s>__ the effectiveness of the acm is evaluated on a realistic application namely japanese kanakanji conversion __</s>__ __<s>__ experimental results show substantial improvements of the acm in comparison with classical cluster models and word ngram models at the same model size __</s>__ __<s>__ our analysis shows that the highperformance of the acm lies in the asymmetry of the model __</s>__ __<s>__ the ngram model is a stochastic model which predicts the next word given the previous words in a word sequence __</s>__ __<s>__ the cluster ngram model is a variant of the ngram model in which similar words are classified in the same cluster __</s>__ __<s>__ it has been demonstrated that using different clusters for predicted and conditional words leads to cluster models that are superior to classical cluster models which use the same clusters for both words __</s>__ __<s>__ this is the basis of the asymmetric cluster model discussed in our study __</s>__ __<s>__ in this paper we first present a formal definition of the acm __</s>__ __<s>__ we then describe in detail the methodology of constructing the acm __</s>__ __<s>__ the effectiveness of the acm is evaluated on a realistic application namely japanese kanakanji conversion __</s>__ __<s>__ experimental results show substantial improvements of the acm in comparison with classical cluster models and word ngram models at the same model size __</s>__ __<s>__ our analysis shows that the highperformance of the acm lies in the asymmetry of the model __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: the ngram model is a stochastic model which predicts the next word given the previous words in a word sequence the cluster ngram model is a variant of the ngram model in which similar words are classified in the same cluster it has been demonstrated that using different clusters for predicted and conditional words leads to cluster models that are superior to classical cluster models which use the same clusters for both words this is the basis of the asymmetric cluster model discussed in our study in this paper we first present a formal definition of the acm we then describe in detail the methodology of constructing the acm the effectiveness of the acm is evaluated on a realistic application namely japanese kanakanji conversion experimental results show substantial improvements of the acm in comparison with classical cluster models and word ngram models at the same model size our analysis shows that the highperformance of the acm lies in the asymmetry of the model\n",
            "INFO:tensorflow:GENERATED SUMMARY: . it turns out that predictive cluster models achieve the best perplexity results at about that predictive cluster models achieve the best perplexity results at about that predictive cluster models achieve the best perplexity results at about that predictive cluster models achieve the best perplexity results at about that predictive cluster models achieve the best perplexity results at about that predictive cluster models achieve the best perplexity results at about about . as the product of the following two probabilities : the acm consists of two submodels\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  by a similar line of reasoning , we would label the relation between __2.a__ and 2.b as evidence . and we extracted automatically 1,000,000 examples of what we hypothesize to be crossdocument nonrelations , by randomly selecting two sentences from distinct documents . current nlp techniques do not enable us to reliably infer from sentence 1.a that “ can not buy arms legally ” and do not give us access to general purpose knowledge bases that assert that “ similar ” . as expected , the __blipp__ corpus yielded much fewer learning cases : we hypothesize that we can determine that a contrast relation holds between the sentences in even if we can not semantically interpret the two sentences , simply because our background knowledge tells us that good and fails are good indicators of contrastive statements . also , since the learning curve for the __blipp__ corpus is steeper than the learning curve for the raw corpus , this suggests that discourse relation classifiers trained on most representative word pairs and millions of training examples can achieve higher levels of performance than classifiers trained on all word pairs . the discourse relation definitions proposed by others are not easier to apply either because they assume the ability to automatically derive , in addition to the semantics of the text spans , the intentions and __illocutions__ associated with them as well . second , given the complexity of the definitions these theories propose , it is clear why it is difficult to build programs that recognize such relations in unrestricted texts . in order to collect training cases , we mined in an unsupervised manner two corpora . in other words , in our approach , we do not distinguish between contrasts of semantic and pragmatic nature , contrasts specific to violated expectations , etc . in lascarides and asher ’ s theory , we would label the relation between __2.a__ and 2.b as explanation because the event in 2.b explains why the event in __2.a__ happened . table 5 displays in general , a word pair can “ signal ” any relation . table 3 shows the performance of all discourse relation classifiers . in reality though , associating a discourse relation with a text span pair is a choice that is clearly influenced by the theoretical framework one is willing to adopt . table 4 summarizes the results . in the same framework , the relation between clauses __2.a__ and 2.b will be labeled as __causalsemanticpositivenonbasic__ . this means that our classifier contributes to an increase in accuracy from to ! ! ! if we had access to robust semantic interpreters , we could , for example , infer from sentence 1.a that “ can not buy arms legally ” , infer from sentence 1.b that “ can buy arms legally ” , use our background knowledge in order to infer that “ similar ” , and apply hobbs ’ s definitions of discourse relations to arrive at the conclusion that a contrast relation holds between the sentences in . __<s>__ we present an unsupervised approach to recognizing discourse relations of contrast __explanationevidence__ condition and elaboration that hold between arbitrary spans of texts __</s>__ __<s>__ we show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93% even when the relations are not explicitly marked by cue phrases __</s>__ __<s>__ we present an unsupervised approach to recognizing discourse relations of contrast __explanationevidence__ condition and elaboration that hold between arbitrary spans of texts __</s>__ __<s>__ we show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93% even when the relations are not explicitly marked by cue phrases __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we present an unsupervised approach to recognizing discourse relations of contrast !!__explanationevidence__!! condition and elaboration that hold between arbitrary spans of texts we show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93% even when the relations are not explicitly marked by cue phrases\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is clear why it is difficult to build programs an unsupervised manner two corpora . also , a similar line of reasoning , we would label the relation between a similar line of reasoning , we would label the relation between a similar line of reasoning , we would label the relation between addition a similar line of reasoning , we would label the relation between 2.b texts . current nlp techniques do not enable us to reliably infer from sentence 1.a that “ 2.b as evidence . second , given the complexity of the definitions these\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  this section lays out a generalized version of ot ’ s theory of production , introducing some notational and representational conventions that may be useful to others and will be important below . since pron is regular , it follows that produce = for example , if x = __abdip__ and z * ’ s from those with k + 1 finitestate ot is a restriction of the formalism discussed above . §6 considers other harmony orderings , a possibility recognized by prince and smolensky . __y¯__ can be made by looking at the non there is an alternative treatment of the lexicon . production under such a grammar is a matter of successive filtering by the constraints c1 , ... in this case smolensky if x contains fewer a ’ s than b ’ s , then produce = . d then for any regular ot grammar , produce = __ib*__ ] . while the formulas above are almost identical , comprehension is in a sense more complex because it varies both the underlying and surface forms . the notation that we have been using so far for candidates is actually misleading , since in fact the candidates y that are compared encode more than just x and z. they also encode a particular alignment or correspondence between x and z. in general , an ot grammar consists of 4 components : a constraint ranking , a harmony ordering , and generating and pronouncing functions . by removing symbols of σ. ellison exploited such properties to give a production algorithm for finitestate * . e __yi−1__ this unconventional formulation is needed for new approaches that care about the exact location of the ? the surface tones indicate the total number of a ’ s and b ’ s in the underlying form , comprehend is actually a finite set in this version , hence regular . y. gen looks up each abstract morpheme ’ s to justify ⇔ we must show when __y¯__ ∈ __¯yi__ that __y¯∈__ h ) as we will see , they identify the problem as the harmony ordering > , rather than the space of constraints or the potential __infinitude__ of the answer set . indeed , for any phonology , it is trivial to design a harmony measure that both production and comprehension optimize . in addition to having the right kind of power linguistically , regular relations are closed under various relevant operations and allow parallel processing of regular sets of strings . for whom that __outfit__ is optimal , i.e. , is at least as __flattering__ as any other __outfit__ z0 in general > may be a partial order : two competing candidates may be equally harmonic or incomparable , and candidates with different underlying forms never compete at all . ot . __aab__ ? 0c ? 0 , inserting a ? but we are really only interested in harmony measures that are defined by __otstyle__ grammars . the construction is inductive . it is the counterpart to gen : just as gen fleshes out x e their empirical adequacy has been __defended__ by eisner . that much was previously shown by markus __hiller__ and paul smolensky , using similar examples . the trouble is that inputs may be arbitrarily long , and so may __accrue__ arbitrarily large numbers of violations . __<s>__ this paper ties up some loose ends in finitestate optimality theory __</s>__ __<s>__ first it discusses how to perform comprehension under optimality theory grammars consisting of finitestate constraints __</s>__ __<s>__ comprehension has not been much studied in ot we show that unlike production it does not always yield a regular set making finitestate methods inapplicable __</s>__ __<s>__ however after giving a suitably flexible presentation of ot we show carefully how to treat comprehension under recent variants of ot in which grammars can be compiled into finitestate transducers __</s>__ __<s>__ we then unify these variants showing that compilation is possible if all components of the grammar are regular relations including the harmony ordering on scored candidates __</s>__ __<s>__ a side benefit of our construction is a far simpler implementation of directional ot __</s>__ __<s>__ this paper ties up some loose ends in finitestate optimality theory __</s>__ __<s>__ first it discusses how to perform comprehension under optimality theory grammars consisting of finitestate constraints __</s>__ __<s>__ comprehension has not been much studied in ot we show that unlike production it does not always yield a regular set making finitestate methods inapplicable __</s>__ __<s>__ however after giving a suitably flexible presentation of ot we show carefully how to treat comprehension under recent variants of ot in which grammars can be compiled into finitestate transducers __</s>__ __<s>__ we then unify these variants showing that compilation is possible if all components of the grammar are regular relations including the harmony ordering on scored candidates __</s>__ __<s>__ a side benefit of our construction is a far simpler implementation of directional ot __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper ties up some loose ends in finitestate optimality theory first it discusses how to perform comprehension under optimality theory grammars consisting of finitestate constraints comprehension has not been much studied in ot we show that unlike production it does not always yield a regular set making finitestate methods inapplicable however after giving a suitably flexible presentation of ot we show carefully how to treat comprehension under recent variants of ot in which grammars can be compiled into finitestate transducers we then unify these variants showing that compilation is possible if all components of the grammar are regular relations including the harmony ordering on scored candidates a side benefit of our construction is a far simpler implementation of directional ot\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a sense more complex varies both the non y that are compared encode more than just x z. they also encode a particular alignment or correspondence between x x the candidates y that are compared encode more than just x z. they also encode a particular alignment or correspondence between x x conventions that may be useful to others will be important below . since pron is a restriction of the formalism discussed above . §6 considers other harmony orderings , a possibility recognized by prince from those with k + 1\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  this section describes the patternmatching algorithm in detail . pattern matching and substitution can be defined more rigorously using tree automata , but for reasons of space these definitions are not given here . the algorithm has two phases . this blocks the matching of shallower patterns , reducing their match values and hence raising their success probability . . it goes to some lengths to handle complex cases such as adjunction and where two or more empty nodes ’ paths cross . let g be the set of such empty node representations derived from the “ gold standard ” evaluation corpus and t this paper presents an algorithm that takes as its input a tree without empty nodes of the kind shown in figure 2 and modifies it by inserting empty nodes and coindexation to produce a the tree shown in figure 1. any indices occuring on nodes in the pattern are systematically __renumbered__ beginning with 1. it can also be regarded as a kind of tree transformation , so the overall system architecture is an instance of the “ __transformdetransform__ ” approach advocated by johnson . the relabelling of auxiliary verbs was performed primarily because charniak ’ s parser produces trees with such labels ; experiments show that auxiliary relabelling has little effect on the algorithm ’ s performance . the entry with pos sbar and no label refers to a “ compound ” type of empty structure labelled sbar consisting of an empty complementizer and an empty s ; a typical example is shown in figure 3. on the other hand , since shallower patterns contain less structure they are likely to match a greater variety of trees than the deeper patterns , they still have ample opportunity to apply . in fact this headbased antecedent representation yields scores very __similiar__ to those obtained using the phrasebased representation . this information is shown under the “ match ” column in table 2 , and is used to filter patterns which would most often be incorrect to apply even though they match . we used sections 2–21 of the penn treebank as the training corpus ; section 24 was used as the development corpus for experimentation and tuning , while the test corpus was used exactly once . table 1 contains summary statistics on the distribution of empty nodes in the penn treebank . the previous section described an algorithm for restoring empty nodes and coindexing their antecedents . rather , they produce trees of the kind shown in figure 2. in fact , the actual implementation of pattern matching and substitution used here is considerably more complex than just described . * we experimented with specialized classifiers for determining if an np the “ label ” column gives the terminal label of the empty node , the “ pos ” column gives its preterminal label and the “ antecedent ” column there are many different possible ways of realizing this intuition , but all of the ones tried gave approximately similar results so we present the simplest one here . if a pattern p matches a tree t , then it is possible to substitute p for the fragment of t that it matches . __<s>__ this paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their coindexed antecedents in phrase structure trees that do not contain this information __</s>__ __<s>__ the patterns are minimal connected tree fragments containing an empty node and all other nodes coindexed with it __</s>__ __<s>__ this paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus __</s>__ __<s>__ evaluating the algorithm on the output of charniak’s parser and the penn treebank shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity __</s>__ __<s>__ this paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their coindexed antecedents in phrase structure trees that do not contain this information __</s>__ __<s>__ the patterns are minimal connected tree fragments containing an empty node and all other nodes coindexed with it __</s>__ __<s>__ this paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus __</s>__ __<s>__ evaluating the algorithm on the output of charniak’s parser and the penn treebank shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their coindexed antecedents in phrase structure trees that do not contain this information the patterns are minimal connected tree fragments containing an empty node and all other nodes coindexed with it this paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus evaluating the algorithm on the output of charniak’s parser and the penn treebank shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity\n",
            "INFO:tensorflow:GENERATED SUMMARY: figure 1. any indices it by inserting empty nodes coindexation to produce a the tree shown figure 1. any indices occuring on nodes the patternmatching algorithm detail it by inserting empty nodes coindexation to produce a the tree shown figure 1. any indices occuring on nodes the patternmatching algorithm detail it by inserting empty nodes coindexation to produce a the tree shown figure 1. any indices occuring on nodes the patternmatching algorithm detail it by inserting empty nodes coindexation to produce a the tree shown figure 1. any indices occuring on nodes the patternmatching algorithm figure 1. any indices occuring\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  an nbest em variant is then employed to jointly reestimate the model parameters such that the ppl on training data is decreased — the likelihood of the training data under our model is increased . in other words , if the two exposed heads bear different information about their parents , they can never be adjoined . the choice among the two schemes is made according to a list of rules based on the identity of the label on the lefthandside of a cf rewrite rule . the same setup was also used in , and . we finally put the decoding results of the two parts together to get the final decoding output . since the slm parses sentences bottomup while the parsers used in , and are topdown , it ’ s not clear how to find a direct correspondence between our schemes of enriching the dependency structure and the ones employed above . an extensive presentation of the slm can be found in . the intermediate nodes created by the above binarization schemes receive the nt label 1. em training can be started . for these reasons , we prefer enriching the syntactic dependencies by information from the left context . one choice is a synchronous multistack search algorithm which is very similar to a beam search . for example , the parent and __opposite+parent__ schemes are worse than baseline in the first iteration when __=0.0__ , the __h2+parent__ and __h2+opposite+parent__ schemes are also worse than h2 scheme in the first iteration when __=0.0__ . as we mentioned in section 3 , the __nt/pos__ vocabularies for the seven models are different because of the enrichment of __nt/pos__ tags . however , the parent and __opposite+parent__ schemes are both worse than the baseline , especially before the em training and with __=0.0__ . the sequence of constructor operations at position grows the wordparse prefix into a wordparse prefix . the results are shown in table 4. since the number of parses for a given word prefix grows exponentially with , , the state space of our model is huge even for relatively short sentences , so we have to use a search strategy that prunes it . the slm , on the other hand , always uses the same dependency structure that is decided beforehand . the results show that an improved parser is indeed helpful in reducing the ppl and wer . because slm training on the __20m__ words of wsj text is very expensive , especially after enriching the __nt/pos__ tags , we only evaluated the wer performance of the seven models with initial statistics from binarized and enriched parse trees . since the wer results in are based on less training data , we do not have a fair comparison between our best model and roark ’ s model . since this restriction of adjoin movement is very tight , pruning may delete some or all the good parsing hypotheses early and the net result may be later development of inadequate parses which lead to poor language modeling and poor parsing performance . __<s>__ we study the impact of richer syntactic dependencies on the performance of the structured language model along three dimensions: parsing accuracy perplexity and worderrorrate __</s>__ __<s>__ we show that our models achieve an improvement in __lp/lr__ ppl and/or wer over the reported baseline results using the slm on the upenn treebank and wall street journal corpora respectively __</s>__ __<s>__ analysis of parsing performance shows correlation between the quality of the parser and the language model performance __</s>__ __<s>__ a remarkable fact is that the enriched slm outperforms the baseline 3gram model in terms of wer by 10% when used in isolation as a second pass language model __</s>__ __<s>__ we study the impact of richer syntactic dependencies on the performance of the structured language model along three dimensions: parsing accuracy perplexity and worderrorrate __</s>__ __<s>__ we show that our models achieve an improvement in __lp/lr__ ppl and/or wer over the reported baseline results using the slm on the upenn treebank and wall street journal corpora respectively __</s>__ __<s>__ analysis of parsing performance shows correlation between the quality of the parser and the language model performance __</s>__ __<s>__ a remarkable fact is that the enriched slm outperforms the baseline 3gram model in terms of wer by 10% when used in isolation as a second pass language model __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we study the impact of richer syntactic dependencies on the performance of the structured language model along three dimensions: parsing accuracy perplexity and worderrorrate we show that our models achieve an improvement in !!__lp/lr__!! ppl and/or wer over the reported baseline results using the slm on the upenn treebank and wall street journal corpora respectively analysis of parsing performance shows correlation between the quality of the parser and the language model performance a remarkable fact is that the enriched slm outperforms the baseline 3gram model in terms of wer by 10% when used in isolation as a second pass language model\n",
            "INFO:tensorflow:GENERATED SUMMARY: schemes are also worse than h2 to on can is increased . the intermediate nodes created by the two exposed heads bear different information about their parents , they the two exposed heads bear different information about their parents , they the two exposed heads bear different information about their parents , they the two exposed heads bear different information about their parents , they the two exposed heads bear different information about their parents , they the two exposed heads bear different information about their parents , they the two exposed heads bear different information about their parents ,\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  x : f = � } , or for that set ’ s characteristic function . for example , consider bruce kaplan , president of __metals__ inc. add a special final round to boost recall , yielding __91.2/80.0/85.2__ for the yarowsky algorithm and __91.3/80.1/85.3__ for their version of the original cotraining algorithm . + , and the area of its upper left quadrant represents pr . = we first derive an expression for the precision of f in terms of g. note that the second line is derived from the first by rule independence . in what follows , f represents an atomic rule under consideration , and g represents the current classifier . i give here an algorithm , the greedy agreement algorithm , that constructs paired rules that agree on unlabeled data , and i examine its performance . in the special case in which rule independence is satisfied , both horizontal and vertical lines are __unbroken__ , as in figure 1. f = + } . = the first assumption is precision independence . precision independence and view independence are distinct assumptions ; neither implies the __other.2__ to sum up , we have refined previous work on the analysis of cotraining , and given a new cotraining algorithm that is theoretically justified and has good empirical performance . by contrast , in the expression p , f is the set of instances for which f i extend this work in two ways . the areas of minority values are marked in figure 1b . x2 such that x1 = x1 and x2 the task is to classify names in text as person , location , or organization . we define the conditional dependence of f and g given y blum and mitchell an atomic rule __f`__ is chosen only if its precision , pr , exceeds at each iteration , each possible extension to one of the rules is considered and scored . one possibility is that minority values are opposite to error values . = + . the empirical investigations described here and below use the data set of . + if f __abstains__ on input x . “ classifier ” is synonymous with “ rule ” . in recent work , prove that a classifier has low generalization error if it agrees on unlabeled data with a second classifier based on a different “ view ” of the data . = __a/nt__ . there is an unlabeled training set containing __89,305__ instances , and a labeled test set containing 289 persons , 186 locations , 402 organizations , and 123 “ other ” , for a total of 1,000 instances . typically , in such a diagram , either the horizontal or vertical line is broken , as in figure 2. a binary rule f can be thought of as the characteristic function of the set of instances { x : the precision of the new classifier is __119/120__ = note that r = 0 __<s>__ this paper refines the analysis of cotraining defines and evaluates a new cotraining algorithm that has theoretical justification gives a theoretical justification for the yarowsky algorithm and shows that cotraining and the yarowsky algorithm are based on different independence assumptions __</s>__ __<s>__ this paper refines the analysis of cotraining defines and evaluates a new cotraining algorithm that has theoretical justification gives a theoretical justification for the yarowsky algorithm and shows that cotraining and the yarowsky algorithm are based on different independence assumptions __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper refines the analysis of cotraining defines and evaluates a new cotraining algorithm that has theoretical justification gives a theoretical justification for the yarowsky algorithm and shows that cotraining and the yarowsky algorithm are based on different independence assumptions\n",
            "INFO:tensorflow:GENERATED SUMMARY: terms of g. note that the second line is derived from the first by rule independence . i give here an algorithm , the greedy agreement algorithm , that constructs paired rules inc. add a special final round to boost recall , yielding for their version of the original cotraining algorithm that the second line is derived from the first by rule independence . i give here an algorithm , the greedy agreement algorithm , that constructs paired rules inc. add a special final round to boost\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  a discussion of results is in section 5. a final capability of xle that increases coverage of the __standardplusfragment__ grammar is a skimming technique . the grammar uses several lexicons and two guessers : one guesser for words recognized by the morphological analyzer but not in the lexicons and one for those not recognized . furthermore , only sentences 2an alternative numerical method would be a combination of iterative scaling techniques with a conditional em algorithm . both the dr and lfg annotations broadly agree in their measure of error reduction . let { __imj=1__ be a set of training data , consisting of pairs of sentences y and partial annotations z , let x be the set of parses for sentence y consistent with annotation z , and let x be the set of all parses produced by the grammar for sentence y. furthermore , let p denote the expectation of function f under distribution when the amount of time or memory spent on a sentence exceeds a __threshhold__ the grammar has 314 rules with regular expression righthand sides which compile into a collection of finitestate machines with a total of __8,759__ states and __19,695__ arcs . section 2 describes the lexicalfunctional grammar , the constraintbased parser , and the robustness techniques employed in this work . according to their dependency relations __scheme.3__ annotating the wsj test set was bootstrapped by parsing the test sentences using the lfg grammar and also checking for consistency with the penn treebank annotation . x ir for i = 1 , ... , n on the set of parses x , and we define discriminative or conditional criteria with respect to the set of grammar parses consistent with the treebank annotations . in this version of the corpus , all wsj labels with sbj are retained and are restricted to phrases corresponding to subj in the lfg grammar ; in addition , it contains np under vp , all lgs tags , all prd tags , vp under vp , sbar , and verb pos tags under vp . this grammar parses the sentence as wellformed chunks specified by the grammar , in particular as ss , nps , pps , and vps . to our knowledge , so far the only direct point of comparison is the parser of carroll et al . furthermore , properties refering to lexical elements based on an auxiliary distribution approach as presented in riezler et al . are included in the model . the effect of the quality of the parses on disambiguation performance can be illustrated by breaking down the fscores according to whether the parser yields full parses , fragment , skimmed , or __skimmed+fragment__ parses for the test sentences . the gradient takes the form : the derivatives in the gradient vector intuitively are again just a difference of two expectations note also that this expression shares many common terms with the likelihood function , suggesting an efficient implementation of the optimization routine . ’ s system , we computed an fscore that does not distinguish different types of dependency relations . the third column reports fscores for examples which receive only __nonfull__ parses , i.e . fragment or skimmed parses or __skimmed+fragment__ parses . __<s>__ we present a stochastic parsing system consisting of a lexicalfunctional grammar a constraintbased parser and a stochastic disambiguation model __</s>__ __<s>__ we report on the results of applying this system to parsing the upenn wall street journal treebank __</s>__ __<s>__ the model combines full and partial parsing techniques to reach full grammar coverage on unseen data __</s>__ __<s>__ the treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models __</s>__ __<s>__ disambiguation performance is evaluated by measuring matches of predicateargument relations on two distinct test sets __</s>__ __<s>__ on a gold standard of manually annotated fstructures for a subset of the wsj treebank this evaluation reaches 79% fscore __</s>__ __<s>__ an evaluation on a gold standard of dependency relations for brown corpus data achieves 76% fscore __</s>__ __<s>__ we present a stochastic parsing system consisting of a lexicalfunctional grammar a constraintbased parser and a stochastic disambiguation model __</s>__ __<s>__ we report on the results of applying this system to parsing the upenn wall street journal treebank __</s>__ __<s>__ the model combines full and partial parsing techniques to reach full grammar coverage on unseen data __</s>__ __<s>__ the treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models __</s>__ __<s>__ disambiguation performance is evaluated by measuring matches of predicateargument relations on two distinct test sets __</s>__ __<s>__ on a gold standard of manually annotated fstructures for a subset of the wsj treebank this evaluation reaches 79% fscore __</s>__ __<s>__ an evaluation on a gold standard of dependency relations for brown corpus data achieves 76% fscore __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we present a stochastic parsing system consisting of a lexicalfunctional grammar a constraintbased parser and a stochastic disambiguation model we report on the results of applying this system to parsing the upenn wall street journal treebank the model combines full and partial parsing techniques to reach full grammar coverage on unseen data the treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models disambiguation performance is evaluated by measuring matches of predicateargument relations on two distinct test sets on a gold standard of manually annotated fstructures for a subset of the wsj treebank this evaluation reaches 79% fscore an evaluation on a gold standard of dependency relations for brown corpus data achieves 76% fscore\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is a skimming technique . the grammar uses several lexicons two guessers : one guesser for words recognized by the morphological analyzer but not is a skimming technique . the grammar uses several lexicons two guessers : one guesser for words recognized by the morphological analyzer but not is a skimming technique . the grammar uses several lexicons two guessers : one guesser for words recognized by the morphological analyzer but not is a skimming technique . the grammar uses several lexicons two guessers : one guesser for words recognized by the morphological analyzer but not\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  a model parameter am , m = 1 , ... , m. the feature functions have then not only a dependence on fj1 and ei1 but also on __zk1__ , ak1 . = a2 = 1. table 1 shows the corpus statistics of this task . this is a very convenient approach to improve the quality of a baseline system . for each of them , if the intended information is conveyed and there are no syntactic errors , the sentence is counted as correct . instead of modeling one probability distribution , we obtain two different knowledge sources that are trained independently . = aj . interestingly , this framework contains as special case the source channel approach if we use and set a1 we even can use both features log pr and log pr , obtaining a more symmetric translation model . as alternative to the sourcechannel approach , we directly model the posterior probability pr . yet , the criterion as it is described in eq . 11 , we use the gis algorithm . we do not observe significant overfitting . : here , we replaced __pˆ�__ by __pˆ�__ . yet , if both decision rules yield the same translation quality , we can use that decision rule which is better suited for efficient search . each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0 . here , pr is the language model of the target language , whereas pr is the translation model . this measure compares the words in the two sentences ignoring the word order . so far , we use the logarithm of the components of a translation model as feature functions . : this approach has been suggested by for a natural language understanding task . the alignment of the alignment templates : the alignment mapping is j → i = aj from source position j to target position here , we omit a detailed description of modeling , training and search , as this is not relevant for the subsequent exposition . obviously , we can perform the same step for translation models with an even richer structure of hidden variables than only the alignment aj1 . the overall architecture of the sourcechannel approach is summarized in figure 1. we are given a source sentence fj1 = f1 , ... , fj , ... , more detailed analysis , subjective judgments by test persons are necessary . to simplify the notation , we shall omit in the following the dependence on the hidden variables of the model . for further details , see . in general , as shown in this figure , there may be additional transformations to make the translation task simpler for the algorithm . • we can use numerous additional features that deal with specific problems of the baseline statistical mt system . ei1 = e1 , ... , ei , ... , ei . to use these three component models in a direct maximum entropy approach , we define three different feature functions for each component of the translation model instead of one feature function for the whole translation model p . 1 perform the following maximization __<s>__ we present a framework for statistical machine translation of natural languages based on direct maximum entropy models which contains the widely used sourcechannel approach as a special case __</s>__ __<s>__ all knowledge sources are treated as feature functions which depend on the source language sentence the target language sentence and possible hidden variables __</s>__ __<s>__ this approach allows a baseline machine translation system to be extended easily by adding new feature functions __</s>__ __<s>__ we show that a baseline statistical machine translation system is significantly improved using this approach __</s>__ __<s>__ we present a framework for statistical machine translation of natural languages based on direct maximum entropy models which contains the widely used sourcechannel approach as a special case __</s>__ __<s>__ all knowledge sources are treated as feature functions which depend on the source language sentence the target language sentence and possible hidden variables __</s>__ __<s>__ this approach allows a baseline machine translation system to be extended easily by adding new feature functions __</s>__ __<s>__ we show that a baseline statistical machine translation system is significantly improved using this approach __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we present a framework for statistical machine translation of natural languages based on direct maximum entropy models which contains the widely used sourcechannel approach as a special case all knowledge sources are treated as feature functions which depend on the source language sentence the target language sentence and possible hidden variables this approach allows a baseline machine translation system to be extended easily by adding new feature functions we show that a baseline statistical machine translation system is significantly improved using this approach\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a the model of eq pr , whereas pr is the translation model . this measure compares the words the model parameter am , m = 1 , ... , m. the feature functions have then not only a dependence on fj1 ei1 but also on model parameter am , m = 1 , ... , m. the feature functions have then not only a dependence on fj1\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the wordbased shallow parser displays an apparently loglinear increase in performance , and surpasses the flatter posbased curve at about 50,000 sentences of training data . to provide __differentlysized__ training sets for learning curve experiments , each training set was also clipped at the following sizes : 100 sentences , 500 , 1000 , 2000 , 5000 , 10,000 , 20,000 and __50,000.__ we review related research in section 5 , and formulate our conclusions in section 6. a more general shortcoming is that the word form of an unknown word often contains useful information that is not available in the present setup . the addition of pos also improves performance . the last paragraph describes results with input consisting of words and pos tags . for the total data set , this yields __1,637,268__ instances , one for each word or punctuation mark . section 4 contains a comparison of the effects with goldstandard and automatically assigned pos . finally , words and pos are combined . as shown in figure 1 , the “ attenuated word + goldstandard pos ” curve starts close to the goldstandard pos curve , attains breakeven with this curve at about 500 sentences , and ends close to but higher than all other curves , including the “ attenuated word ” curve . his chunker works on the basis of pos information alone , whereas the second module , the attacher , also uses lexical information . in our experiments , we used a variant of the ib1 memorybased learner and classifier as implemented in timbl . : nodes in the tree are labeled with a syntactic category and up to four function tags that specify grammatical relations , subtypes of adverbials , discrepancies between syntactic form and syntactic function and notions like topicalization . for creating the posbased task , all words are replaced by the goldstandard pos tags associated with them in the penn treebank . with realistic pos the improvement is much smaller . ’ s vote by the inverse of its distance to the test example . this is clearly __disadvantageous__ and specific to this choice of __alnation__ of words and pos . the first two articles mention that words and pos together perform better than pos alone . in one experiment , it has to be performed on the basis of the “ goldstandard ” , __assumedperfect__ pos taken directly from the training data , the penn treebank , so as to abstract from a particular pos tagger and to provide an upper bound . the types and definitions of chunks are identical to the ones used here . if __tribl__ encounters an unknown word in the test material , it stops already at the decision tree stage and returns the default class without even using the information provided by the context . abney ’ s chunking parser consists of two modules : a chunker and an attacher . in a third , a special encoding of lowfrequency words is used . the yaxis represents f on combined chunking and function assignment . for the chunk part of the code , we adopt the “ inside ” , “ outside ” , and “ between ” encoding originating from . all data was converted to instances as illustrated in table 2.1. comparative experiments with a real pos tagger produce lower results . __<s>__ we describe a case study in which a memorybased learning algorithm is trained to simultaneously chunk sentences and assign grammatical function tags to these chunks __</s>__ __<s>__ we compare the algorithm’s performance on this parsing task with varying training set sizes and different input representations __</s>__ __<s>__ in particular we compare input consisting of words only a variant that includes word form information for lowfrequency words goldstandard pos only and combinations of these __</s>__ __<s>__ the wordbased shallow parser displays an apparently loglinear increase in performance and surpasses the flatter posbased curve at about 50 000 sentences of training data __</s>__ __<s>__ the lowfrequency variant performs even better and the combinations is best __</s>__ __<s>__ comparative experiments with a real pos tagger produce lower results __</s>__ __<s>__ we argue that we might not need an explicit intermediate postagging step for parsing when a sufficient amount of training material is available and word form information is used for lowfrequency words __</s>__ __<s>__ we describe a case study in which a memorybased learning algorithm is trained to simultaneously chunk sentences and assign grammatical function tags to these chunks __</s>__ __<s>__ we compare the algorithm’s performance on this parsing task with varying training set sizes and different input representations __</s>__ __<s>__ in particular we compare input consisting of words only a variant that includes word form information for lowfrequency words goldstandard pos only and combinations of these __</s>__ __<s>__ the wordbased shallow parser displays an apparently loglinear increase in performance and surpasses the flatter posbased curve at about 50 000 sentences of training data __</s>__ __<s>__ the lowfrequency variant performs even better and the combinations is best __</s>__ __<s>__ comparative experiments with a real pos tagger produce lower results __</s>__ __<s>__ we argue that we might not need an explicit intermediate postagging step for parsing when a sufficient amount of training material is available and word form information is used for lowfrequency words __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we describe a case study in which a memorybased learning algorithm is trained to simultaneously chunk sentences and assign grammatical function tags to these chunks we compare the algorithm’s performance on this parsing task with varying training set sizes and different input representations in particular we compare input consisting of words only a variant that includes word form information for lowfrequency words goldstandard pos only and combinations of these the wordbased shallow parser displays an apparently loglinear increase in performance and surpasses the flatter posbased curve at about 50 000 sentences of training data the lowfrequency variant performs even better and the combinations is best comparative experiments with a real pos tagger produce lower results we argue that we might not need an explicit intermediate postagging step for parsing when a sufficient amount of training material is available and word form information is used for lowfrequency words\n",
            "INFO:tensorflow:GENERATED SUMMARY: the wordbased shallow parser displays an apparently loglinear increase , surpasses the wordbased shallow parser displays an apparently loglinear increase , surpasses the flatter posbased curve at about 50,000 of training data . : nodes shallow parser displays an apparently loglinear increase , surpasses the flatter posbased curve at about 50,000 of training data . : nodes shallow parser an apparently loglinear increase , performance , 2000 , 5000 , 10,000 , 20,000\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the next section describes unification grammars and maxwell and kaplan packed representation . the algorithm rests on the observation that so long as features are local to the parse fragments used in the packed representations , the statistics required for parsing and estimation are the kinds of quantities that dynamic programming algorithms for graphical models can perform . that does not involve this enumeration . that is , we require that : finally , a packed representation r represents the set of parses q that are identified by values that satisfy the __nogoods__ , i.e. , q let i < j iff there is a path from mi to mj in this tree . it seems that the maxwell and kaplan packed representation may permit more compact representations than the disjunctive representations used by miyao et al. , but this does not imply that the algorithms proposed here are more efficient . in parsing applications it is important to be able to extract the most probable parse __ˆ__ of string y with respect to a __subg__ . this characterisation omits many details about unification grammars and the algorithm by which the packed representations are actually constructed ; see maxwell iii and kaplan for details . maxwell iii and kaplan describes a parsing algorithm for unificationbased grammars that takes as input a string y and returns a packed representation r then we can calculate the partial derivatives required by the conjugate gradient algorithm as well . miyao and tsujii is the closest related work we know of . this section characterises the properties of unification grammars and the maxwell and kaplan packed parse representations that will be important for what follows . however , the dynamic programming algorithms presented here require the information encoded in properties to be local with respect to the features f used in the packed parse representation . note that except for trivial grammars f and q are infinite . for most linguistically realistic grammars this set is finite , and for moderate sized grammars and training corpora by the same arguments as above , d only contains variables ordered after xi , so vn if f is a function whose domain is x , we say that f depends on the set of variables d = { __xj|ix__ , x ' e x , x while for uniformity we write conditions as functions on the entire vector x , in practice maxwell and kaplan if yi is the yield of the parse __i__ , the conditional likelihood of the parses given their yields is : then the maximum conditional likelihood estimate thus , while properties are required to be local relative to features , we can use the ability of the underlying unification grammar to encode essentially arbitrary nonlocal information in features to introduce properties that also encode nonlocal information . is the subset of h that do not depend on any variables at all .3 because such parsers prune the search space they can not guarantee correct results , unlike the algorithms proposed here . __<s>__ stochastic unificationbased grammars define exponential distributions over the parses generated by a unificationbased grammar __</s>__ __<s>__ existing algorithms for parsing and estimation require the enumeration of all of the parses of a string in order to determine the most likely one or in order to calculate the statistics needed to estimate a grammar from a training corpus __</s>__ __<s>__ this paper describes a graphbased dynamic programming algorithm for calculating these statistics from the packed ubg parse representations of maxwell and kaplan which does not require enumerating all parses __</s>__ __<s>__ like many graphical algorithms the dynamic programming algorithm’s complexity is worstcase exponential but is often polynomial __</s>__ __<s>__ the key observation is that by using maxwell and kaplan packed representations the required statistics can be rewritten as either the max or the sum of a product of functions __</s>__ __<s>__ this is exactly the kind of problem which can be solved by dynamic programming over graphical models __</s>__ __<s>__ stochastic unificationbased grammars define exponential distributions over the parses generated by a unificationbased grammar __</s>__ __<s>__ existing algorithms for parsing and estimation require the enumeration of all of the parses of a string in order to determine the most likely one or in order to calculate the statistics needed to estimate a grammar from a training corpus __</s>__ __<s>__ this paper describes a graphbased dynamic programming algorithm for calculating these statistics from the packed ubg parse representations of maxwell and kaplan which does not require enumerating all parses __</s>__ __<s>__ like many graphical algorithms the dynamic programming algorithm’s complexity is worstcase exponential but is often polynomial __</s>__ __<s>__ the key observation is that by using maxwell and kaplan packed representations the required statistics can be rewritten as either the max or the sum of a product of functions __</s>__ __<s>__ this is exactly the kind of problem which can be solved by dynamic programming over graphical models __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: stochastic unificationbased grammars define exponential distributions over the parses generated by a unificationbased grammar existing algorithms for parsing and estimation require the enumeration of all of the parses of a string in order to determine the most likely one or in order to calculate the statistics needed to estimate a grammar from a training corpus this paper describes a graphbased dynamic programming algorithm for calculating these statistics from the packed ubg parse representations of maxwell and kaplan which does not require enumerating all parses like many graphical algorithms the dynamic programming algorithm’s complexity is worstcase exponential but is often polynomial the key observation is that by using maxwell and kaplan packed representations the required statistics can be rewritten as either the max or the sum of a product of functions this is exactly the kind of problem which can be solved by dynamic programming over graphical models\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is proposed . it we the parse fragments used to be local with respect to the parse fragments used the packed representations , the statistics required for parsing estimation are the kinds of quantities that dynamic programming algorithms for graphical models tree . it seems that the maxwell as features are local to the parse fragments used\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the ranking algorithm rewards most specific concepts first ; for example , a sentence containing “ milan __kucan__ ” has a higher score than a sentence contains only either milan or __kucan__ . each sentence is marked with its publication date and a reference date is inserted after every date expression . mckeown but no system or human scored perfect in grammaticality . we apply a simple sentence filter that only retains the lead 10 sentences . unit __s1.1__ says “ thousands of people are feared dead ” and unit __m2.2__ says “ 3,000 and perhaps ... duc is a new evaluation series supported by nist under tides , to further progress in summarization and enable researchers to participate in largescale experiments . the second baseline , coverage baseline , took the first sentence in the first document , the first sentence in the second document and so on until it had a summary of 50 , 100 , 200 , or 400 words . borrowed from information retrieval research , precision is used to measure how effectively a system generates good summary sentences . it was also used as a baseline in a preliminary multidocument summarization study by marcu and gerber with relatively good results . clusters are formed through strict lexical connection . when it came to the measure for cohesion the results are confusing . we conclude with future directions . neats so far only considers features pertaining to individual sentences . in duc2001 we simply used the first sentence of its document . nist did not define any official performance metric in duc2001 . we present the performance of neats in duc2001 in content and quality measures . neats ’ s performance for averaged pseudo precision equals human ’ s at about 58 % . this example highlights the difficulty of judging the content coverage of system summaries against model summaries and the inadequacy of using recall as defined . are these two equivalent ? figure 1 shows the top 5 concepts with their relevancy scores for the topic “ __slovenia__ __secession__ from __yugoslavia__ ” in the duc2001 test collection . section 3 gives a brief overview of the evaluation procedure used in duc2001 . as we mentioned earlier , nist assessors not only marked the sharing relations among system units and model units , they also indicated the degree of match , i.e. , all , most , some , hardly any , or none . with the individual key concepts available , we proceed to cluster these concepts in order to identify major subtopics within the main topic . we would like to apply some compression techniques or use linguistic units smaller than sentences to improve our retention score . to remedy this problem , we introduce a __buddy__ system to improve cohesion and coherence . therefore , time disambiguation and normalization are very important in multidocument summarization . 6 nist assessors wrote two separate summaries per topic . before we present our results , we describe the corpus and evaluation procedures of the document understanding conference 2001 . this ranking algorithm performs relatively well , but it also results in many ties . a total of 12 systems participated in that task . “ the quake was centered in a remote __mountainous__ area ” . __<s>__ neats is a multidocument summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order __</s>__ __<s>__ neats is among the best performers in the large scale summarization evaluation duc 2001 __</s>__ __<s>__ neats is a multidocument summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order __</s>__ __<s>__ neats is among the best performers in the large scale summarization evaluation duc 2001 __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: neats is a multidocument summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order neats is among the best performers in the large scale summarization evaluation duc 2001\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is marked the such of scored perfect a reference date expression date expression . mckeown but no system or human scored perfect largescale experiments . the second baseline , coverage baseline , took the first document , the first sentence ” has a higher score than a sentence contains only either milan or a sentence contains only either milan or a sentence contains only either milan or a sentence contains only either milan or a sentence contains only either milan or a sentence contains only either milan\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the system will only discuss restaurants that rank highly according to the user ’ s dining preferences , and will only describe attributes of those restaurants the user considers important . the ink stream and m the meaning stream . the __browserand__ componentbased architecture of the multimodal ui facilitated its reuse in a log viewer that reads multimodal log files , replays interactions between the user and system , and allows analysis and annotation of the data . number and type indicate the number of entities in a selection and their type , theatre ) . __mcube__ messages are encoded in xml , providing a general mechanism for message parsing and facilitating logging . we employ the approach proposed in in which the ink meaning lattice is converted to a transducer we also discovered that speech recognition performance was initially hindered by placement of the ‘ __clicktospeak__ ’ button and the recognition feedback box on the bottomright side of the device , leading many users to speak ‘ to ’ this area , rather than toward the microphone on the upper left side . for example , in order to make a comparison between the set of restaurants shown in figure 6 , the text planner first ranks the restaurants within the set according to the predicted ranking of the user model . when multiple selection gestures are present an aggregation technique is employed to overcome the problems with deictic plurals and numerals described in johnston . it starts by zooming in on the first station and then gradually zooms out , graphically presenting each stage of the route along with a series of synchronized tts prompts . the g path in the result is used to __reestablish__ the connection between sem symbols and their specific contents in i : g . after initial openended piloting trials , more structured user tests were conducted , for which we developed a set of six scenarios ordered by increasing level of difficulty . the latticebased finitestate approach to multimodal understanding enables both multimodal integration and dialogue context to compensate for recognition errors . agents may reside either on the client device or elsewhere on the network and can be implemented in multiple different languages . multimodal integrator __mmfst__ receives the speech lattice and the ink meaning lattice and builds a multimodal meaning lattice which captures the potential joint interpretations of the speech and ink inputs . they are free to provide input using speech , by drawing on the display with a stylus , or by using synchronous multimodal combinations of the two modes . when the user draws on the map their ink is captured and any objects potentially selected , such as currently displayed restaurants , are identified . this grammar captures not just multimodal integration patterns but also the parsing of speech and gesture , and the assignment of meaning . the generation process performs templatebased generation for simple responses and updates the system ’ s model of the user ’ s intentions after generation . gesture and handwriting recognition gesture and handwriting recognition agents provide possible classifications of electronic ink for the ui . testing match in nyc this iterative development approach highlighted several important problems early on . __<s>__ mobile interfaces need to allow the user and system to adapt their choice of communication modes according to user preferences the task at hand and the physical and social environment __</s>__ __<s>__ we describe a multimodal application architecture which combines finitestate multimodal language processing a speechact based multimodal dialogue manager dynamic multimodal output generation and usertailored text planning to enable rapid prototyping of multimodal interfaces with flexible input and adaptive output __</s>__ __<s>__ our testbed application match provides a mobile multimodal __speechpen__ interface to restaurant and subway information for new york city __</s>__ __<s>__ mobile interfaces need to allow the user and system to adapt their choice of communication modes according to user preferences the task at hand and the physical and social environment __</s>__ __<s>__ we describe a multimodal application architecture which combines finitestate multimodal language processing a speechact based multimodal dialogue manager dynamic multimodal output generation and usertailored text planning to enable rapid prototyping of multimodal interfaces with flexible input and adaptive output __</s>__ __<s>__ our testbed application match provides a mobile multimodal __speechpen__ interface to restaurant and subway information for new york city __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: mobile interfaces need to allow the user and system to adapt their choice of communication modes according to user preferences the task at hand and the physical and social environment we describe a multimodal application architecture which combines finitestate multimodal language processing a speechact based multimodal dialogue manager dynamic multimodal output generation and usertailored text planning to enable rapid prototyping of multimodal interfaces with flexible input and adaptive output our testbed application match provides a mobile multimodal !!__speechpen__!! interface to restaurant and subway information for new york city\n",
            "INFO:tensorflow:GENERATED SUMMARY: their type , theatre ) . it starts by zooming , providing a will only discuss restaurants that rank highly according to the data . number type type indicate the number of entities a log viewer that reads multimodal log files , replays interactions between the meaning stream . the ink stream architecture m the meaning stream . the ink stream architecture m the meaning stream . the ink stream architecture m the meaning stream . the ink stream architecture m the meaning stream . the ink stream architecture m the meaning stream . the ink stream architecture\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "INFO:tensorflow:We've been decoding with same checkpoint for 61 seconds. Time to load new checkpoint\n",
            "INFO:tensorflow:Loading checkpoint /content/drive/My Drive/MA_colab/PG_Model2/logs/myexperiment/train/model.ckpt-15726\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/MA_colab/PG_Model2/logs/myexperiment/train/model.ckpt-15726\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  for this case , the domaindependent features may have been particularly important , making it difficult to compare the results of this approach to others working on less restricted domains . i or until all the unlabeled data has been labeled . it seems that cotraining is useful in rather specialized constellations only . in addition , this kind of test set approximates the decisions made by a simple resolution algorithm that cause in a realworld setting , information about a pronoun ’ s semantic class obviously is not available prior to its resolution . cardie and wagstaff describe an unsupervised clustering approach to noun phrase coreference resolution in which features are assigned to single noun phrases only . we regard these cases as irrelevant because they do not contribute any knowledge for the classifier . a number of requirements for these views are mentioned in the literature , e.g. , that they have to be disjoint or even conditionally independent ) . in our annotation , coreference is represented in terms of a member attribute on markables . from this set , no instances were removed because no knowledge whatsoever about the data can be assumed in a realistic setting . they only used pairs of anaphors and their closest antecedents as positive examples in training , but evaluated according to vilain et al . . it was criticized that the features used by mccarthy and lehnert are highly idiosyncratic and applicable only to one particular domain . driven by the necessity to provide robust systems for the muc system evaluations , researchers began to look for those features which were particular important for the task of reference resolution . in strube et al . in the literature on reference resolution it is claimed that the antecedent ’ s grammatical function and its realization are important . the med is computed from these editing operations and the length of the potential antecedent m or the length of the anaphor n. cotraining is a metalearning algorithm which exploits unlabeled in addition to labeled training data for classifier learning . we favour this definition because it strengthens the predictive power of the word distance between potential anaphor and potential antecedent . in figure 1 , three curves and three baselines are plotted : for 20 , 20 __0its__ is the baseline , i.e . the initial result obtained by just combining the two initial classifiers . this produced 250 data sets with a total of __92750__ instances of potential antecedentanaphor pairs . then we ran the cotraining experiment with the np form pairs of anaphors and __nonantecedents__ were labeled dn if at least one true antecedent occurred in between . they mention that it was important for the training data to contain transitive positives , i.e. , all possible coreference relations within an anaphoric chain . then we briefly introduce the cotraining paradigm , which is followed by a description of the corpus we use , the corpus annotation , and the way we prepared the data for using a binary classifier in the cotraining algorithm . they distinguish between features which focus on individual noun phrases and features which focus on the anaphoric relation . __<s>__ in this paper we investigate the practical applicability of cotraining for the task of building a classifier for reference resolution __</s>__ __<s>__ we are concerned with the question if cotraining can significantly reduce the amount of manual labeling work and still produce a classifier with an acceptable performance __</s>__ __<s>__ in this paper we investigate the practical applicability of cotraining for the task of building a classifier for reference resolution __</s>__ __<s>__ we are concerned with the question if cotraining can significantly reduce the amount of manual labeling work and still produce a classifier with an acceptable performance __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in this paper we investigate the practical applicability of cotraining for the task of building a classifier for reference resolution we are concerned with the question if cotraining can significantly reduce the amount of manual labeling work and still produce a classifier with an acceptable performance\n",
            "INFO:tensorflow:GENERATED SUMMARY: has been labeled . it seems that cotraining rather specialized constellations only . we regard these only . it or until all the unlabeled data has been labeled . it seems that cotraining rather specialized constellations only . it or until all the unlabeled data has been labeled . it seems that cotraining rather specialized constellations only . it or until all the unlabeled data has been labeled . it seems that cotraining rather specialized constellations only . it or until all the unlabeled data has been labeled . it seems that cotraining\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  as i increases , so does the set ci . let s = we will combine random variables x1 ... thus , if lexical effects are present , we expect the model that uses caching to provide lower entropy estimates . wn } be the test sentence . we claim , however , that the entropy of these random variables is on average the __same2__ . the probability model for sentence s with parse tree t is : where parents are words which are parents of node x in the the tree t. our claim is that the entropy of yi , h stays constant for all i. by the definition of relative mutual information between xi and ci , where the last term is the mutual information between the word and context given the sentence . it has been shown by kuhn and mohri that lexical effects can be easily captured by caching . the random variable we are interested in is yi , a random variable that has the same distribution as __xi|x1__ = w1 , ... , we are only interested in the mean value of the h for wj e si , where si is the ith sentence . wi−1 for some fixed words these causes may be split into two categories : lexical and nonlexical . this suggests that such measurements may be able to pick up more obviously semantic contextual influences than simply the repeating words captured by caching models . w1 ... { w1 ... the first , which we call ci , contains x1 through __xj−1__ , i.e . all the words from the preceding sentences . the same corpus , training and testing sets were used . we have shown that entropy of the sentences taken without context increases with the sentence number , which is in agreement with the above principle . finally we compute the entropy using the estimator described in . the results are reported on figure 1 . the trend is fairly obvious , especially for small sentence numbers : sentences get harder as sentence number increases , i.e . the probability of the sentence given the model decreases . we have as many random variables as we have words in a text . there has been work in the speech community inspired by this constancy rate principle . it is wellknown from information theory that the most efficient way to send information through noisy channels is at a constant rate . the communication medium we examine in this paper is text , and we present some evidence that this principle holds here . entropy is the highest when all values are equally probable , and is lowest when one of the choices has probability of 1 , i.e . deterministically known in advance . these results are interesting in their own right , and may have practical implications as well . let t be our training corpus . since we have proven the presence of the nonlexical effects in the previous experiment , we can see that both lexical and nonlexical effects are present . __<s>__ we present a constancy rate principle governing language generation __</s>__ __<s>__ we show that this principle implies that local measures of entropy should increase with the sentence number __</s>__ __<s>__ we demonstrate that this is indeed the case by measuring entropy in three different ways __</s>__ __<s>__ we also show that this effect has both lexical and nonlexical causes __</s>__ __<s>__ we present a constancy rate principle governing language generation __</s>__ __<s>__ we show that this principle implies that local measures of entropy should increase with the sentence number __</s>__ __<s>__ we demonstrate that this is indeed the case by measuring entropy in three different ways __</s>__ __<s>__ we also show that this effect has both lexical and nonlexical causes __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we present a constancy rate principle governing language generation we show that this principle implies that local measures of entropy should increase with the sentence number we demonstrate that this is indeed the case by measuring entropy in three different ways we also show that this effect has both lexical and nonlexical causes\n",
            "INFO:tensorflow:GENERATED SUMMARY: up more obviously semantic contextual influences than simply the repeating words captured by caching models . w1 ... { w1 ... { w1 ... the first , which we call ci , contains x1 through than simply the repeating words captured by caching models . w1 ... { w1 ... { w1 ... the first , which we call ci , contains x1 through , contains x1 through , contains x1 through , contains x1 through , contains x1 through , contains x1 through , contains x1 through , contains x1 sets were used is : where parents are words\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the __bulm__ derivation can be best explained by an example in figure 1. a sketch of the algorithm is as follows . we define an event as a parse action together with its context . we have __20951__ unlabeled sentences for the active learner to select samples . given a sentence , the existing model could generate the top most likely parses , __eachhaving__ a probability : __whereis__ the possible parse __andis__ its associated score . improving speed of sentence clustering is also worthwhile . gorithm converges . right , as the tag wd is the leftmost child of the constituent s ; and the third action is tagging the second word from given the sentence and the two proceeding actions , and so on and so forth . to this end , a modelbased structural distance is defined to quantify how “ far ” two sentences are apart , and with the help of this distance , the active training set is clustered so that we can define and compute the “ density ” of a sample ; second , we propose and test several entropybased measures to quantify the uncertainty of a sample in the active training set using an existing model , as it makes sense to ask human beings to annotate the portion of data for which the existing model is not doing well . most uncertain per cluster : in our implementation , we cluster the active training set so that the number of clusters equals the batch size . the current model ’ s uncertainty about a sentence could be because similar sentences are underrepresented in the training set , or similar sentences are intrinsically difficult . there is a corresponding vocabulary for tag or label , and there are four extension directions : right , left , up and unique . we have shown in section 2.1 that a parse tree can be represented by a sequence of events , each of which can in turn be represented as bitstrings through answering questions . in our implementation , contexts are internally represented as bitstrings through a set of predesigned questions . this work is partially supported by darpa under __spawar__ contract number __n660019928916__ . the tree can then be represented by a sequence of events , which can be “ poured ” down the grown trees , and the count can be updated accordingly – denote the updated count as . __tering__ and the word entropy . as explained above , longer sentences also have larger sentence entropy . since the space of the entire parses is too large and can not be modeled directly , a parse tree is decomposed as a series of individual actions . as a result , longer sentences would have higher change of entropy , in other words , larger impact on models . the horizontal line on the graph is the performance if all 20k sentences are used . in the __bulm__ derivation , there are three types of parse actions : tag , label and extension . our efforts are devoted to two aspects : first , we believe that the selected samples should reflect the underlying distribution of the training corpus . __<s>__ it is necessary to have a annotated corpus to build a statistical parser __</s>__ __<s>__ acquisition of such a corpus is costly and timeconsuming __</s>__ __<s>__ this paper presents a method to reduce this demand using active learning which selects what samples to annotate instead of annotating blindly the whole training corpus __</s>__ __<s>__ sample selection for annotation is based upon __“representativeness”__ and “usefulness” __</s>__ __<s>__ a modelbased distance is proposed to measure the difference of two sentences and their most likely parse trees __</s>__ __<s>__ based on this distance the active learning process analyzes the sample distribution by clustering and calculates the density of each sample to quantify its representativeness __</s>__ __<s>__ further more a sentence is deemed as useful if the existing model is highly uncertain about its parses where uncertainty is measured by various entropybased scores __</s>__ __<s>__ experiments are carried out in the shallow semantic parser of an air travel dialog system __</s>__ __<s>__ our result shows that for about the same parsing accuracy we only need to annotate a third of the samples as compared to the usual random selection method __</s>__ __<s>__ it is necessary to have a annotated corpus to build a statistical parser __</s>__ __<s>__ acquisition of such a corpus is costly and timeconsuming __</s>__ __<s>__ this paper presents a method to reduce this demand using active learning which selects what samples to annotate instead of annotating blindly the whole training corpus __</s>__ __<s>__ sample selection for annotation is based upon __“representativeness”__ and “usefulness” __</s>__ __<s>__ a modelbased distance is proposed to measure the difference of two sentences and their most likely parse trees __</s>__ __<s>__ based on this distance the active learning process analyzes the sample distribution by clustering and calculates the density of each sample to quantify its representativeness __</s>__ __<s>__ further more a sentence is deemed as useful if the existing model is highly uncertain about its parses where uncertainty is measured by various entropybased scores __</s>__ __<s>__ experiments are carried out in the shallow semantic parser of an air travel dialog system __</s>__ __<s>__ our result shows that for about the same parsing accuracy we only need to annotate a third of the samples as compared to the usual random selection method __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: it is necessary to have a annotated corpus to build a statistical parser acquisition of such a corpus is costly and timeconsuming this paper presents a method to reduce this demand using active learning which selects what samples to annotate instead of annotating blindly the whole training corpus sample selection for annotation is based upon !!__“representativeness”__!! and “usefulness” a modelbased distance is proposed to measure the difference of two sentences and their most likely parse trees based on this distance the active learning process analyzes the sample distribution by clustering and calculates the density of each sample to quantify its representativeness further more a sentence is deemed as useful if the existing model is highly uncertain about its parses where uncertainty is measured by various entropybased scores experiments are carried out in the shallow semantic parser of an air travel dialog system our result shows that for about the same parsing accuracy we only need to annotate a third of the samples as compared to the usual random selection method\n",
            "INFO:tensorflow:GENERATED SUMMARY: on can be best explained by an existing as a parse action together with its context . the current model ’ the “ density ” of a sample ; second , we propose test several entropybased measures to quantify the uncertainty of a sample the active training set using an existing model , as it makes sense to ask human beings to annotate the portion of data for which the existing model is as follows . we define an event as a parse action together with its context . the current model ’ s uncertainty about a sentence could be\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  and we define the forward probabilities as the probability that it will start from and output on the left stream , and on the right stream and be in state , and the backward probabilities as the probability that starting from state it will output on the right and on the left and then terminate , ie end in state . for a parametric model with free parameters , the set of all these models will form a smooth dimensional manifold in the space of all distributions . this is equivalent to the task of finding the most likely string generated by a hmm , which is nphard , but it is possible to sample from the conditional distribution , which allows an efficient stochastic computation . we store all of the observed pairs of strings . __nakisa__ it is a tensor because it transforms properly when the parametrization is changed . the techniques presented here operate directly on sequences of atomic symbols , using a much less articulated representation , and much less input information . given a fst , and a string , we often need to find the string that maximizes . given a generative model for a string , one can use the sufficient statistics of those generative models as features . there are a number of reasons why this simple approach will not work : first , for many languages the inflected form is lexically not phonologically specified and thus the model will not be able to identify the correct form ; secondly , modelling all of the irregular exceptions in a single transduction is computationally intractable at the moment . this algorithm will maximize the joint probability of the training data . the data is classified according to the type of the plural , and is mapped onto a syllabic skeleton , with each phoneme represented as a bundle of phonological features . the second term in equation 2 corresponding to the normalization can be neglected . otherwise we sample repeatedly from the conditional distribution of each of the submodels . the fst then defines a joint probability distribution on pairs of strings from the alphabet . the test data consists of all the remaining verbs . it is possible to modify the normal dynamicprogramming training algorithm for hmms , the baumwelch algorithm to work with fsts as well . this was taken as evidence for the presence of two qualitatively distinct modules in human morphological processing . however these approaches are not a complete solution to the problem of learning morphology , since they do not directly produce the transduction . the training data consists of all of the verbs with a nonzero lemma spoken frequency in the 1.3 million word cobuild corpus . these results are presented in table 5. the curvature of this manifold can be described by a __riemannian__ tensor – this tensor is just the expected fisher information for that model . __ept__ in sampa transcription . __<s>__ this paper discusses the supervised learning of morphology using stochastic transducers trained using the expectationmaximization algorithm __</s>__ __<s>__ two approaches are presented: first using the transducers directly to model the process and secondly using them to define a similarity measure related to the fisher kernel method and then using a memorybased learning technique __</s>__ __<s>__ these are evaluated and compared on data sets from english german slovene and arabic __</s>__ __<s>__ this paper discusses the supervised learning of morphology using stochastic transducers trained using the expectationmaximization algorithm __</s>__ __<s>__ two approaches are presented: first using the transducers directly to model the process and secondly using them to define a similarity measure related to the fisher kernel method and then using a memorybased learning technique __</s>__ __<s>__ these are evaluated and compared on data sets from english german slovene and arabic __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper discusses the supervised learning of morphology using stochastic transducers trained using the expectationmaximization algorithm two approaches are presented: first using the transducers directly to model the process and secondly using them to define a similarity measure related to the fisher kernel method and then using a memorybased learning technique these are evaluated and compared on data sets from english german slovene and arabic\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . the techniques presented here operate directly on sequences of atomic symbols , using a much less for the backward probabilities as the probability that starting from state it will output on the left stream , the probability that starting from state it will output on the left stream , the probability that starting from state it will output on the left stream , the probability that starting from state it will output on the left stream , the probability that starting from state it will output on the left stream , the probability that\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  section 6 describes a way to model the interaction of ellipsis resolution and scope resolution in an underspecified structure . in __udrt__ , scope decisions are modelled as subordination constraints . a __udrs__ is disambiguated by adding subordination constraints . the elliptical clause can also be contained in the source , cf . the indefinite np gets narrow scope under “ before ” . whenever an occurrence of ellipsis is recognized , a counter is incremented . remember that in each process of ellipsis resolution the parallelism module returns a bijective function which expresses the parallelism between labels and discourse referents in source and target . a __udrs__ is a triple consisting of a top label , a set of labelled conditions or discourse referents , and a set of subordination constraints . section 3 formulates the general setup of ellipsis resolution assumed in the rest of the paper . __crouch__ considers __ordersensitivity__ of interpretation a serious drawback . understood in this sense , underspecification often obviates the need for complete disambiguation . in prolog , class membership is most efficiently tested via unification . to bind an anaphor to some antecedent expression , a binding constraint and an equality constraint between two discourse referents are introduced . it also provides a bijective function associating the parallel labels and discourse referents in source and target . to this end , a method to encode “ phantom conditions ” has been presented , i.e . subformulae whose presence depends on the scope configuration . for reasons that will become clear only in section 7 discourse referents also have contexts . john went to the station , and every student did too , on a bike . if the subsequent discourse contains a plural anaphoric np such as “ both solutions ” , two or more discourse referents designating solutions are looked for . but none of the approaches mentioned can ascertain this fact without complete scope resolution . every professor found a solution before most students did , and every assistant did too . the example is problematic for all approaches which assume source and target scope order to be identical . constraints are only evaluated when the underspecified representation is finally interpreted . section 2 gives a short introduction to __udrt__ . erk and koller go on to propose an extension of clls that permits the reading . rather , some “ parallelism ” module is assumed to take care of task 1. consequently , sentence shows that subordination constraints may affect more than one pair of labels . section 8 concludes . erk and koller discuss sentence which has a reading in which each student went to the station on a different bike . the qlf approach gives an interesting answer to this question : it uses reentrancy to propagate scope decisions among parallel structures . another consequence is , however , that the strategy of postponing disambiguation steps is in some cases insufficient . now we can model the parallelism effects by stipulating that a subordination constraint connects two equivalence classes and rather than two individual labels and . if the decisions on scope order have not yet been taken , how can they be guaranteed to be the same in source and target ? __<s>__ the paper presents an approach to ellipsis resolution in a framework of scope underspecification __</s>__ __<s>__ it is argued that the approach improves on previous proposals to integrate ellipsis resolution and scope underspecification in that application processes like anaphora resolution do not require full disambiguation but can work directly on the underspecified representation __</s>__ __<s>__ furthermore it is shown that the approach presented can cope with the examples discussed by dalrymple et al __</s>__ __<s>__ as well as a problem noted recently by erk and koller __</s>__ __<s>__ the paper presents an approach to ellipsis resolution in a framework of scope underspecification __</s>__ __<s>__ it is argued that the approach improves on previous proposals to integrate ellipsis resolution and scope underspecification in that application processes like anaphora resolution do not require full disambiguation but can work directly on the underspecified representation __</s>__ __<s>__ furthermore it is shown that the approach presented can cope with the examples discussed by dalrymple et al __</s>__ __<s>__ as well as a problem noted recently by erk and koller __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: the paper presents an approach to ellipsis resolution in a framework of scope underspecification it is argued that the approach improves on previous proposals to integrate ellipsis resolution and scope underspecification in that application processes like anaphora resolution do not require full disambiguation but can work directly on the underspecified representation furthermore it is shown that the approach presented can cope with the examples discussed by dalrymple et al as well as a problem noted recently by erk and koller\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is disambiguated by adding subordination constraints . the elliptical clause this underspecified . the elliptical clause this sense , underspecification often obviates the need for complete disambiguation . remember that will a way to model the interaction of ellipsis resolution scope resolution an underspecified structure . it also provides a bijective function associating the interaction of ellipsis resolution scope resolution an underspecified structure . it also provides a bijective function associating the interaction of ellipsis resolution scope resolution\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the question term could appear in the documents obtained from the web in various ways . is answered by “ denver ’ s new airport , __topped__ with white __fiberglass__ __cones__ in imitation of the rocky mountains in the background , continues to lie empty ” , because the system picked the answer “ the background ” using the pattern co = the performance of the system depends significantly on there being only one anchor word , which allows a single word match between the question and the candidate answer sentence . using a named entity tagger and/or an ontology would enable the system to use the knowledge that “ background ” is not a location . these techniques are greatly aided by the fact that there is no need to handtag a corpus , while the abundance of data on the web makes it easier to determine reliable statistical estimates . in the first case , the trec corpus was used as the input source and ir was performed by the ir component of our qa system . the rather long list of patterns obtained would have been very difficult for any human to come up with manually . it fails to perform under certain conditions as exemplified by the question “ when was __lyndon__ b. johnson born ? ” . ” we check the presence of the following strings in the answer sentence all the answers need to be __enlisted__ to ensure a high confidence in the precision score of each pattern , in the present framework . as will be seen later , this allows us to differentiate patterns such as d from its more general substring c . the system can not locate the answer in “ london , which has one of the most __busiest__ airports in the world , lies on the banks of the river __thames__ ” due to the explosive danger of unrestricted wildcard matching , as would be required in the pattern “ < question > , * i ) mozart was born in < __any_word__ > ii ) mozart was born in __1756__ calculate the precision of each pattern by the formula p = ca / co since the output from the web contains many correct answers among the top ones , a simple word count could help in eliminating many unlikely answers . “ the < name > in < answer > , ” . a similar result for qa was obtained by brill et al . . retain only the patterns matching a sufficient number of examples . canonicalization of words is also an issue . no external knowledge has been added to these patterns . these may include named entity taggers , wordnet , parsers , handtagged corpora , and ontology lists . our method uses the machine learning technique of bootstrapping to build a large tagged corpus starting with only a few examples of qa pairs . the trec corpus does not have enough candidate answers with high precision score and has to settle for answers extracted from sentences matched by low precision patterns . the apparent power of such patterns surprised many . these questions were run through the testing phase of the algorithm . __<s>__ in this paper we explore the power of surface text patterns for opendomain question answering systems __</s>__ __<s>__ in order to obtain an optimal set of patterns we have developed a method for learning such patterns automatically __</s>__ __<s>__ a tagged corpus is built from the internet in a bootstrapping process by providing a few handcrafted examples of each question type to altavista __</s>__ __<s>__ patterns are then automatically extracted from the returned documents and standardized __</s>__ __<s>__ we calculate the precision of each pattern and the average precision for each question type __</s>__ __<s>__ these patterns are then applied to find answers to new questions __</s>__ __<s>__ using the trec10 question set we report results for two cases: answers determined from the trec10 corpus and from the web __</s>__ __<s>__ in this paper we explore the power of surface text patterns for opendomain question answering systems __</s>__ __<s>__ in order to obtain an optimal set of patterns we have developed a method for learning such patterns automatically __</s>__ __<s>__ a tagged corpus is built from the internet in a bootstrapping process by providing a few handcrafted examples of each question type to altavista __</s>__ __<s>__ patterns are then automatically extracted from the returned documents and standardized __</s>__ __<s>__ we calculate the precision of each pattern and the average precision for each question type __</s>__ __<s>__ these patterns are then applied to find answers to new questions __</s>__ __<s>__ using the trec10 question set we report results for two cases: answers determined from the trec10 corpus and from the web __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in this paper we explore the power of surface text patterns for opendomain question answering systems in order to obtain an optimal set of patterns we have developed a method for learning such patterns automatically a tagged corpus is built from the internet in a bootstrapping process by providing a few handcrafted examples of each question type to altavista patterns are then automatically extracted from the returned documents and standardized we calculate the precision of each pattern and the average precision for each question type these patterns are then applied to find answers to new questions using the trec10 question set we report results for two cases: answers determined from the trec10 corpus and from the web\n",
            "INFO:tensorflow:GENERATED SUMMARY: the question term could appear the documents obtained from the web various various ways . as will be seen later , this allows us to differentiate the documents obtained from the web various ways . is answered by “ denver ’ s new airport the documents obtained from the web various ways . is answered by “ denver ’ s new airport the question term could appear the documents obtained from the web various ways . is answered by “ denver ’ s new airport , topped\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  suppose we have five candidate classes a , b , c , d and e , and the true class of x is b. figure 1 shows the created training examples . morphological analysis is conducted by choosing the most likely path on it . the computational cost in testing is also large , because all the classifiers have to work on each test example . one advantage of revision learning is its small computational cost . for any subpaths from the beginning of the sentence in the lattice , its generative probability can be calculated using hmms . to solve this problem , we propose a revision learning method which combines a model with high generalization capacity and a model with small computational cost to achieve high performance with small computational cost . if the classifier answers the example as incorrect , the next highest ranked class becomes the next candidate for checking . the __oneversusrest__ method is not used because it is not applicable to morphological analysis of nonsegmented languages directly . however , this method has the problem of being computationally costly in training , because the negative examples are created for all the classes other than the true class , and the total number of the training examples becomes large . revision learning uses a binary classifier with higher capacity to revise the errors made by the stochastic model with lower capacity as follows : during the training phase , a ranking is assigned to each class by the stochastic model for a training example , that is , the candidate classes are sorted in descending order of its conditional probability given the example . however , revision learning could not surpass the __oneversusrest__ . the dictionary for hmms is constructed from all the words in the training data . the __oneversusrest__ method is known as one of such methods . many tasks in natural language processing such as pos tagging are regarded as a multiclass classification problem . recently , combinations of multiple learners have been studied to achieve high performance . we assume a binary classifier f that returns positive or negative real value for the class of x , where the absolute value |f | reflects the confidence of the classification . the corpus is randomly separated into training data of __33,831__ sentences and test data of __3,758__ sentences . using linear kernel , the accuracy decreases a little , but the computational cost is much lower than the second order polynomial kernel . we adopt hmms as the stochastic model and svms as the binary classifier . learning models with higher capacity may not be of practical use because of their prohibitive computational cost . then a binary classifier for each class is trained using the examples , and five classifiers are created for this problem . during the test phase , first the ranking of the candidate classes for a given example is assigned by the stochastic model as in the training . __<s>__ this paper presents a revision learning method that achieves high performance with small computational cost by combining a model with high generalization capacity and a model with small computational cost __</s>__ __<s>__ this method uses a high capacity model to revise the output of a small cost model __</s>__ __<s>__ we apply this method to english partofspeech tagging and japanese morphological analysis and show that the method performs well __</s>__ __<s>__ this paper presents a revision learning method that achieves high performance with small computational cost by combining a model with high generalization capacity and a model with small computational cost __</s>__ __<s>__ this method uses a high capacity model to revise the output of a small cost model __</s>__ __<s>__ we apply this method to english partofspeech tagging and japanese morphological analysis and show that the method performs well __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper presents a revision learning method that achieves high performance with small computational cost by combining a model with high generalization capacity and a model with small computational cost this method uses a high capacity model to revise the output of a small cost model we apply this method to english partofspeech tagging and japanese morphological analysis and show that the method performs well\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . information a model applicable to morphological is also large , because the classifiers have to work on each test example . one advantage of revision learning is also large , because all the classifiers have to work on each test example . one advantage of revision learning is its small computational cost . for any subpaths from the beginning of the sentence testing is also large , because all the classifiers have to work on each test example . one advantage of revision learning is its small computational cost . for any subpaths from\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the type can be grammatical or the position of w� in a context window : the relation indicates that the term dog , was the direct object of the verb walk . the next section describes the method of evaluating each thesaurus created by the combination of a given context extraction system and corpus size . is the number of different words aj appears in relations with . since minipar performs morphological analysis on the context relations we have added an existing morphological analyser to the other extractors . to avoid sample bias , the words were randomly selected from wordnet such that they covered a range of values for the following word properties : the larger windows with low correlation between the thesaurus term and context , extract a massive context representation but the results are about 10 % worse than the syntactic extractors . more recently , semantic resources have also been used in collocation discovery , smoothing and model estimation and text classification . the times reported below include the naive bayes pos tagging time . the jaccard measure is the cardinality ratio of the intersection and union of attribute sets is the attribute set for wn ) : the generalised jaccard measure allows each relation to have a significance weight associated with it : where f is the frequency of the relation and n extractors marked with an asterisk , for example w , do not distinguish between different positions of the word w ' in the window . vectorspace thesaurus extraction can be separated into two independent processes . the estimate indicates that minipar will continue to be the best performer on direct matching . the macquarie consists of 812 large topics , each of which is separated into __21174__ small synonym sets . to help overcome the problems of coarsegrained direct comparisons we use three different types of measure to evaluate thesaurus quality : a match is an extracted synonym that appears in the corresponding gold standard synonym list . thesaurus extraction is a good task to use to experiment with scaling context spaces . however , sextant runs about 28 times faster than minipar . but also , thesaurus extraction is a task where success has been limited when using small corpora ; corpora of the order of 300 million words have already been shown to be more successful at this task . we trained a thesaurus extraction system with a range of __contextextracting__ frontends to demonstrate the interaction between context quality , extraction time and representation size . we have demonstrated the behaviour of a simple learning algorithm on much more complicated contextual information on very large corpora . the first thing to note is the time spent extracting contextual information : minipar takes significantly longer to run than the other extractors . for the purposes of evaluation , we selected 70 single word noun terms for thesaurus extraction . secondly , sextant and minipar have quite similar results overall , but minipar is slightly better across most measures . the second step in thesaurus extraction performs clustering or nearestneighbour analysis to determine which terms are similar based on their context vectors . __<s>__ context is used in many nlp systems as an indicator of a term’s syntactic and semantic function __</s>__ __<s>__ the accuracy of the system is dependent on the quality and quantity of contextual information available to describe each term __</s>__ __<s>__ however the quantity variable is no longer fixed by limited corpus resources __</s>__ __<s>__ given fixed training time and computational resources it makes sense for systems to invest time in extracting high quality contextual information from a fixed corpus __</s>__ __<s>__ however with an effectively __limitless__ quantity of text available extraction rate and representation size need to be considered __</s>__ __<s>__ we use thesaurus extraction with a range of context extracting tools to demonstrate the interaction between context quantity time and size on a corpus of 300 million words __</s>__ __<s>__ context is used in many nlp systems as an indicator of a term’s syntactic and semantic function __</s>__ __<s>__ the accuracy of the system is dependent on the quality and quantity of contextual information available to describe each term __</s>__ __<s>__ however the quantity variable is no longer fixed by limited corpus resources __</s>__ __<s>__ given fixed training time and computational resources it makes sense for systems to invest time in extracting high quality contextual information from a fixed corpus __</s>__ __<s>__ however with an effectively __limitless__ quantity of text available extraction rate and representation size need to be considered __</s>__ __<s>__ we use thesaurus extraction with a range of context extracting tools to demonstrate the interaction between context quantity time and size on a corpus of 300 million words __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: context is used in many nlp systems as an indicator of a term’s syntactic and semantic function the accuracy of the system is dependent on the quality and quantity of contextual information available to describe each term however the quantity variable is no longer fixed by limited corpus resources given fixed training time and computational resources it makes sense for systems to invest time in extracting high quality contextual information from a fixed corpus however with an effectively !!__limitless__!! quantity of text available extraction rate and representation size need to be considered we use thesaurus extraction with a range of context extracting tools to demonstrate the interaction between context quantity time and size on a corpus of 300 million words\n",
            "INFO:tensorflow:GENERATED SUMMARY: the type can be grammatical or the position of w� a context window : the relation indicates that the term dog , was the direct object of the verb walk . the times reported below include the naive bayes pos tagging time . the times reported below include the naive bayes pos tagging time . the times reported below include the naive bayes pos tagging time . the times reported below include the naive bayes pos tagging time . the times reported below include the naive bayes pos tagging time . the times reported below include the naive bayes pos\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  something s ; somebody s somebody into __ving__ something . but in some cases the probability is 1.0. in our multilingual applications , the grid information provides a contextbased means of associating a verb with a __levin+__ class according to its usage in the sl sentence . because a word sense was assigned even if only one coder judged it to apply , human coding has been treated as having a precision of 100 % . this probability is the prior probability of specific wordnet verb senses . when the errors of multiple classifiers are not significantly correlated , the result of combining votes from a set of individual classifiers often outperforms the best result from any single classifier . fourth , whereas a single word sense for each token in a text corpus is often assumed , the absence of sentential context leads to a situation where several wordnet senses may be equally appropriate for a database entry . , where is the occurrence of the entire grid for verb entry and table 1 illustrates the relation between __levin+__ classes and wordnet for the verb drop . some of the assignments were in doubt , since class splitting had occurred subsequent to those assignments , with all old wordnet senses carried over to new subclasses . seven semantic relationship types exist between synsets , including , for example , antonymy , hyperonymy , and entailment . 2 gives recall and precision measures for all variations of this voting scheme , both with and without enforcement of the __samesynset__ assumption . much , but not all , of this mapping was accomplished manually . on the one hand , even the higher of the kappa coefficients mentioned above is significantly lower than the standard suggested for good reliability or even the level where tentative conclusions may be drawn , . there are 48 grid classes , with a onetomany relationship between grid and __levin+__ classes . this suggests that the precision of the human coding is approximately 87 % . our mapping operation uses several other data elements pertaining to wordnet : semantic relationships between synsets , frequency data , and syntactic information . voting first occurs within the group , and the group ’ s vote is brought forward with a weight equaling the sum of the group members ’ weights . all words in the lexical database “ text ” are disambiguated , not just a small number for which detailed knowledge is available . where __☆__ __★__ , is an occurrence of tag in semcor and is an occurrence of any of a set of tags for verb in semcor , with being one of the senses possible for verb . extrapolating from this sample to the full set of solo judgments in the database leads to an estimate that approximately __1725__ of those senses are incorrect . no consensus was reached on the remaining 6 instances . this suggests that the grid and __levin+__ probabilities could profitably be dropped from further use . computed values generally range between __.25__ our mapping of verbs in __levin+__ classes to wordnet senses relies in part on the relation between thematic roles in __levin+__ and verb frames in wordnet . values average __<s>__ this paper describes automatic techniques for mapping __9611__ entries in a database of english verbs to wordnet senses __</s>__ __<s>__ the verbs were initially grouped into 491 classes based on syntactic features __</s>__ __<s>__ mapping these verbs into wordnet senses provides a resource that supports disambiguation in multilingual applications such as machine translation and crosslanguage information retrieval __</s>__ __<s>__ our techniques make use of a training set of __1791__ disambiguated entries representing __1442__ verb entries from 167 classes __</s>__ __<s>__ this paper describes automatic techniques for mapping __9611__ entries in a database of english verbs to wordnet senses __</s>__ __<s>__ the verbs were initially grouped into 491 classes based on syntactic features __</s>__ __<s>__ mapping these verbs into wordnet senses provides a resource that supports disambiguation in multilingual applications such as machine translation and crosslanguage information retrieval __</s>__ __<s>__ our techniques make use of a training set of __1791__ disambiguated entries representing __1442__ verb entries from 167 classes __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper describes automatic techniques for mapping !!__9611__!! entries in a database of english verbs to wordnet senses the verbs were initially grouped into 491 classes based on syntactic features mapping these verbs into wordnet senses provides a resource that supports disambiguation in multilingual applications such as machine translation and crosslanguage information retrieval our techniques make use of a training set of !!__1791__!! disambiguated entries representing !!__1442__!! verb entries from 167 classes\n",
            "INFO:tensorflow:GENERATED SUMMARY: s ; somebody s ; somebody s somebody into ; somebody s somebody s somebody s ; somebody s somebody s ; somebody s somebody s ; somebody s somebody s ; somebody s somebody s ; somebody s somebody s ; somebody s somebody s ; somebody s somebody into\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  always guessing the majority class would yield an accuracy of 59 % . to eliminate any possible influence from the testing data , i added “ and ” to every query , which tells altavista not to include the epinions web site in its searches . past work has demonstrated that adjectives are good indicators of subjective , evaluative sentences . a natural question , given the preceding results , is what makes movie reviews hard to classify ? the directionality of a document is determined by its deep argumentative structure , rather than a shallow analysis of its adjectives . the simplicity of __pmiir__ may encourage further work with semantic orientation . table 2 shows an example for a recommended review and table 3 the first step of the algorithm is to extract phrases containing adjectives or adverbs . a phrase has a positive semantic orientation when it has good associations and a negative semantic orientation when it has bad associations . another area for future work is to empirically compare __pmiir__ and the algorithm of hatzivassiloglou and mckeown . although they do not consider the task of classifying reviews , it seems their algorithm could be plugged into the classification algorithm presented in section 2 , where it would replace __pmiir__ and equation in the second step . reviews at epinions are not written by professional writers ; any person with a web browser can become a member of epinions and contribute a review . so is positive when phrase is more strongly associated with “ excellent ” and negative when phrase is more strongly associated with “ poor ” . the second pattern , for example , means that two consecutive words are extracted if the first word is an adverb and the second word is an adjective , but the third word can not be a noun . the latter difficulty might be addressed by using semantic orientation combined with other features in a supervised classification algorithm . the company __mindfuleye7__ offers a technology called __lexanttm__ that appears similar to tong however , i cached all query results , and since there is no need to recalculate hits and hits for every phrase , each phrase requires an average of slightly less than two queries . except for the travel reviews , there is surprisingly little variation in the accuracy within a domain . their algorithm performs well , but it is designed for isolated adjectives , rather than phrases containing adjectives or adverbs . table 5 shows the experimental results . as mentioned in the introduction , one application is to provide summary statistics for search engines . the classification algorithm is evaluated on 410 reviews from __epinions2__ , randomly sampled from four different domains : reviews of automobiles , banks , movies , and travel destinations . the first step is to use a partofspeech tagger to identify phrases in the input text that contain adjectives or adverbs . the log of this ratio is the amount of information that we acquire about the presence of one of the words when we observe the other . in addition to recommended and not recommended , epinions reviews are classified using the five star rating system . __<s>__ this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended or not recommended __</s>__ __<s>__ the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs __</s>__ __<s>__ a phrase has a positive semantic orientation when it has good associations and a negative semantic orientation when it has bad associations __</s>__ __<s>__ in this paper the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word __“excellent”__ minus the mutual information between the given phrase and the word __“poor”__ __</s>__ __<s>__ a review is classified as recommended if the average semantic orientation of its phrases is positive __</s>__ __<s>__ the algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from epinions sampled from four different domains __</s>__ __<s>__ the accuracy ranges from 84% for automobile reviews to 66% for movie reviews __</s>__ __<s>__ this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended or not recommended __</s>__ __<s>__ the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs __</s>__ __<s>__ a phrase has a positive semantic orientation when it has good associations and a negative semantic orientation when it has bad associations __</s>__ __<s>__ in this paper the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word __“excellent”__ minus the mutual information between the given phrase and the word __“poor”__ __</s>__ __<s>__ a review is classified as recommended if the average semantic orientation of its phrases is positive __</s>__ __<s>__ the algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from epinions sampled from four different domains __</s>__ __<s>__ the accuracy ranges from 84% for automobile reviews to 66% for movie reviews __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper presents a simple unsupervised learning algorithm for classifying reviews as recommended or not recommended the classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs a phrase has a positive semantic orientation when it has good associations and a negative semantic orientation when it has bad associations in this paper the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word !!__“excellent”__!! minus the mutual information between the given phrase and the word !!__“poor”__!! a review is classified as recommended if the average semantic orientation of its phrases is positive the algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from epinions sampled from four different domains the accuracy ranges from 84% for automobile reviews to 66% for movie reviews\n",
            "INFO:tensorflow:GENERATED SUMMARY: its searches . past work has demonstrated that adjectives are good indicators of subjective , evaluative are good indicators of subjective , evaluative a negative semantic orientation . table 2 shows an example for tells altavista not to include the epinions web site its searches . past work has demonstrated that adjectives are good indicators of subjective , evaluative a negative semantic orientation . table 2 shows an example for tells altavista not to include the epinions web site its searches . past work has demonstrated that adjectives are good indicators of subjective , evaluative are good indicators of subjective\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  structures the user was attempting . we have undertaken a series of studies to establish an __orderofacquisition__ model for our learner population , native users of american sign language . the model we are developing is called slalom . other data which is more relevant to our goals also presents itself . the theory is that one expects to find errors on elements currently being acquired ; thus errors made by early learners and not by more advanced learners represent structures which the early learners are working on but which the advanced learners have acquired . another problem which additional samples will help to solve is sparseness of data . as icicle is intended to be used by an individual over time and across many pieces of writing , the cycle will be repeated with the same individual many times . in order for icicle to deliver relevant instruction , it needs to determine which of these possibilities most likely reflects the actual performance of the student . we are therefore currently developing a user model to address the system ’ s need to make these parse selections intelligently and to adapt tutoring choices to the individual . we have proposed that slalom be structured in such a way as to capture these expectations by explicitly representing the relationships between grammatical structures in terms of when they are acquired ; namely , indicating which features are typically acquired before other features , and which are typically acquired at the same time . “ she is taught piano on __tuesdays.__ ” although our samples were scored on the __sixpoint__ twe scale , we had sparse data at either end of the scale , so we concentrated our efforts on the three middle levels , which we renamed low , middle , and high . although we experimented in this work with __equalizing__ the error counts using different length measures , we did not have access to the numbers that would have provided the most meaningful normalization : namely , the number of times a structure is attempted . if two of the three levels are both marked , it means that they both committed the error more frequently than the third , but the difference between those two levels was unremarkable . intermediate : missing appropriate __+ing__ morphology . advanced : __botched__ attempt at passive formation . the low and middle levels are insufficiently distinguished from each other , and there were very few errors committed most often by the highest level . the system determines the grammatical errors in the writing , and responds with tutorial feedback aimed at enabling the student to perform corrections . in our first efforts , we have been guided by the observation that the errors committed by learners at different stages of acquisition are clues to the natural order that acquisition follows . all of this begins to draw a picture of the sequence in which these structures are mastered across these levels . a large percentage of the deaf population has reading/writing proficiency levels significantly below their hearing peers , and yet the population represents a broad range of ability . this work has been supported by nsf grants # __ger9354869__ and # __iis9978021__ . __<s>__ in this paper we discuss our approach toward establishing a model of the acquisition of english grammatical structures by users of our english language tutoring system which has been designed for deaf users of american sign language __</s>__ __<s>__ we explore the correlation between a corpus of errortagged texts and their holistic proficiency scores assigned by experts in order to draw initial conclusions about what language errors typically occur at different levels of proficiency in this population __</s>__ __<s>__ since errors made at lower levels presumably represent constructions acquired before those on which errors are found only at higher levels this should provide insight into the order of acquisition of english grammatical forms __</s>__ __<s>__ in this paper we discuss our approach toward establishing a model of the acquisition of english grammatical structures by users of our english language tutoring system which has been designed for deaf users of american sign language __</s>__ __<s>__ we explore the correlation between a corpus of errortagged texts and their holistic proficiency scores assigned by experts in order to draw initial conclusions about what language errors typically occur at different levels of proficiency in this population __</s>__ __<s>__ since errors made at lower levels presumably represent constructions acquired before those on which errors are found only at higher levels this should provide insight into the order of acquisition of english grammatical forms __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in this paper we discuss our approach toward establishing a model of the acquisition of english grammatical structures by users of our english language tutoring system which has been designed for deaf users of american sign language we explore the correlation between a corpus of errortagged texts and their holistic proficiency scores assigned by experts in order to draw initial conclusions about what language errors typically occur at different levels of proficiency in this population since errors made at lower levels presumably represent constructions acquired before those on which errors are found only at higher levels this should provide insight into the order of acquisition of english grammatical forms\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a series elements currently being a series of studies sparse data at either end of the scale , so we concentrated our efforts a way as to capture these expectations by explicitly representing the relationships between grammatical structures across many pieces of writing , the cycle will be repeated with the same individual many\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  in japanese , interword spacing is rarely used . our __rg+dt__ system generates a recognition rule from each ne in the training data . * : __locationname__ , shi if the word is a proper noun or a number , its character type is not replaced by ‘ * ’ . : __locationname__ , if the person ’ s name is composed of only one word , it is classified as __personsingle__ . we generate the word boundary rewriting rules as follows . once the refined rules are generated , we can apply them to a new document . * : __orgname__ > __organization,0,0__ since __yokohama__ honda and kyoto sony also follow this pattern , the second element training time was about 3 minutes on a pentium iii 866 mhz 256 mb memory linux machine . in japanese , there is no such useful hint . the third rule extracts __yokohamaginkou__ as an organization . the __rg+dt__ system attained __84.10__ % for , __84.02__ % for , and __84.03__ % for . accordingly , __shinai__ is classified as __locationend__ . irex ) was held in 1999 , and fifteen systems participated in the formal run of the japanese ne __excercise__ . we use 17 character types for words , e.g. , __singlekanji__ , __allkanji__ , __allkatakana__ , __alluppercase__ , float , __smallinteger__ . * : in this case , we get the following recognition rule . by following uchimoto , we disregard words that appear fewer than five times and other features that appear fewer than three times . and __ioc__ is an unknown word the character type of a oneword ne gives a useful hint for its classification . thus , the last word of an ne is often a head that is more useful than other words for the classification . for instance , we get the following rule when a person ’ s name is incorrectly tagged as a location ’ s name by a pos tagger . accordingly , the decision tree systems did not directly use words as features . first , we compare their starting points and select the earliest ones . by adding inhouse data , the proposed system ’ s performance was improved by several points , while a standard me toolkit crashed . we also replace numbers by dummy constants because most numerical nes follow typical patterns , and their specific values are often useless for ne recognition . the second approach employs a statistical method , which is expected to be more robust and to require less human intervention . now , we obtain the following recognition rules from the above examples . * : __alluppercase__ : __miscpropernoun__ > __organization,0,0__ . for instance , __oosakawan__ follows this pattern , but it is a location ’ s name . table 1 shows the details . it is relatively easy to detect english nes because of capitalization . suffixes of numerical nes are not replaced , either . therefore , we do not restrict proper nouns by a suffix dictionary , and we do not restrict numbers either . ’ s toolkit crashes even on a 2 gb memory machine . recall = for instance , a morphological analyzer may divide a __fourcharacter__ expression __oosakashinai__ into two words __oosaka__ and __shinai__ , but the training data would be tagged as < location > __oosakashi__ < __/location__ > nai . all of the training data were based on the mainichi newspaper ’ s 1994 and 1995 __cdroms__ . __borthwick__ ’ s and uchimoto ’ s __<s>__ named entity recognition is a task in which proper nouns and numerical information in a document are detected and classified into categories such as person organization location and date __</s>__ __<s>__ ne recognition plays an essential role in information extraction systems and question answering systems __</s>__ __<s>__ it is well known that handcrafted systems with a large set of heuristic rules are difficult to maintain and corpusbased statistical approaches are expected to be more robust and require less human intervention __</s>__ __<s>__ several statistical approaches have been reported in the literature __</s>__ __<s>__ in a recent japanese ne workshop a maximum entropy system outperformed decision tree systems and most handcrafted systems __</s>__ __<s>__ here we propose an alternative method based on a simple rule generator and decision tree learning __</s>__ __<s>__ our experiments show that its performance is comparable to the me approach __</s>__ __<s>__ we also found that it can be trained more efficiently with a large set of training data and that it improves readability __</s>__ __<s>__ named entity recognition is a task in which proper nouns and numerical information in a document are detected and classified into categories such as person organization location and date __</s>__ __<s>__ ne recognition plays an essential role in information extraction systems and question answering systems __</s>__ __<s>__ it is well known that handcrafted systems with a large set of heuristic rules are difficult to maintain and corpusbased statistical approaches are expected to be more robust and require less human intervention __</s>__ __<s>__ several statistical approaches have been reported in the literature __</s>__ __<s>__ in a recent japanese ne workshop a maximum entropy system outperformed decision tree systems and most handcrafted systems __</s>__ __<s>__ here we propose an alternative method based on a simple rule generator and decision tree learning __</s>__ __<s>__ our experiments show that its performance is comparable to the me approach __</s>__ __<s>__ we also found that it can be trained more efficiently with a large set of training data and that it improves readability __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: named entity recognition is a task in which proper nouns and numerical information in a document are detected and classified into categories such as person organization location and date ne recognition plays an essential role in information extraction systems and question answering systems it is well known that handcrafted systems with a large set of heuristic rules are difficult to maintain and corpusbased statistical approaches are expected to be more robust and require less human intervention several statistical approaches have been reported in the literature in a recent japanese ne workshop a maximum entropy system outperformed decision tree systems and most handcrafted systems here we propose an alternative method based on a simple rule generator and decision tree learning our experiments show that its performance is comparable to the me approach we also found that it can be trained more efficiently with a large set of training data and that it improves readability\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a generates a recognition rule from each ne the training data . * : shi if the training data . * : shi if the training data . * : shi if the training data . * : shi if the training data . * : shi if the word boundary rewriting rules as follows . once the refined rules are generated , the second element training time was about 3 get the following recognition rule . by following uchimoto , we disregard words that appear fewer than five times . once\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the experimental results reported in this paper are based on a grammar under development at __riacs__ for a spoken dialogue interface to a semiautonomous robot , the personal satellite assistant . the feature structure for each lexical item and grammar rule is rewritten such that singleton variables are unified with a special value ’ any ’ , and every nonsingleton variable expression is embedded in a val term . grammar specification language , which is a form of contextfree grammar in a __bnflike__ notation , with one rule defining each nonterminal , and allowing alternation and kleene closure on the righthandside . as a sideeffect of this compilation , productions are eliminated . as can be readily seen , the compilation time for the k & k algorithm is dramatically lower than the m & g algorithm , while producing a similarly lower recognition performance , measured in both word error rate and recognition speed . additional rules are then introduced to deal with the singleton variable cases . this is in turn converted into a grammar in nuance ’ s this approach has been used in several systems , commandtalk , __rialist__ psa simulator , __witas__ , and __sethivoice__ . if after checking all rule applications bottom up , no new feature structures have been added to , then the least fixedpoint had been found , and the process terminates . for future work we plan to explore possible integrations of these two algorithms . in particular , in order to derive a finite cf grammar , we will need to consider only those features that have a finite number of possible values , or at least consider only finitely many of the possible values for infinitely valued features . n : the computation of the fixedpoint , described in table 1 , proceeds as follows . n : ] the recognition results were obtained on a test set of 250 utterances . np : another technique to reduce ambiguity was motivated by a desire to reduce the amount of prepositional phrase attachment ambiguity in our grammar . note that the approximation need not incorporate all of the features from the original grammar in order to provide a sound approximation . table 2 contains a summary of some key statistics generated using both techniques . a follows b iff now , the follows relation can be used to reduce ambiguity by modifying other productions where then remove the production for . a major problem with grammarbased finitestate or contextfree language models is that they can be tedious to build and difficult to maintain , as they can become quite large very quickly as the scope of the grammar increases . the first step in the instantiation is to unify every combination of daughters with all possible feature structures from . after this transformation , singleton variables will not unify with nonsingleton variable expressions , only with other singletons . since each nonterminal is now ground , it is trivial to assign each nonterminal a unique atomic symbol , and rewrite the grammar as a cfg . while this may sound small , if the grammar were expanded by instantiating variables in all legal permutations , it would contain over contextfree rules . __<s>__ current alternatives for language modeling are statistical techniques based on large amounts of training data and handcrafted contextfree or finitestate grammars that are difficult to build and maintain __</s>__ __<s>__ one way to address the problems of the grammarbased approach is to compile recognition grammars from grammars written in a more expressive formalism __</s>__ __<s>__ while theoretically straightforward the compilation process can exceed memory and time bounds and might not always result in accurate and efficient speech recognition __</s>__ __<s>__ we will describe and evaluate two approaches to this compilation problem __</s>__ __<s>__ we will also describe and evaluate additional techniques to reduce the structural ambiguity of the language model __</s>__ __<s>__ current alternatives for language modeling are statistical techniques based on large amounts of training data and handcrafted contextfree or finitestate grammars that are difficult to build and maintain __</s>__ __<s>__ one way to address the problems of the grammarbased approach is to compile recognition grammars from grammars written in a more expressive formalism __</s>__ __<s>__ while theoretically straightforward the compilation process can exceed memory and time bounds and might not always result in accurate and efficient speech recognition __</s>__ __<s>__ we will describe and evaluate two approaches to this compilation problem __</s>__ __<s>__ we will also describe and evaluate additional techniques to reduce the structural ambiguity of the language model __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: current alternatives for language modeling are statistical techniques based on large amounts of training data and handcrafted contextfree or finitestate grammars that are difficult to build and maintain one way to address the problems of the grammarbased approach is to compile recognition grammars from grammars written in a more expressive formalism while theoretically straightforward the compilation process can exceed memory and time bounds and might not always result in accurate and efficient speech recognition we will describe and evaluate two approaches to this compilation problem we will also describe and evaluate additional techniques to reduce the structural ambiguity of the language model\n",
            "INFO:tensorflow:GENERATED SUMMARY: nuance ’ s this approach has been used or on a grammar under development at a spoken dialogue interface to a semiautonomous robot , the personal satellite assistant . the feature structure for each lexical item a spoken dialogue interface to a semiautonomous robot , the personal satellite assistant . the feature structure for each lexical item a spoken dialogue interface to a semiautonomous robot , the personal satellite assistant . the feature structure for each lexical item a spoken dialogue interface to a semiautonomous robot , the personal satellite assistant . the feature structure for each lexical item grammar\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  in order to simplify the notation we write , and define : , with respect to the distribution . features of ngrams and features of triggers were used in both kinds of models , and the wsme model trained with ps had better performance . the great number of such sentences makes it impossible , from computing perspective , to calculate the sum , even for a moderate __length3__ . in order to define the grammatical features , we first introduce some notation . the corpus was divided into sentences according to the bracketing . the model parameters were completed with the estimation of the global normalization constant . the sum extends over all the sentences of a given length . the goal of the me principle is that , given a set of features , a set of functions and a set of __constraints1__ , we have to find the probability distribution that satisfies the constraints and minimizes the relative entropy where is the normalization constant andare parameters to be found . now , if all the paths collapse at any given time , from that point in time , we are sure that we are sampling from the true target distribution . finally , section 4 presents the experiments carried out using a part of the wall street journal in order __evalute__ the behavior of this proposal . in mcmc , a path of the markov chain is ran for a long time , after which the visited states are considered as a sampling element . the reason for this can be found in the capability of scfgs to model the longterm dependencies established between the different lexical units of a sentence , and the possibility to incorporate the stochastic information that allows for an adequate modeling of the variability phenomena . however , only one of them is here given that the results were very similar . the parameters of the two models are estimated from a training sample . the grammatical information is combined with features of ngrams and triggers . this corpus was automatically labelled and manually checked . and , so , we smooth the model . in section 3 , we define the grammatical features and the way of obtaining them from the scfg . a part of the wall street journal which had been processed in the penn __treebanck__ project was used in the experiments . a formal framework to include longdistance and local information in the same language model is based on the maximum entropy principle . : the first row represents the set of features used . we then considered it appropriate to use ps in the training procedure of the wsme . therefore , a sentence may be a grammatically incorrect sentence , if derivations with low frequency appears . thus if two paths coincide in the same state in time , they will remain in the same states the rest of the time . the training procedure to estimate the parameters of the model is the improved iterative scaling __algorithmn__ . there were two kinds of labelling : postag labelling and syntactic labelling . __<s>__ in this paper we propose adding longterm grammatical information in a whole sentence __maximun__ entropy language model in order to improve the performance of the model __</s>__ __<s>__ the grammatical information was added to the wsme model as features and were obtained from a stochastic contextfree grammar __</s>__ __<s>__ finally experiments using a part of the penn treebank corpus were carried out and significant improvements were __acheived__ __</s>__ __<s>__ where which is usually known as the history of __</s>__ __<s>__ the effort in the language modeling techniques is usually directed to the estimation of __</s>__ __<s>__ the language model defined by the expression is named the conditional language model __</s>__ __<s>__ in principle the determination of the conditional probability in is expensive because the possible number of word sequences is very great __</s>__ __<s>__ traditional conditional language models assume that the probability of the __worddoes__ not depend on the entire history and the history is limited by an equivalence relation and is rewritten as: __</s>__ __<s>__ in this paper we propose adding longterm grammatical information in a whole sentence __maximun__ entropy language model in order to improve the performance of the model __</s>__ __<s>__ the grammatical information was added to the wsme model as features and were obtained from a stochastic contextfree grammar __</s>__ __<s>__ finally experiments using a part of the penn treebank corpus were carried out and significant improvements were __acheived__ __</s>__ __<s>__ where which is usually known as the history of __</s>__ __<s>__ the effort in the language modeling techniques is usually directed to the estimation of __</s>__ __<s>__ the language model defined by the expression is named the conditional language model __</s>__ __<s>__ in principle the determination of the conditional probability in is expensive because the possible number of word sequences is very great __</s>__ __<s>__ traditional conditional language models assume that the probability of the __worddoes__ not depend on the entire history and the history is limited by an equivalence relation and is rewritten as: __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in this paper we propose adding longterm grammatical information in a whole sentence !!__maximun__!! entropy language model in order to improve the performance of the model the grammatical information was added to the wsme model as features and were obtained from a stochastic contextfree grammar finally experiments using a part of the penn treebank corpus were carried out and significant improvements were !!__acheived__!! where which is usually known as the history of the effort in the language modeling techniques is usually directed to the estimation of the language model defined by the expression is named the conditional language model in principle the determination of the conditional probability in is expensive because the possible number of word sequences is very great traditional conditional language models assume that the probability of the !!__worddoes__!! not depend on the entire history and the history is limited by an equivalence relation and is rewritten as:\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a the used of the behavior of the behavior of this proposal to on can is ran the learning of scfgs to model the longterm dependencies established : , with respect to the distribution . features of ngrams features of triggers were used the capability of scfgs to model the longterm dependencies established : , with respect to the distribution . features of ngrams features of triggers were used\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  figure 3 is an example . the functions lhs and rhs map rules to their left hand and right hand sides , respectively . an example is given in figure 4. thus the call to __pfinside__ in line 1 of __pfgovernors__ may involve either a computation of pcfg inside probabilities , or __headlexicalized__ inside probabilities . the governor algorithm annotates parse forest symbols and rules with functions from governor labels to real numbers . and carroll et the algorithm we will define pools governor labels in this way . we believe this is because , for our grammars and corpora , there is limited ambiguity in the position of the head within a given __categoryspan__ combination . is also extended to map trees licensed by the parse forest grammar to trees licensed by the underlying grammar . is defined to be the multiset image of under . in these terms , is the conditional expectation of , conditioned on the yield being . working top down , if fills in an array which is supposed to agree with the quantity defined above . the governor tuple is then replaced by in the definition of the governor label for a terminal vertex . our parser constructs parse forests organized according to span . for a given markup triple , let __termined__ by tree . note that is a vector space , so that expectations and conditional expectations may be defined . the governor labels defined above are derived from the specific symbols of a context free grammar . in this case , just as in the original tree , the label for the fourth terminal position is np , vp , read . given an inside algorithm , the flow may be computed by the flow algorithm in figure 6 , or by the insideoutside algorithm . lopar already provided functions for the computation of the __headmarked__ parse forest , for the flow computation and for traversing the parse forest in depthfirst and __topologicallysorted__ order ) . , the number of tree analyses for a sentence is frequently large , greater than for about 1/10 of the sentences in the british national corpus . the strategy has the advantage , in our view , that it allows one to base markup algorithms on relatively sophisticated grammars , and to take advantage of the lexically sensitive probabilistic weighting of trees which is provided by a lexicalized probability model . see eisner and satta for discussion and an algorithm with time and space requirements proportional to the fourth power of the length of the input sentence in the worst case . the inside algorithm for ordinary pcfgs is given in figure 5. suppose that a probabilistic grammar licenses headed tree analyses for a sentence , and assigns them probabilistic weights . the flow and governor algorithms stated below call an algorithm __pfinside__ which computes inside probabilities in so it was only necessary to add functions for data initialization , for the computation of the governor vector at each node and for printing the result . governor labels for , where is a function giving probability parameters for the underlying grammar . __<s>__ in a headed tree each terminal word can be uniquely labeled with a governing word and grammatical relation __</s>__ __<s>__ this labeling is a summary of a syntactic analysis which eliminates detail reflects aspects of semantics and for some grammatical relations is nearly uncontroversial __</s>__ __<s>__ we define a notion of expected governor markup which sums vectors indexed by governors and scaled by probabilistic tree weights __</s>__ __<s>__ the quantity is computed in a parse forest representation of the set of tree analyses for a given sentence using vector sums and scaling by inside probability and flow __</s>__ __<s>__ in a headed tree each terminal word can be uniquely labeled with a governing word and grammatical relation __</s>__ __<s>__ this labeling is a summary of a syntactic analysis which eliminates detail reflects aspects of semantics and for some grammatical relations is nearly uncontroversial __</s>__ __<s>__ we define a notion of expected governor markup which sums vectors indexed by governors and scaled by probabilistic tree weights __</s>__ __<s>__ the quantity is computed in a parse forest representation of the set of tree analyses for a given sentence using vector sums and scaling by inside probability and flow __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in a headed tree each terminal word can be uniquely labeled with a governing word and grammatical relation this labeling is a summary of a syntactic analysis which eliminates detail reflects aspects of semantics and for some grammatical relations is nearly uncontroversial we define a notion of expected governor markup which sums vectors indexed by governors and scaled by probabilistic tree weights the quantity is computed in a parse forest representation of the set of tree analyses for a given sentence using vector sums and scaling by inside probability and flow\n",
            "INFO:tensorflow:GENERATED SUMMARY: 1 word may involve either a computation of pcfg inside probabilities , or from governor labels to real numbers . the governor algorithm annotates parse forest to their left hand right hand sides , respectively . an example is given figure 4. thus the call to corpora , there is supposed to agree thus the call to corpora , there is supposed to agree thus the call to corpora , there is supposed to agree thus the call to corpora , there is supposed to agree with the quantity defined above . working top down , if fills by the\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  our work differs from traditional qa research in its use of statistical models to predict variables that represent a user ’ s informational goals . precise indicates that an exact answer has been requested , e.g. , a name or date ; additional refers to a level of detail characterized by a __oneparagraph__ answer ; and extended indicates a longer , more detailed answer . a total of __6,436__ questions were tagged by hand . in both cases , the same __1291__ queries were used for testing . figure 2 depicts the average of the results obtained over five runs , while figure 3 shows the results of a single run . these variables represent the topic of discussion , the type of the expected answer , and information that restricts the scope of the answer , respectively . we extracted __97,640__ questions , which constitute about 6 % of the __1,649,404__ queries in the log files collected during a period of three weeks in the year 2000. in section 4 , we discuss our predictive models . further , the predictive accuracy for information need , topic , focus and restriction drops substantially for queries that have 11 words or more . the nlp components of these systems employed handcrafted rules to infer the type of answer expected . the decision trees described in this section are those that yield the best predictive performance . ” is a topic itself query . we extracted a total of 21 structural features , including the number of distinct partsofspeech – nouns , verbs , nps , etc – in a question , whether the main noun is plural or singular , which noun is a proper noun , and the pos of the head verb postmodifier . however , our focus decision tree includes additional attributes in its second split . size of the training set . one approach to the qa task consists of applying the ir methods to retrieve documents relevant to a user ’ s question , and then using the shallow nlp to extract features from both the user ’ s question and the most promising retrieved documents . this difference in performance may be attributed to the fact that the __predictiononly__ model is a “ smoothed ” version of the mixed model . however , since the set of good queries is 10 % smaller , it is considered a better option . these results suggest using modeling techniques which can take advantage of dependencies among target variables . this approach was adopted in . in the next section , we review related research . is an attribute query ; “ how does __lightningform__ ? ” topic , focus and restriction contain a pos in the parse tree of a user ’ s question . the effect of the size of the training set on predictive performance was assessed by considering four sizes of training/test sets : small , medium , large , and __xlarge__ . further , the predictive performance obtained for the set __good4617__ is only slightly better than that obtained for the set __all5145__ . in this paper , we focus on the predictive models , rather than on the provision of answers to users ’ questions . we then evaluate the predictions obtained from models built under different training and modeling conditions . __<s>__ we describe a set of supervised machine learning experiments centering on the construction of statistical models of whquestions __</s>__ __<s>__ these models which are built from shallow linguistic features of questions are employed to predict target variables which represent a user’s informational goals __</s>__ __<s>__ we report on different aspects of the predictive performance of our models including the influence of various training and testing factors on predictive performance and examine the relationships among the target variables __</s>__ __<s>__ we describe a set of supervised machine learning experiments centering on the construction of statistical models of whquestions __</s>__ __<s>__ these models which are built from shallow linguistic features of questions are employed to predict target variables which represent a user’s informational goals __</s>__ __<s>__ we report on different aspects of the predictive performance of our models including the influence of various training and testing factors on predictive performance and examine the relationships among the target variables __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we describe a set of supervised machine learning experiments centering on the construction of statistical models of whquestions these models which are built from shallow linguistic features of questions are employed to predict target variables which represent a user’s informational goals we report on different aspects of the predictive performance of our models including the influence of various training and testing factors on predictive performance and examine the relationships among the target variables\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is proposed from traditional qa research for = from traditional qa research the log files collected during . precise indicates that an exact answer has been requested , e.g. , a name or date ; additional refers to a level of detail characterized by a name or date ; additional refers to a level of detail characterized by a name or date ; additional refers to a level of detail characterized by a name or date ; additional refers to a level of detail characterized by a name or date ; additional refers to a level of\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the next section describes the probability model underlying __incdrop__ and the objective function that results from the model . word tokens . the prior distribution is derived from a __fivestep__ model for the generation of texts . however , it was designed as a computational model of how children segment speech in the course of acquiring their native language , an application that does not permit the use of manually segmented training texts . the test corpus was divided into 16 samples of 1,000 words each so we could assess the variance in performance across samples of a given genre . that maps each position in the text to be generated to the index of the word that will appear in that position . the conditional distribution determines the most probable segmentation of t , according to the model . manually curated linguistic knowledge tends to be more accurate than what can be gleaned by an adaptive learning system , especially when handling relatively rare cases . $ , the reserved sentenceboundary marker . it __bootsraps__ its own dictionary , which is initially empty , using a probability model and viterbistyle optimization algorithm . using these terms , we define precision and recall as follows : true positives recall = this is a deterministic process , so the unique possible outcome has probability 1.0. the __6/72__ term normalizes the sum to one . the first term on the right hand side of is the relative frequency of the word so far , with one added to both the numerator and the denominator . because speech contains no known acoustic marking of word boundaries , children must segment the utterances they hear in order to learn the words of their language . following the procedure used by __teahan__ et al . , we treated each byte as a separate input symbol for both mbdp1 and ppm . considering precision and recall together , it appears that mbdp1 performs much better when the available training corpus is smaller than 212 words , somewhat better when the training corpus is between 212 words and 218 words , and __indistinguishably__ when the training corpus is 220 words . thus , mbdp1 — the algorithm underlying an abstract cognitive model known as __incdrop__ , for incremental distributional regularity optimization — requires neither a dictionary nor a segmented training corpus . this manually segmented corpus is represented in the standard gb coding scheme , which uses two bytes for each chinese character . occasionally , phrases do in fact consist of only one or a few words . all of these methods require either a preexisting dictionary or else a supervised training regimen using a manually segmented corpus . { __t471__ be the resulting lexicon . choose an ordering function s : { 1 , . . . , m } — > { 1 , . , n } the first corpus we used is guo jin 's mandarin chinese ph corpus , containing more than one million words of newspaper stories from the xinhua news agency of pr china written between january , 1990 and march , __1991.__ section 4 reports experiments in which mbdp1 is compared to ppm using the ph corpus of chinese newspaper text and the english portion of the hansard corpus . __<s>__ this paper describes a system for segmenting chinese text into words using the mbdp1 algorithm __</s>__ __<s>__ mbdp1 is a knowledgefree segmentation algorithm that bootstraps its own lexicon which starts out empty __</s>__ __<s>__ experiments on chinese and english corpora show that mbdp1 reliably outperforms the best previous algorithm when the available handsegmented training corpus is small __</s>__ __<s>__ as the size of the handsegmented training corpus grows the performance of mbdp1 converges toward that of the best previous algorithm __</s>__ __<s>__ the fact that mbdp1 can be used with a small corpus is expected to be useful not only for the rare event of adapting to a new language but also for the common event of adapting to a new genre within the same language __</s>__ __<s>__ this paper describes a system for segmenting chinese text into words using the mbdp1 algorithm __</s>__ __<s>__ mbdp1 is a knowledgefree segmentation algorithm that bootstraps its own lexicon which starts out empty __</s>__ __<s>__ experiments on chinese and english corpora show that mbdp1 reliably outperforms the best previous algorithm when the available handsegmented training corpus is small __</s>__ __<s>__ as the size of the handsegmented training corpus grows the performance of mbdp1 converges toward that of the best previous algorithm __</s>__ __<s>__ the fact that mbdp1 can be used with a small corpus is expected to be useful not only for the rare event of adapting to a new language but also for the common event of adapting to a new genre within the same language __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper describes a system for segmenting chinese text into words using the mbdp1 algorithm mbdp1 is a knowledgefree segmentation algorithm that bootstraps its own lexicon which starts out empty experiments on chinese and english corpora show that mbdp1 reliably outperforms the best previous algorithm when the available handsegmented training corpus is small as the size of the handsegmented training corpus grows the performance of mbdp1 converges toward that of the best previous algorithm the fact that mbdp1 can be used with a small corpus is expected to be useful not only for the rare event of adapting to a new language but also for the common event of adapting to a new genre within the same language\n",
            "INFO:tensorflow:GENERATED SUMMARY: the probability model underlying the objective function that results from the model . word tokens . the prior distribution determines the most probable segmentation of how children segment speech the course of acquiring their native language , an application that does not permit the use of manually segmented training texts . the test corpus was divided into 16 samples of 1,000 words each so we could assess the variance performance across samples of a given genre . that maps each position performance across samples of a given genre . that maps each position\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the tagset totalled over 200 tags . this definition entails that l g w and also that a pattern is also a pattern , etc . getting these transcripts is a highly expensive task involving the cooperation and time of nurses and physicians in the busy icu . moreover , as they have fixed length , they tend to be pretty similar . the algorithm proceeds as follows the support of pattern p given a set of sequences s is the number of sequences that contain at least one match of p. furthermore , some tags occur fairly regularly towards either the beginning or the end of the transcript , while others are spread more or less evenly throughout . we can think of these patterns as the basic elements of a plan , representing small clusters of semantic units that are similar in size , for example , to the nucleussatellite pairs of __rst.1__ by learning ordering constraints over these elements , we produce a plan that can be expressed as a constraintsatisfaction problem . constraint confidence . the text was subsequently annotated with semantic tags as shown in figure 1. see figure 4 for an example . we used an adaptation of __teiresias__ . in biological he is __58yearold__ male . generalizing . the last step of our algorithm measures the frequencies of all possible order constraints among pairs of clusters , retaining those that occur often enough to be considered important , according to some relevancy measure . currently , when a patient is brought to the intensive care unit after surgery , one of the residents who was present in the operating room gives a briefing to the icu nurses and residents . as explained in , motif detection is usually targeted with alignment techniques ) or with combinatorial pattern discovery techniques such as the ones we used here . this concept is useful for visualization of the cluster in qualitative evaluation . the resident was equipped with a wearable tape recorder to tape the briefings , which were transcribed to provide the base of our empirical data . given the difficulty for humans in finding patterns systematically in our data , we needed unsupervised techniques such as those developed in computational genomics . we based our unsupervised learning algorithm on techniques used in computational genomics , where from large amounts of seemingly unorganized genetic sequences , patterns representing meaningful biological features are discovered . support . this difference is central to our current research , given that order constraints are our main focus . each of the resulting clusters has a single pattern represented by the centroid of the cluster . these categories are the ones used for our current research . we also discard any constraint that it is violated in any training sequence . in this paper , we focus on learning the plan elements and the ordering constraints between them . * e , where ? in this section , we provide a brief explanation of our pattern discovery methodology . represents a don ’ t care position . they are sets of ordered pairs , where the first position records the sequence number and the second position records the offset in that sequence where p matches . __<s>__ in a language generation system a content planner embodies one or more __“plans”__ that are usually __hand–crafted__ sometimes through manual analysis of target text __</s>__ __<s>__ in this paper we present a system that we developed to automatically learn elements of a plan and the ordering constraints among them __</s>__ __<s>__ as training data we use semantically annotated transcripts of domain experts performing the task our system is designed to mimic __</s>__ __<s>__ given the large degree of variation in the spoken language of the transcripts we developed a novel algorithm to find parallels between transcripts based on techniques used in computational genomics __</s>__ __<s>__ our proposed methodology was evaluated __two–fold:__ the learning and generalization capabilities were quantitatively evaluated using cross validation obtaining a level of accuracy of 89% __</s>__ __<s>__ a qualitative evaluation is also provided __</s>__ __<s>__ in a language generation system a content planner embodies one or more __“plans”__ that are usually __hand–crafted__ sometimes through manual analysis of target text __</s>__ __<s>__ in this paper we present a system that we developed to automatically learn elements of a plan and the ordering constraints among them __</s>__ __<s>__ as training data we use semantically annotated transcripts of domain experts performing the task our system is designed to mimic __</s>__ __<s>__ given the large degree of variation in the spoken language of the transcripts we developed a novel algorithm to find parallels between transcripts based on techniques used in computational genomics __</s>__ __<s>__ our proposed methodology was evaluated __two–fold:__ the learning and generalization capabilities were quantitatively evaluated using cross validation obtaining a level of accuracy of 89% __</s>__ __<s>__ a qualitative evaluation is also provided __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in a language generation system a content planner embodies one or more !!__“plans”__!! that are usually !!__hand–crafted__!! sometimes through manual analysis of target text in this paper we present a system that we developed to automatically learn elements of a plan and the ordering constraints among them as training data we use semantically annotated transcripts of domain experts performing the task our system is designed to mimic given the large degree of variation in the spoken language of the transcripts we developed a novel algorithm to find parallels between transcripts based on techniques used in computational genomics our proposed methodology was evaluated !!__two–fold:__!! the learning and generalization capabilities were quantitatively evaluated using cross validation obtaining a level of accuracy of 89% a qualitative evaluation is also provided\n",
            "INFO:tensorflow:GENERATED SUMMARY: the tagset totalled over 200 tags . this definition entails that l g w the busy icu . moreover , as they have fixed length , they tend to be pretty similar . the algorithm proceeds as follows the support of pattern p given a set of sequences s is the number of sequences that contain at least one match of p. furthermore , some tags occur fairly regularly towards either the busy icu . moreover , as they have fixed length , they tend to be pretty similar . the algorithm proceeds as follows the support of pattern p\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the hierarchy in figure 1.1 , __caroll__ , minnen , and briscoe . document , thereby providing means to associate information at any level of detail or complexity to the annotated structure . the categories are organized in a hierarchy , from general to specific . again , the provision of a standard set of categories , together with the requirement that __schemespecific__ categories at the highest level of abstraction , syntactic annotation schemes represent the following kinds of information : for example , the annotation in figure 1 , drawn from the penn treebank __ii3__ , uses lisplike list structures to specify constituency relations and provide syntactic category labels for constituents . note that in this example , relations are encoded only when they appear explicitly in the original annotation an xslt script could be used to create a second xml document that includes the relations implicit in the embedding . figure 3 shows the overall architecture of the xces framework for syntactic annotation . however , especially for existing formats , it is typically more straightforward to perform the twostep process . structural skeleton : a domaindependent abstract structural framework for syntactic 5 cf . in practice , annotators and users of annotated corpora will rarely see xml and rdf instantiations of annotated data ; rather , they will access the data via interfaces that automatically generate , interpret , and display the data in easytoread formats . dialect specification : defines , using xml schemas , xslt scripts , and xsl style sheets , the projectspecific xml format for syntactic annotations . we accomplish this by treating the description of any specific syntactic annotation scheme as a process involving several knowledge sources that interact at various levels . it has also been noted that that the parseval __bracketprecision__ measure penalizes parsers that return more structure than exists in the relatively flat treebank structures , even if they are correct . figure 4 shows the annotation from the ptb rendered in the abstract xml format . __etc.5__ note that rdf descriptions function much like class definitions in an objectoriented programming language the hierarchy of < struct > elements corresponds to the nodes in a phrase structure analysis ; each < struct conversely , the grammar rules implicit in annotated treebanks , which are typically not annotated according to a formal grammar , can be easily extracted from the abstract structural encoding . > element is typed accordingly . the framework has been applied to the representation of terminology and computational lexicons , thus demonstrating its general applicability for a variety of linguistic annotation types . 4 socalled hybrid systems combine constituency analysis and functional dependencies , usually producing a shallow constituent parse that brackets major phrase types and identifying the dependencies between heads of constituents . it is widely recognized that the proliferation of annotation schemes runs counter to the need to reuse language resources , and that standards for linguistic annotation are becoming increasingly mandatory . to answer this need , we have developed a representation framework comprised of an abstract model for a variety of different annotation types , which can be instantiated in different ways depending on the annotator __<s>__ it is widely recognized that the proliferation of annotation schemes runs counter to the need to reuse language resources and that standards for linguistic annotation are becoming increasingly mandatory __</s>__ __<s>__ to answer this need we have developed a representation framework comprised of an abstract model for a variety of different annotation types which can be instantiated in different ways depending on the annotator __❑s__ approach and goals __</s>__ __<s>__ in this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation __</s>__ __<s>__ we show how the framework can contribute to comparative evaluation and merging of parser output and diverse syntactic annotation schemes __</s>__ __<s>__ it is widely recognized that the proliferation of annotation schemes runs counter to the need to reuse language resources and that standards for linguistic annotation are becoming increasingly mandatory __</s>__ __<s>__ to answer this need we have developed a representation framework comprised of an abstract model for a variety of different annotation types which can be instantiated in different ways depending on the annotator __❑s__ approach and goals __</s>__ __<s>__ in this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation __</s>__ __<s>__ we show how the framework can contribute to comparative evaluation and merging of parser output and diverse syntactic annotation schemes __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: it is widely recognized that the proliferation of annotation schemes runs counter to the need to reuse language resources and that standards for linguistic annotation are becoming increasingly mandatory to answer this need we have developed a representation framework comprised of an abstract model for a variety of different annotation types which can be instantiated in different ways depending on the annotator !!__❑s__!! approach and goals in this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation we show how the framework can contribute to comparative evaluation and merging of parser output and diverse syntactic annotation schemes\n",
            "INFO:tensorflow:GENERATED SUMMARY: 1 , drawn from the penn treebank , thereby providing means to associate information at any level of detail or complexity to the annotated structure . the categories are organized figure 1 , drawn from the penn treebank , thereby providing means to associate information at any level of detail or complexity to the annotated structure . the categories are organized\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  see or for a detailed discussion of this translation model and a description of its parameters . how did this all come about ? we extracted only tuples in which the english and french phrases contained at least two words . tables 4 and 5 show that the translation memories significantly help the decoder find translations of high probability . we decided to use such harsh evaluation criteria because , in previous experiments , we repeatedly found that harsh criteria can be applied consistently . correct we selected only “ contiguous ” alignments , i.e. , alignments in which the words in the english phrase generated only words in the french phrase and each word in the french phrase was generated either by the null word or a word from the english phrase . it is clear that ebmt and smt systems have different strengths and weaknesses . to do this , one would simply have to train the statistical model on the translation memory provided as input , determine the viterbi alignments , and enhance the existing translation memory with wordlevel alignments as produced by the statistical translation model . for example , “ autres __r´egions__ de le pays que ” and “ other parts of canada than ” were judged as incorrect . each english word __eis__ then translated with probability t __einto__ a french word , where ranges from 1 to the number of words into which __eis__ translated . for example , in translating the french sentence “ bien __entendu__ , il __parle__ de une __belle__ __victoire__ . ” , the greedy decoder initially assumes that a good translation of it is “ well heard , it talking a beautiful victory ” because the best translation of “ bien ” is “ well ” , the best translation of “ __entendu__ ” is “ heard ” , and so on . we tried out two distinct methods for choosing a translation equivalent , thus constructing two different probabilistic __tmems__ : the frequencybased translation memory was created by associating with each french phrase the english equivalent that occurred most often in the collection of phrases that we extracted . if the input sentence is found “ as is ” in the translation memory , its translation is simply returned and there is no further processing . ibm model 4 revolves around the notion of word alignment over a pair of sentences . for example , “ this is rather provision disturbing ” was judged as a correct semantical translation of “ __voil`a__ une disposition __plotˆot__ __inqui´etante__ ” , but “ this disposal is rather disturbing ” was judged as incorrect . correct ? if the meaning was just a little different , the translation was considered semantically incorrect . the probability of a given alignment a and target sentence f given a source sentence e is given by d e d null where the factors delineated by symbols correspond to hypothetical steps in the following generative process : each english word __eis__ assigned with probability n e a fertility , which corresponds to the number of french words into which e is going to be translated . no __<s>__ we present a set of algorithms that enable us to translate natural language sentences by exploiting both a translation memory and a statisticalbased translation model __</s>__ __<s>__ our results show that an automatically derived translation memory can be used within a statistical framework to often find translations of higher probability than those found using solely a statistical model __</s>__ __<s>__ the translations produced using both the translation memory and the statistical model are significantly better than translations produced by two commercial systems: our hybrid system translated perfectly 58% of the 505 sentences in a test collection while the commercial systems translated perfectly only __4042%__ of them __</s>__ __<s>__ we present a set of algorithms that enable us to translate natural language sentences by exploiting both a translation memory and a statisticalbased translation model __</s>__ __<s>__ our results show that an automatically derived translation memory can be used within a statistical framework to often find translations of higher probability than those found using solely a statistical model __</s>__ __<s>__ the translations produced using both the translation memory and the statistical model are significantly better than translations produced by two commercial systems: our hybrid system translated perfectly 58% of the 505 sentences in a test collection while the commercial systems translated perfectly only __4042%__ of them __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we present a set of algorithms that enable us to translate natural language sentences by exploiting both a translation memory and a statisticalbased translation model our results show that an automatically derived translation memory can be used within a statistical framework to often find translations of higher probability than those found using solely a statistical model the translations produced using both the translation memory and the statistical model are significantly better than translations produced by two commercial systems: our hybrid system translated perfectly 58% of the 505 sentences in a test collection while the commercial systems translated perfectly only !!__4042%__!! of them\n",
            "INFO:tensorflow:GENERATED SUMMARY: , il de une . ” , the greedy decoder initially assumes that a good translation of it is “ well heard , it talking a beautiful victory ” because the best translation of “ bien ” is “ well ” , the english we extracted the english phrase generated only words\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  fˆ : ’ s solution by having only one abstract constant s together with the following type assignment : in other terms , for any lexicon l : , where the atomic strings ‘ a ’ , ‘ b ’ , and ‘ c ’ are declared to be constants of type . this allows lexicons to be sequentially composed . this __proofsearch__ relies on linear higherorder matching , which is a decidable problem . __−◦__ s ) condition 3 , in the above definition of a lexicon , is necessary and sufficient to ensure that the homomorphisms induced by a lexicon commute with the typing relations . ay . indeed , two regular __acgs__ that shares the same abstract language correspond to a regular language homomorphism composed with a regular language inverse homomorphism . σ2 = , a lexicon l from __σ1__ to σ2 is defined to be a pair l this implies that , contrarily to the usual categorial approaches , word order constraints can not be expressed at the logical level . a __|∃u__ ∈ a . from a practical point of view , this means that the sequential composition of two lexicons may be compiled . the object language of this second __acg__ is defined as follows : this allows the __acg__ __g13__ to be defined as __he1__ , e3 , __l13__ , si . they present a deductive approach in which linear logic is used as a glue language for assembling meanings . this is mandatory for the applicative paradigm of section 3. the solution is well known : it suffices to represent strings of symbols as compositions of functions . this type assignment obeys an inference system whose judgements are __sequents__ of the following form : we now introduce the abstract notions of a vocabulary and a lexicon , on which the central notion of an abstract categorial grammar is based . the additive connectives of linear logic ‘ & ’ and ‘ ® ’ corresponds respectively to the cartesian product and the disjoint union . in other words , we must show how to encode strings as linear __aterms__ . given two vocabularies __σ1__ = and it allows __aterms__ to be used at a syntactic level , which is an approach that has been advocated by . = __he1__ , e2 , __l12__ , si . consider an arbitrary atomic type * , and define the type ‘ string ’ to be . s 7→ * t that extends f. similarly this is only a matter of choice . first of all , we must explain how __acgs__ may manipulate strings of symbols . as we will see this apparent loss in expressive power is compensated by the first point . consequently , we have two distinct entries in the semantic lexicon , one for each possible reading . the syntactic lexicon , however , would be more involved , with entries such as : s 7→ the __exponentials__ of linear logic are modal operators that may be used to go beyond linearity . 2 definition of a multiplicative kernel in this section , we define an elementary grammatical formalism based on the ideas presented in the introduction . x ∧ __tryto__ john and indeed , each new logical connective may be interpreted , through the __curryhoward__ isomorphism , as anew type constructor . __<s>__ we introduce a new categorial formalism based on intuitionistic linear logic __</s>__ __<s>__ this formalism which derives from current typelogical grammars is abstract in the sense that both syntax and semantics are handled by the same set of primitives __</s>__ __<s>__ as a consequence the formalism is reversible and provides different computational paradigms that may be freely composed together __</s>__ __<s>__ we introduce a new categorial formalism based on intuitionistic linear logic __</s>__ __<s>__ this formalism which derives from current typelogical grammars is abstract in the sense that both syntax and semantics are handled by the same set of primitives __</s>__ __<s>__ as a consequence the formalism is reversible and provides different computational paradigms that may be freely composed together __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: we introduce a new categorial formalism based on intuitionistic linear logic this formalism which derives from current typelogical grammars is abstract in the sense that both syntax and semantics are handled by the same set of primitives as a consequence the formalism is reversible and provides different computational paradigms that may be freely composed together\n",
            "INFO:tensorflow:GENERATED SUMMARY: fˆ : ’ s solution by having only one abstract constant s together with the following type assignment : other terms , for any lexicon l : , where the atomic strings ‘ a ’ , ‘ b ’ , other terms , for any lexicon l : , where the atomic strings ‘ a ’ , ‘ b ’ , other terms , for any lexicon l : , where the atomic strings ‘ a ’ , ‘ b ’ , other terms , for any lexicon l : , where the atomic strings ‘ a ’ , ‘\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the probability of each rule is assigned by observing how often each rule was used in the training corpus , yielding a probabilistic contextfree grammar . moreover , the advantage of tt is exploited : a simple estimation procedure , and a definite analysis of a given phoneme string . therefore , we split the training corpus into 9 corpora , where the size of the corpora increases logarithmically from 4500 to 2.1 million words . the contextfree grammar fragment in figure 3 describes a so called training grammar with brackets . he reported a generalisation error for words of __2.22__ % on english data . figure 4 shows the distribution of the number of syllables in the test corpus ranked by the number of syllables , which is a decreasing function . there are no intermediate levels between the syllable and the phonemes . however , only those analyses are considered that meet the tagged brackets . the unambiguous analysis of the input word with the syllable structure grammar is shown in figure 1. the rules depict that the syllables comprise different phoneme strings . daelemans and van den bosch report a 96 % accuracy on finding syllable boundaries for dutch with a backpropagation learning algorithm . in section 4 we describe the grammars and experiments for german data . the probability of a rule is inferred by an iterative training procedure with an extended version of the insideoutside algorithm . grammar transformation . this means that the consonants in the onset and in the coda are assigned different weights . our approach offers a machine learning algorithm for predicting syllable boundaries . that provides syllable boundaries , our socalled treebank . the test corpus without syllable boundaries , is processed by a parser ) and the probabilistic contextfree grammars sustaining the most probable parse of each word . we use a part of a german newspaper corpus , the __stuttgarter__ zeitung , consisting of 3 million words which are divided into 9/10 training and 1/10 test corpus . a be the nonterminal that r expands , then the probability assigned to r is given by we then transform the pcfg by dropping the brackets in the rules resulting in an analysis grammar . the input of the system is a phoneme string without brackets . in section 5 the evaluation of the system is described . the grammar differentiate between monosyllabic words , syllables that occur in inital , medial , and final position . in one of her experiments , the standard probability model was applied to a syllabification task , yielding about 89.9 % accuracy . the remainder of the paper is organized as follows . a tts system needs a module where the words converted from graphemes to phonemes are syllabified before they can be further processed to speech . in a first step , we look up the words and their syllabification in a pronunciation dictionary . these restrictions are language specific ; e.g. , the phoneme sequence is quite frequent in english codas but it never appears in english onsets . when the corpus size is doubled , the accuracy of the treebank grammar is still 1.5 % below the positional syllable structure grammar . __<s>__ an approach to automatic detection of syllable boundaries is presented __</s>__ __<s>__ we demonstrate the use of several manually constructed grammars trained with a novel algorithm combining the advantages of treebank and bracketed corpora training __</s>__ __<s>__ we investigate the effect of the training corpus size on the performance of our system __</s>__ __<s>__ the evaluation shows that a handwritten grammar performs better on finding syllable boundaries than does a treebank grammar __</s>__ __<s>__ an approach to automatic detection of syllable boundaries is presented __</s>__ __<s>__ we demonstrate the use of several manually constructed grammars trained with a novel algorithm combining the advantages of treebank and bracketed corpora training __</s>__ __<s>__ we investigate the effect of the training corpus size on the performance of our system __</s>__ __<s>__ the evaluation shows that a handwritten grammar performs better on finding syllable boundaries than does a treebank grammar __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: an approach to automatic detection of syllable boundaries is presented we demonstrate the use of several manually constructed grammars trained with a novel algorithm combining the advantages of treebank and bracketed corpora training we investigate the effect of the training corpus size on the performance of our system the evaluation shows that a handwritten grammar performs better on finding syllable boundaries than does a treebank grammar\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is assigned by observing how often each rule was used the training corpus , yielding a probabilistic contextfree grammar . moreover , the advantage of tt the probability corpus , yielding a probabilistic contextfree grammar . moreover , the advantage of tt the probability corpus , yielding a probabilistic contextfree grammar . moreover , the advantage of tt is assigned by observing how often each rule was used\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  text appearing wordforword to express the same information ; in our problem , we consider how much newspaper text can be maximally covered by words from pa copy . these texts cover 265 different stories from july 1999 to june 2000 and all newspaper stories have been manually classified at the documentlevel . __+63__ a naive bayes classifier was used because of its success in previous classification tasks , however we are aware of its naive assumptions that attributes are assumed independent and data to be normally distributed . figure 1 shows the result of gst for the example in section 2 . 772 of these texts are pa copy and __944__ from the nine newspapers . for example consider the following : original a __drinkdriver__ who ran into the queen mother 's official daimler was fined $ 700 and banned from driving for two years . in other words we measure the proportion of unique ngrams in b that are found in a. we have concentrated on just three : ngram overlap measures , greedy string tiling , and sentence alignment . at the lexical or word sequence level , individual words and phrases within a newspaper story are classified as to whether they are used to express the same information as words in news agency copy and or used to express information not found in agency copy . here cognates are defined as pairs of terms that are identical , share the same stems , or are substitutable in the given context . table 1 shows the results of the single ternary classifiers . if no such candidate is found , the dt sentence is assumed to be independent of the st. based on individual dt sentence alignments , the overall possibility of derivation for the dt is estimated with a score ranging between 0 and 1. at the document level , newspaper stories are assigned to one of three possible categories coarsely reflecting the amount of text reused from the news agency and the dependency of the newspaper story upon news agency copy for the provision of & ; facts & ; . these results were ordered according to each set to create the same 10 datasets for each approach thereby enabling comparison . they are : 1 ) __whollyderived__ : all text in the newspaper article is rewritten only from news agency copy ; 2 ) __partiallyderived__ : some text is derived from the news agency , but other sources have also been used ; and 3 ) nonderived : news agency has not been used as the source of the article ; although words may still cooccur between the newspaper article and news agency copy on the same topic , the journalist is confident the news agency has not been used . note the high misclassification of pd as both wd and nd , reflecting the difficulty of separating this class . often __newsworkers__ , such as the reporter and editor , will rely upon news agency copy as the basis of a news story or to verify facts and assess the importance of a story in the context of all those appearing on the newswire . __<s>__ in this paper we present results from the meter project whose aim is to explore issues pertaining to text reuse and derivation especially in the context of newspapers using newswire sources __</s>__ __<s>__ although the reuse of text by journalists has been studied in linguistics we are not aware of any investigation using existing computational methods for this particular task __</s>__ __<s>__ we investigate the classification of newspaper articles according to their degree of dependence upon or derivation from a newswire source using a simple 3level scheme designed by journalists __</s>__ __<s>__ three approaches to measuring text similarity are considered: ngram overlap greedy string tiling and sentence alignment __</s>__ __<s>__ measured against a manually annotated corpus of source and derived news text we show that a combined classifier with features automatically selected performs best overall for the ternary classification achieving an average flmeasure score of 0 664 across all three categories __</s>__ __<s>__ in this paper we present results from the meter project whose aim is to explore issues pertaining to text reuse and derivation especially in the context of newspapers using newswire sources __</s>__ __<s>__ although the reuse of text by journalists has been studied in linguistics we are not aware of any investigation using existing computational methods for this particular task __</s>__ __<s>__ we investigate the classification of newspaper articles according to their degree of dependence upon or derivation from a newswire source using a simple 3level scheme designed by journalists __</s>__ __<s>__ three approaches to measuring text similarity are considered: ngram overlap greedy string tiling and sentence alignment __</s>__ __<s>__ measured against a manually annotated corpus of source and derived news text we show that a combined classifier with features automatically selected performs best overall for the ternary classification achieving an average flmeasure score of 0 664 across all three categories __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in this paper we present results from the meter project whose aim is to explore issues pertaining to text reuse and derivation especially in the context of newspapers using newswire sources although the reuse of text by journalists has been studied in linguistics we are not aware of any investigation using existing computational methods for this particular task we investigate the classification of newspaper articles according to their degree of dependence upon or derivation from a newswire source using a simple 3level scheme designed by journalists three approaches to measuring text similarity are considered: ngram overlap greedy string tiling and sentence alignment measured against a manually annotated corpus of source and derived news text we show that a combined classifier with features automatically selected performs best overall for the ternary classification achieving an average flmeasure score of 0 664 across all three categories\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a the same information ; we express the same information ; this the same information ; this the same information ; this the same information ; the appearing wordforword to express the same information ; the appearing wordforword to express the same information ; our problem , we consider how much newspaper text can be maximally covered by words from pa copy . these texts cover 265 different stories from july 1999 to be normally distributed . figure 1 shows the result found wordforword to express the same information ;\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the remaining word pairs are assigned a probability by backoff . since these measures take into account the probability distribution over all competing words within the decoder , they are , hopefully , better correlated with error rate , and expected to evaluate lms more precisely than perplexity . the last column of table 1 shows the relative model sizes with respect to the probabilitybased pruned model with the cer 13.8 % . probability serves as the baseline pruning criterion . finally , we described our techniques of finding the optimal setting of the threshold pair given a specific model size . first , pilot experiments show that the results of wordbased and characterbased models are qualitatively very similar . thus the most likely word has the rank of one , and the least likely has rank |v| , where |v |is the vocabulary size . in this paper , three pruning criteria will be studied : probability , rank , and entropy . the corresponding loss function __lfentropy__ is defined as where h is the entropy before pruning given history wi1 , h ’ we now investigate methods of combining pruning criteria described above . to deal with this problem , katz proposed a backoff scheme . our results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately . the goal of the pruning criterion is to estimate the performance loss due to pruning each bigram individually . our experiments show that rank , which has the best correlation with cer , achieves the best performance when there is only one criterion used in bigram model pruning . n is conceptually identical to p in equation . this is a similar problem to speech recognition except that it does not include acoustic ambiguity . finally , all these optimal threshold settings serve as the sample data , from which the optimal function can be learned . we can see that cer decreases as we keep more and more bigrams in the model . the bigram probabilities are estimated from the training data by maximum likelihood estimation . we performed testing on one subset containing 80 % of the test set . the results using count cutoff pruning . we also show that the highperformance of rank lies in its strong correlation with error rate . mle leads to __zerovalue__ probabilities for unseen bigrams . a steeper curve indicates a better pruning criterion . we notice that __lfprobability__ of equation is very similar to that proposed by __seymore__ and rosenfeld , where the loss function is here n the rank of the target word w is defined as the word ’ s position in an ordered list of the bigram probabilities p where w∈ v , and v is the vocabulary . uncommon explicit bigram estimates p from the model to reduce the number of parameters while minimizing the performance loss . these measures are based on the probability distribution over the whole vocabulary . for comparison , we also include in figure 2 we then built a very large number of pruned bigram models using different threshold setting , where the values __<s>__ reducing language model size is a critical issue when applying a lm to realistic applications which have memory constraints __</s>__ __<s>__ in this paper three measures are studied for the purpose of lm pruning __</s>__ __<s>__ they are probability rank and entropy __</s>__ __<s>__ we evaluated the performance of the three pruning criteria in a real application of chinese text input in terms of character error rate __</s>__ __<s>__ we first present an empirical comparison showing that rank performs the best in most cases __</s>__ __<s>__ we also show that the highperformance of rank lies in its strong correlation with error rate __</s>__ __<s>__ we then present a novel method of combining two criteria in model pruning __</s>__ __<s>__ experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately at the same cer __</s>__ __<s>__ reducing language model size is a critical issue when applying a lm to realistic applications which have memory constraints __</s>__ __<s>__ in this paper three measures are studied for the purpose of lm pruning __</s>__ __<s>__ they are probability rank and entropy __</s>__ __<s>__ we evaluated the performance of the three pruning criteria in a real application of chinese text input in terms of character error rate __</s>__ __<s>__ we first present an empirical comparison showing that rank performs the best in most cases __</s>__ __<s>__ we also show that the highperformance of rank lies in its strong correlation with error rate __</s>__ __<s>__ we then present a novel method of combining two criteria in model pruning __</s>__ __<s>__ experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately at the same cer __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: reducing language model size is a critical issue when applying a lm to realistic applications which have memory constraints in this paper three measures are studied for the purpose of lm pruning they are probability rank and entropy we evaluated the performance of the three pruning criteria in a real application of chinese text input in terms of character error rate we first present an empirical comparison showing that rank performs the best in most cases we also show that the highperformance of rank lies in its strong correlation with error rate we then present a novel method of combining two criteria in model pruning experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately at the same cer\n",
            "INFO:tensorflow:GENERATED SUMMARY: model pruning . n show that the cer 13.8 % . probability serves as the baseline pruning criterion . finally , we described our techniques of finding the optimal setting of the threshold pair given a specific model size . first , pilot experiments show that the cer 13.8 % . probability serves as the baseline pruning criterion . finally , we described our techniques of finding the optimal setting of the threshold pair given a specific model size . first , pilot experiments show that the cer 13.8 % . probability serves as the baseline pruning criterion . finally\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  the layout of this paper is as follows . for example , in the case of nondisjoint feature classes such as __containsdigitandalpha__ and __containsdigitanddash__ , the former will take precedence . and/or word string itself while the second is the external evidence gathered from its context . in order to resolve the sparseness problem , two levels of backoff modeling are applied to approximate 1 ) first level backoff scheme is based on different contexts of word features and words themselves , and n in this section , we will report the experimental results of our system for english ner on muc6 and muc7 ne shared tasks , as shown in table 6 , and then for the impact of training data size on performance using muc7 training data . the rest of the features distinguish types of capitalization and all other words such as punctuation marks . previous approaches have typically used manually constructed finite state patterns , which attempt to match against a sequence of words in much the same way as a general regular expression matcher . = f moreover , our system even outperforms any published rulebased systems . the intuition behind this is the phenomena of name alias . the job of our generative model is to directly generate the original ne tags from the output words of the noisy channel . however , the performance of a machinelearning system is always poorer than that of a rulebased one by about 2 % . the main reason may be due to its better ability of capturing the locality of phenomena , which indicates names in text . this set of triggers is collected semiautomatically from the nes and their local context of the training data . the result is shown in figure 2 for muc7 ne task . this may be because current machinelearning approaches capture important evidence behind ner problem much less effectively than human experts who handcraft the rules , although machinelearning approaches always provide important statistical information that is not available to human experts . the first eleven features arise from the need to distinguish and annotate monetary amounts , percentages , times and dates . the approach behind our ner system is based on the hmmbased chunk tagger in text chunking , which was ranked the best individual system in __conll'2000__ . < fi , wi > . here , the precision measures the number of correct nes in the answer file over the total number of nes in the answer file and the recall measures the number of correct nes in the answer file over the total number of nes in the key file while fmeasure is the weighted harmonic mean of precision and recall : with __β2__ the internal subfeatures are found within the word and/or word string itself to capture internal evidence while external subfeatures are derived within the context to capture external evidence . there has been a considerable amount of work on ner problem , which aims to address many of these ambiguity , robustness and portability issues . __<s>__ this paper proposes a hidden markov model and an hmmbased chunk tagger from which a named entity recognition system is built to recognize and classify names times and numerical quantities __</s>__ __<s>__ through the hmm our system is able to apply and integrate four types of internal and external __evidences:__ 1) simple deterministic internal feature of the words such as capitalization and __digitalization__ 2) internal semantic feature of important triggers 3) internal gazetteer feature 4) external macro context feature __</s>__ __<s>__ in this way the ner problem can be resolved effectively __</s>__ __<s>__ evaluation of our system on muc6 and muc7 english ne tasks achieves fmeasures of 96 6% and 94 1% respectively __</s>__ __<s>__ it shows that the performance is significantly better than reported by any other machinelearning system __</s>__ __<s>__ moreover the performance is even consistently better than those based on handcrafted rules __</s>__ __<s>__ this paper proposes a hidden markov model and an hmmbased chunk tagger from which a named entity recognition system is built to recognize and classify names times and numerical quantities __</s>__ __<s>__ through the hmm our system is able to apply and integrate four types of internal and external __evidences:__ 1) simple deterministic internal feature of the words such as capitalization and __digitalization__ 2) internal semantic feature of important triggers 3) internal gazetteer feature 4) external macro context feature __</s>__ __<s>__ in this way the ner problem can be resolved effectively __</s>__ __<s>__ evaluation of our system on muc6 and muc7 english ne tasks achieves fmeasures of 96 6% and 94 1% respectively __</s>__ __<s>__ it shows that the performance is significantly better than reported by any other machinelearning system __</s>__ __<s>__ moreover the performance is even consistently better than those based on handcrafted rules __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper proposes a hidden markov model and an hmmbased chunk tagger from which a named entity recognition system is built to recognize and classify names times and numerical quantities through the hmm our system is able to apply and integrate four types of internal and external !!__evidences:__!! 1) simple deterministic internal feature of the words such as capitalization and !!__digitalization__!! 2) internal semantic feature of important triggers 3) internal gazetteer feature 4) external macro context feature in this way the ner problem can be resolved effectively evaluation of our system on muc6 and muc7 english ne tasks achieves fmeasures of 96 6% and 94 1% respectively it shows that the performance is significantly better than reported by any other machinelearning system moreover the performance is even consistently better than those based on handcrafted rules\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a the sparseness problem , two levels of backoff modeling are applied to approximate 1 ) the sparseness problem , two levels of backoff modeling are applied to approximate 1 ) the sparseness problem , two levels of backoff modeling are applied to approximate 1 ) first level backoff scheme is as follows . for example , of nondisjoint feature classes the case of nondisjoint feature classes such as the former will take precedence . and/or word string itself\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  now we say that the realization problem of a grammar g is to decide for a given input semantics 5 and an index the npcompleteness result for the realization problem explains immediately why all existing complete generation algorithms have exponential runtimes in the worst case . now let ’ s say we want to find the realizations of the input semantics s = the trees are unordered , i.e . there is no intrinsic order among the children of a node . it should be possible to either encode these constructions directly in the dependency grammar , or filter out wrong realizations in a postprocessing step . an alternative approach is to push the partitioning process into the parser as well . thus it is sometimes even possible to finish parsing without committing to single lexical entries for some nodes . furthermore , we find that our proof brings out the sources of the complexity more clearly . since completely free order is what we want for the realization problem , we do not need these mechanisms and do not go into them here . let a be the label the results of this paper open up a new perspective from which such restrictions can be sought , especially considering that all the naturallanguage examples we tried are indeed processed in polynomial time . we also assign a semantics to every elementary tree , so that lexical entries are pairs of the form , where co is a multiset of semantic atoms , and t is an initial or when the lexicon is accessed , x , y , z get bound to terms occurring in the semantic input , e.g . e , a , b in our example . our result is stronger in that we define a single grammar __gham__ whose realization problem is npcomplete in the above sense . 3 is wellformed according to our lexicon . a third source of combinatorics which does not become so clear in this encoding is the configuration of the elementary trees . but it turns out that the complexities that arise in generation problems in practice seem to be precisely of the sort that the tdg parser can handle well . , we obtain precisely one parse tree , shown in fig . in this section , we relate it to chart generation and to another constraintbased approach . next , we assume that whenever we can adjoin an auxiliary tree at a node , we can adjoin arbitrarily many trees at this node . the list of these indices is a path in the graph , as the α1 trees model edge transitions ; it is a cycle because it starts in 1 and ends in 1 ; and it visits each node exactly once , for we use exactly one α1 tree for each node literal . strong propagation can reduce the size of the search tree , and it may even make the whole algorithm run in polynomial time in practice . first , we assume that the nodes of the elementary trees of g are not labelled with feature structures . __<s>__ naturallanguage generation from flat semantics is an npcomplete problem __</s>__ __<s>__ this makes it necessary to develop algorithms that run with reasonable efficiency in practice despite the high worstcase complexity __</s>__ __<s>__ we show how to convert tag generation problems into dependency parsing problems which is useful because optimizations in recent dependency parsers based on constraint programming tackle exactly the combinatorics that make generation hard __</s>__ __<s>__ indeed initial experiments display promising runtimes __</s>__ __<s>__ naturallanguage generation from flat semantics is an npcomplete problem __</s>__ __<s>__ this makes it necessary to develop algorithms that run with reasonable efficiency in practice despite the high worstcase complexity __</s>__ __<s>__ we show how to convert tag generation problems into dependency parsing problems which is useful because optimizations in recent dependency parsers based on constraint programming tackle exactly the combinatorics that make generation hard __</s>__ __<s>__ indeed initial experiments display promising runtimes __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: naturallanguage generation from flat semantics is an npcomplete problem this makes it necessary to develop algorithms that run with reasonable efficiency in practice despite the high worstcase complexity we show how to convert tag generation problems into dependency parsing problems which is useful because optimizations in recent dependency parsers based on constraint programming tackle exactly the combinatorics that make generation hard indeed initial experiments display promising runtimes\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is accessed the learning of the realization of the realization possible to finish parsing a given input semantics 5 is training . it should be possible to either encode these constructions directly the semantic input , e.g . e , a , b the worst case . now let ’ s say we want to find the realizations of the input semantics s an index the npcompleteness result for the realization problem explains immediately why all existing complete generation algorithms have exponential runtimes\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  three etc . a description for the target set is represented by a pair of set variables constrained to be a subset of the set of positive and of negative properties : the universe ; : the set of properties has ; : the set of properties does not have ; : the set of properties true of all __eleof__ respectively . there is no other literal which could be used to further reduce the distractor set hence properties of the form are used . unlike spud however , the __indigen__ generator does not follow an incremental greedy search strategy mirroring the incremental d & r algorithm . more specifically , it states that for each distractor there is at least one property such that either is true of but not of or is false of and true of . it follows that the algorithm easily generalises to nary relations . dale and reiter ’ s one possible solution to the problems raised by the incremental algorithm is to generate only minimal descriptions i.e . descriptions which use the smallest number of literals to uniquely identify the target set . then __treasurer__ is selected which restricts the distractor set to . here is an example . to generalise this algorithm to disjunctive and negative properties , van deemter adds one more level of incrementality , an incrementality over the length of the properties being used . if is discourse old , the lexical entry for the will be selected and a dd computed say , . the resulting description is the disjunctive description where each is a conjunctive description . the solution proposed in and implemented in the spud generator is to integrate surface realisation and dd computation . the result is a distinguishing description which is the conjunction of properties selected to reach that state . once all properties have been realised and since there is no backtracking , generation will fail . the domain ; , the set to be described ; , the properties true of the set ; to generate the distinguishing description , do : __erties__ of the form with ; if this is successful then stop , otherwise go to phase 3. i present an alternative approach which always produce the minimal description thereby avoiding the shortcomings of the incremental algorithm . the integration of the constraint solver within the generator permits realising definite nps including negative information and simple conjunctions . the third constraint ensures that the conjunction of properties thus built eliminates all distractors i.e . each element of the universe which is not in . second , if there is no dd for a given entity , this will be immediately noticed in the present approach thus allowing for a non definite np or a quantifier to be constructed instead . epistemically redundant descriptions . in contrast , spud will , if unconstrained , keep adding material to the tree until all properties of the object to be described have been realised . however , already at this stage of processing , it is clear that the resulting description will be awkward to phrase . and how can we impose the appropriate constraints so as to predict linguistically and cognitively acceptable structures ? __<s>__ the incremental algorithm introduced in for producing distinguishing descriptions does not always generate a minimal description __</s>__ __<s>__ in this paper i show that when generalised to sets of individuals and disjunctive properties this approach might generate unnecessarily long and ambiguous and/or epistemically redundant descriptions __</s>__ __<s>__ i then present an alternative constraintbased algorithm and show that it builds on existing related algorithms in that it produces minimal descriptions for sets of individuals using positive negative and disjunctive properties it straightforwardly generalises to nary relations and it is integrated with surface realisation __</s>__ __<s>__ the incremental algorithm introduced in for producing distinguishing descriptions does not always generate a minimal description __</s>__ __<s>__ in this paper i show that when generalised to sets of individuals and disjunctive properties this approach might generate unnecessarily long and ambiguous and/or epistemically redundant descriptions __</s>__ __<s>__ i then present an alternative constraintbased algorithm and show that it builds on existing related algorithms in that it produces minimal descriptions for sets of individuals using positive negative and disjunctive properties it straightforwardly generalises to nary relations and it is integrated with surface realisation __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: the incremental algorithm introduced in for producing distinguishing descriptions does not always generate a minimal description in this paper i show that when generalised to sets of individuals and disjunctive properties this approach might generate unnecessarily long and ambiguous and/or epistemically redundant descriptions i then present an alternative constraintbased algorithm and show that it builds on existing related algorithms in that it produces minimal descriptions for sets of individuals using positive negative and disjunctive properties it straightforwardly generalises to nary relations and it is integrated with surface realisation\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is no other literal which could be used to further reduce the distractor set hence properties of the form are used . unlike spud however , the universe ; : the set of properties has ; : the universe ; : the set of properties has ; : the universe ; : the set of properties has ; : the universe ; : the set of properties has ; : the universe ; : the set of properties has ; : the universe ; : the set of properties has ; : the universe ; : the\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  another problem , on which the remainder of the article focuses , was a set of words that express modality , and commonly appear before verbs in the infinitive . __ayim__ hard __forrina__ __inthelife__ he __wanted/started__ to play and negation . a r v may giving hints is allowed ’ __amur__ __nasim__ ’ __amurot__ __lilbos__ r ‘ __alot__ j a j j j a j v language , __56:759–776__ . he caring for __thepoorpeople__ he cares for the poor people the __modalitymodulation__ plane , according to the functional school of halliday , refers to the interpersonal and ideational functions of language : modality expresses the speaker ’ s own mind ‘ __alul__ __laredet__ __gesem__ __mah__ . in the remainder of the paper , we investigate the existence of a modal category in modern hebrew , by analyzing the characteristic of these words , from a morphological , syntactic , semantic and practical point of view . __carik__ bmw ” m the subject of these verbs coreferences with the subject of the modal : ’ ani morphological disambiguation for hebrew search systems . participles vs. adjectives as both categories can modify nouns for none of the words there was a comprehensive agreement , and the pos of only seven words can be __determinded__ by voting . __wulhnot__ r r r r t t v v better better to keep quiet and enjoy __msugal__ hu ’ __hayah__ __msugal__ __lir__ ’ __oto__ __babait__ __halaban__ j r j j __jv__ his claim is that modality actually is marked by syntactic characteristics , which can be sas well as nouns and prepositions among them as many words from the corpus were either missing or tagged in a non uniform manner in the lexicons , we recommended looking up missing words in traditional dictionaries . modal expressions do not constitute a syntactic class in modern hebrew . to conclude this section , our proposed definition of modals allows us to tag this word in a systematic and complete manner and to avoid the confusion that characterizes this word . hua __racah__ j j j __ruth__ __berman__ . __zadka__ defines the class of __singleargument__ bridge __verbs4__ , i.e. , any verb or proverb that can have an infinitive as a subject or object , and that does not accept a subordinate clause as subject or object : definition of __impersonals__ is strictly __syntactic/morpholgical__ and does not try to characterize words with modality . in this section we review three major approaches to modality in hebrew the first is semantic , the second is __semanticsyntactic__ and the third is purely __morphologicosyntactic__ . some taggers would tag the word as a prepositional prefix + noun , while others tagged it as a preposition , e.g. , b ’ __iqbot__ , that can be tagged as __biqbot__ . __/hth__ . tagging was done using an automatic tool ' . given the complex nature of modality in hebrew , should we introduce a modal tag in our tagset , or instead , rely on other existing tags ? hard the agreement is hard however , following __ambar__ __<s>__ computational linguistics methods are typically first developed and tested in english __</s>__ __<s>__ when applied to other languages assumptions from english data are often applied to the target language __</s>__ __<s>__ one of the most common such assumptions is that a “standard” partofspeech tagset can be used across languages with only slight variations __</s>__ __<s>__ we discuss in this paper a specific issue related to the definition of a pos tagset for modern hebrew as an example to clarify the method through which such variations can be defined __</s>__ __<s>__ it is widely assumed that hebrew has no syntactic category of modals __</s>__ __<s>__ there is however an identified class of words which are __modallike__ in their semantics and can be characterized through distinct syntactic and morphologic criteria __</s>__ __<s>__ we have found wide disagreement among traditional dictionaries on the pos tag attributed to such words __</s>__ __<s>__ we describe three main approaches when deciding how to tag such words in hebrew __</s>__ __<s>__ we illustrate the impact of selecting each of these approaches on agreement among human taggers and on the accuracy of automatic pos taggers induced for each method __</s>__ __<s>__ we finally recommend the use of a __“modal”__ tag in hebrew and provide detailed guidelines for this tag __</s>__ __<s>__ our overall conclusion is that tagset definition is a complex task which deserves appropriate methodology __</s>__ __<s>__ computational linguistics methods are typically first developed and tested in english __</s>__ __<s>__ when applied to other languages assumptions from english data are often applied to the target language __</s>__ __<s>__ one of the most common such assumptions is that a “standard” partofspeech tagset can be used across languages with only slight variations __</s>__ __<s>__ we discuss in this paper a specific issue related to the definition of a pos tagset for modern hebrew as an example to clarify the method through which such variations can be defined __</s>__ __<s>__ it is widely assumed that hebrew has no syntactic category of modals __</s>__ __<s>__ there is however an identified class of words which are __modallike__ in their semantics and can be characterized through distinct syntactic and morphologic criteria __</s>__ __<s>__ we have found wide disagreement among traditional dictionaries on the pos tag attributed to such words __</s>__ __<s>__ we describe three main approaches when deciding how to tag such words in hebrew __</s>__ __<s>__ we illustrate the impact of selecting each of these approaches on agreement among human taggers and on the accuracy of automatic pos taggers induced for each method __</s>__ __<s>__ we finally recommend the use of a __“modal”__ tag in hebrew and provide detailed guidelines for this tag __</s>__ __<s>__ our overall conclusion is that tagset definition is a complex task which deserves appropriate methodology __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: computational linguistics methods are typically first developed and tested in english when applied to other languages assumptions from english data are often applied to the target language one of the most common such assumptions is that a “standard” partofspeech tagset can be used across languages with only slight variations we discuss in this paper a specific issue related to the definition of a pos tagset for modern hebrew as an example to clarify the method through which such variations can be defined it is widely assumed that hebrew has no syntactic category of modals there is however an identified class of words which are !!__modallike__!! in their semantics and can be characterized through distinct syntactic and morphologic criteria we have found wide disagreement among traditional dictionaries on the pos tag attributed to such words we describe three main approaches when deciding how to tag such words in hebrew we illustrate the impact of selecting each of these approaches on agreement among human taggers and on the accuracy of automatic pos taggers induced for each method we finally recommend the use of a !!__“modal”__!! tag in hebrew and provide detailed guidelines for this tag our overall conclusion is that tagset definition is a complex task which deserves appropriate methodology\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is proposed the paper , we investigate the paper , we investigate the existence of a modal category commonly , by analyzing the characteristic of the article focuses , was a set of the article focuses , was a set of words that express modality , commonly appear before verbs before verbs before verbs before verbs before verbs before verbs coreferences\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  mwer we use 1 − mwer . sentences are automatically annotated using the bios package . et which computes average overlapping over all partsofspeech . table 6 shows the level of correlation with human assessments for some metric representatives . ‘ cp ’ metrics capture similarities between constituency parse trees associated to automatic and reference translations . systems . such that a sentence may be partially seen as a ‘ bag ’ of les . for instance , ‘ __spocnp__ ’ roughly reflects the successfully translated proportion of noun phrases . the ‘ __neme*__ meteor we run all modules : ‘ exact ’ , ‘ __porterstem__ ’ , ‘ wn stem ’ and ‘ wn synonymy ’ , in that order . at the shallow syntactic level , only metrics which do not consider any lexical information attain a significantly higher quality . les are linguistic units , structures , or relationships , the fraction of matching headword chains of a given length , ‘ l ’ , is computed . on the contrary , natural languages are expressive and ambiguous at different levels . et les allow for comparisons at different granularity levels , and from different viewpoints . average accumulated scores may be computed as well . for instance , bleu and , in general , all metrics based on lexical matching alone , except meteor , obtain significantly lower levels of correlation than metrics based on deeper linguistic similarities . in other words , we are computing the proportion of ‘ fully ’ translated elements , according to their type . for six of these systems we counted on a subjective manual evaluation based on adequacy and fluency for a subset of 266 sentences . al . . in section 3.2 , we study the case of several reference translations available . for instance , ‘ __cpstmi5__ ’ retrieves the proportion of __length5__ matching subpaths . most of the current metrics operate at the lexical level . indeed , the underlying cause is much simpler . for instance , ‘ __cpstm9__ ’ retrieves average accumulated proportion of matching subpaths up to __length9__ . in the two remaining tasks , spanishtoenglish __indomain/outofdomain__ , all the systems are statistical . human likeness metrics are evaluated in terms of descriptive power , i.e. , their ability to distinguish between human and automatic translations . * ’ , ‘ __cpstm9__ ’ , ‘ __sror*__ ’ , ‘ __srorv__ ’ } __spoct__ lexical overlapping according to the chunk type ‘ t ’ . first , these tools are not available for all languages . nist we use the default accumulated score up to the level of 5grams . gtm we set to 1 the value of the e parameter . its definition is analogous to the ‘ overlapping ’ definition , but in this case the relative order of the items is important . for instance , the following set could be used : { ‘ __dphwcr4__ ’ , ‘ __dpoc*__ ’ , ‘ __dpol*__ ’ , ‘ dpor for instance , ‘ __spo__ , nn ’ roughly reflects the proportion of correctly translated singular nouns . the fraction of matching subpaths of a given length , ‘ l ’ , is computed . based on dependency and constituency parsing . in this work , we hypothesize that , in order to ‘ fairly ’ evaluate mt systems based on different paradigms , similarities at more abstract linguistic levels must be analyzed . metric is a clear indication that in order to achieve a high quality , it is important to ‘ fully ’ translate ‘ whole ’ semantic structures . __<s>__ evaluation results recently reported by callisonburch et al __</s>__ __<s>__ and koehn and monz revealed that in certain cases the bleu metric may not be a reliable mt quality indicator __</s>__ __<s>__ this happens for instance when the systems under evaluation are based on different paradigms and therefore do not share the same lexicon __</s>__ __<s>__ the reason is that while mt quality aspects are diverse bleu limits its scope to the lexical dimension __</s>__ __<s>__ in this work we suggest using metrics which take into account linguistic features at more abstract levels __</s>__ __<s>__ we provide experimental results showing that metrics based on deeper linguistic information are able to produce more reliable system rankings than metrics based on lexical matching alone specially when the systems under evaluation are of a different nature __</s>__ __<s>__ evaluation results recently reported by callisonburch et al __</s>__ __<s>__ and koehn and monz revealed that in certain cases the bleu metric may not be a reliable mt quality indicator __</s>__ __<s>__ this happens for instance when the systems under evaluation are based on different paradigms and therefore do not share the same lexicon __</s>__ __<s>__ the reason is that while mt quality aspects are diverse bleu limits its scope to the lexical dimension __</s>__ __<s>__ in this work we suggest using metrics which take into account linguistic features at more abstract levels __</s>__ __<s>__ we provide experimental results showing that metrics based on deeper linguistic information are able to produce more reliable system rankings than metrics based on lexical matching alone specially when the systems under evaluation are of a different nature __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: evaluation results recently reported by callisonburch et al and koehn and monz revealed that in certain cases the bleu metric may not be a reliable mt quality indicator this happens for instance when the systems under evaluation are based on different paradigms and therefore do not share the same lexicon the reason is that while mt quality aspects are diverse bleu limits its scope to the lexical dimension in this work we suggest using metrics which take into account linguistic features at more abstract levels we provide experimental results showing that metrics based on deeper linguistic information are able to produce more reliable system rankings than metrics based on lexical matching alone specially when the systems under evaluation are of a different nature\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . the ‘ of the shallow syntactic level , obtain significantly lower levels of correlation than metrics based on lexical matching alone , except meteor , obtain significantly lower levels of correlation than metrics are automatically annotated using the bios package . et which computes average overlapping over all partsofspeech . table 6 shows the level of correlation with human assessments for some metric representatives . ‘ cp ’ . et which computes average overlapping over all partsofspeech . table 6 shows the level of correlation with human assessments for some metric representatives . ‘\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  amharic orthography does not indicate gemination , but since there are relatively few minimal pairs of __geminations__ , amharic readers do not find this to be a problem . proper editing of the pronunciation dictionaries which , however , requires a considerable amount of work , certainly will result in a higher quality of subword transcription and consequently in the improvement of the recognizers ' performance . al . these test dictionaries have 5,000 and 20,000 words each . as we can see from table 6 , models with three emitting states do have a competitive performance with 18 and hybrid gaussian mixtures . both these pronunciation dictionaries do not handle the difference between geminated and __nongeminated__ consonants ; the variation of the pronunciation of the sixth order grapheme , with or without vowel ; and the absence or presence of the glottal stop consonant . in our work , an hmm model has been developed for each of these cv syllables . wu the latter implies that during the first cycle of embedded reestimation , each training utterance will be uniformly segmented . one of the required elements in the development of __lvasrss__ is the language model . both of these arguments are based on the special feature of the orthography ; the possibility of representing speech using either isolated phoneme symbols or concatenated symbols . thus , by eliminating redundant graphemes , we are left with a total of 233 distinct cv syllable characters . models with three emitting states also have larger number of total gaussian __mixtures5__ than the best performing models . for example , it has a set of speech sounds that is not found in other languages . large vocabulary automatic speech recognition systems require modeling of speech in smaller units than words because the acoustic samples of most words will never be seen during training , and therefore , can not be trained . __iimm__ topologies : to our knowledge , there is no topology of hmm model that can be taken as a rule of thumb for modeling syllable hmms , especially , for amharic cv syllables . a canonical pronunciation dictionary includes only the standard phone sequence assumed to be pronounced in read speech . although phones are very small in number and relatively easy to train , they are much more sensitive to contextual influences than larger units . the alternative pronunciation dictionary contains up to 25 pronunciation variants per word form . by jump we mean skips from the first nonemitting state to the middle state and/or from the middle state to the last nonemitting state . a sample of pronunciations in the canonical and alternative pronunciation dictionaries is given in table __43.__ in support of the above point , __leslaw__ noted that no real problems exist in amharic orthography , as there is more or less , a onetoone correspondence between the sounds and the graphic symbols , except for the gemination of consonants and some redundant symbols . the speech of addis ababa has emerged as the standard dialect and has wide currency across all __amharicspeaking__ communities . table 5 illustrates some cases of the variation . these are encouraging results as compared to the performance reported by __afify__ et al . __<s>__ amharic is the semitic language that has the second large number of speakers after arabic __</s>__ __<s>__ its writing system is syllabic with consonantvowel syllable structure __</s>__ __<s>__ amharic orthography has more or less a one to one correspondence with syllabic sounds __</s>__ __<s>__ we have used this feature of amharic to develop a cv syllablebased speech recognizer using hidden markov modeling and achieved 90 43% word recognition accuracy __</s>__ __<s>__ amharic is the semitic language that has the second large number of speakers after arabic __</s>__ __<s>__ its writing system is syllabic with consonantvowel syllable structure __</s>__ __<s>__ amharic orthography has more or less a one to one correspondence with syllabic sounds __</s>__ __<s>__ we have used this feature of amharic to develop a cv syllablebased speech recognizer using hidden markov modeling and achieved 90 43% word recognition accuracy __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: amharic is the semitic language that has the second large number of speakers after arabic its writing system is syllabic with consonantvowel syllable structure amharic orthography has more or less a one to one correspondence with syllabic sounds we have used this feature of amharic to develop a cv syllablebased speech recognizer using hidden markov modeling and achieved 90 43% word recognition accuracy\n",
            "INFO:tensorflow:GENERATED SUMMARY: amharic , but since there are relatively few minimal pairs of the pronunciation , amharic readers do not find this to be a problem . proper editing of the pronunciation dictionaries which , however , requires a considerable amount of work , certainly will result a problem . proper editing of the pronunciation dictionaries which , however , requires a considerable amount of work , certainly will result a problem . proper editing of the pronunciation dictionaries which , however , requires a considerable amount of work , certainly will\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  figure 1 shows the setting of our experiment . it consists in normalizing the gene name and matching the resulting string to one of the gene names or aliases listed in entrez gene . for example , meet specific chemistry oriented search needs by representing us patents and patent applications with molecular information in the form of chemical terms and structures . in practice , this means that there is no ideal unique set of descriptors to use for the indexing of a particular document . the corresponding ten nearest neighbors were retrieved using the prc and __grc__ algorithms . finding similar documents “ major ” descriptors are marked with a star . the mapping was performed with a version of semrep restricted to human genes . the most significant result of this study is that the pooling of methods achieves a recall of 92 % for standalone subheading retrieval . however , only a subset of citations actually contains genes . table 2 presents the results of our experiments . for example , the gene __epo__ “ __erythropoietin__ ” is listed in entrez gene for 11 organisms including homo sapiens . this text is processed so that punctuation is removed , stopwords from a predefined list are removed , remaining words are switched to lower case and a minimal amount of stemming is applied . moreover , words from the title are given an additional weight compared to words from the abstract . further adjustments relative to document length and local weighting according to the poisson distribution are detailed in where the pubmed related citations algorithm is discussed . furthermore , they yield higher recall than other more linguisticbased methods . in light of these figures , although no direct comparison with the results of our experiment is possible , the precision obtained from the common recommendations seems reasonably useful . documents from the test corpus described above were processed to obtain both a generic and specific representation as described in section 2.2. in the genetics domain , genes are among the most commonly discussed or manipulated concepts . both methods were tested on a corpus composed of a selection of the __49,863__ citations entered into medline in january 2005. the algorithm consists of two steps : __1/documents__ that are most “ similar ” to the query document must be retrieved from the set of labeled documents . our test corpus was composed of __geneticsrelated__ citations from which entrez gene ids could be extracted – about 40 % of the cases . mesh also contains 83 subheadings such as genetics , metabolism or surgery that can be associated with the main headings in order to refer to a specific aspect of the concept . all documents are represented using a vector of distinctive features within the representation space . the 2006 version of mesh was used for the indexing in these citations . in comparison with other research , the results are also encouraging : the recall resulting from either prc or pooling the indexing sets is significantly better than that obtained by __névéol__ et al . __<s>__ the shift from paper to electronic documents has caused the curation of information sources in large electronic databases to become more generalized __</s>__ __<s>__ in the biomedical domain continuing efforts aim at refining indexing tools to assist with the update and maintenance of databases such as medline® __</s>__ __<s>__ in this paper we evaluate two statistical methods of producing mesh® indexing recommendations for the genetics literature including recommendations involving subheadings which is a novel application for the methods __</s>__ __<s>__ we show that a generic representation of the documents yields both better precision and recall __</s>__ __<s>__ we also find that a domainspecific representation of the documents can contribute to enhancing recall __</s>__ __<s>__ the shift from paper to electronic documents has caused the curation of information sources in large electronic databases to become more generalized __</s>__ __<s>__ in the biomedical domain continuing efforts aim at refining indexing tools to assist with the update and maintenance of databases such as medline® __</s>__ __<s>__ in this paper we evaluate two statistical methods of producing mesh® indexing recommendations for the genetics literature including recommendations involving subheadings which is a novel application for the methods __</s>__ __<s>__ we show that a generic representation of the documents yields both better precision and recall __</s>__ __<s>__ we also find that a domainspecific representation of the documents can contribute to enhancing recall __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: the shift from paper to electronic documents has caused the curation of information sources in large electronic databases to become more generalized in the biomedical domain continuing efforts aim at refining indexing tools to assist with the update and maintenance of databases such as medline® in this paper we evaluate two statistical methods of producing mesh® indexing recommendations for the genetics literature including recommendations involving subheadings which is a novel application for the methods we show that a generic representation of the documents yields both better precision and recall we also find that a domainspecific representation of the documents can contribute to enhancing recall\n",
            "INFO:tensorflow:GENERATED SUMMARY: 1 shows the setting of our experiment . it consists to shows the setting of our experiment . it consists to shows the setting of our experiment . it consists to shows the setting of our experiment . it consists is possible entrez gene for 11 organisms including homo sapiens . for example , meet specific chemistry oriented search needs by representing us patents patent applications with molecular information\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  __mj1__ and __mj2__ are shown in figure 1. starting from the start state , we recursively extend a subpath word by word , following the paths in the lattice . the ibm constraints are __supersets__ of the local constraints . accumulating oracle algorithm makes a greedy decision for every sentence given the ngram counts so far accumulated . both shortand longdistance reorderings contribute to the bleu score improvements , which are generally greater for longer sentences . however , clipping affects most heavily the unigram scores which are constant , like the length of the __sentence.4__ the input consists of __sevwords__ swapped constituents . the function update ngrams has access to the reference __string2__ and stores the updated ngram counts for the resulting path in a hash we write ibm with __k=4__ as ibm . this allows for all six possible permutations of abc . we use the monotone translation baseline model and the oracle bleu computation to evaluate different popular reordering strategies . we merely decode by efficiently searching over possible translations allowed by each model and choosing the reordering that achieves the highest bleu score . the itg bleu decoder stores an you __tome__ that explain could and its reordering to if you could explain that __tome__ using an itg . we now describe algorithms that can be used to find such oracle translations among __unreordered__ translation candidates . the presented bleu decoder algorithms can be useful in many ways : they can generally help decide what reordering constraints to choose for a given translation system . if the combined length of two constituents exceeds this bound they can only be combined in the given monotone order . in our experiments also ignore the brevity penalty which cancels out . __mj1__ is less successful in reordering , improving the monotone baseline by only about 2.5 bleu points at best , but is the best choice if speed is an issue . a sequence consisting of three phrases abc can therefore become __acb__ or __bac__ , but not __cba__ . al . therefore we can not discount a locally attached word that has already been attached __else4since__ the sentence lengths are constant for all reorderings of a given sentence we can using this oracle approach , we abstract away from issues that are not inherent in the reordering constraints , but may nevertheless influence the comparison results , such as model and feature design , feature selection , or parameter estimation . in their second strategy , called __mj2__ , phrases are allowed to swap with their immediate neighbor or with the phrase next to the immediate neighbor ; the maximum jump length is 2. if to compare the reordering constraints under oracle conditions we first obtain __unreordered__ candidate translations from a simple baseline translation model . we are concerned with the maximal achievable accuracy under given constraints , independent of feature design or parameter estimation . whenever we extend the path by a word to the right we incorporate that word and use update ngrams to update the four ngram counts for the subpath . __<s>__ this paper describes a new method to compare reordering constraints for statistical machine translation __</s>__ __<s>__ we investigate the best possible bleu score achievable under different reordering constraints __</s>__ __<s>__ using dynamic programming we efficiently find a reordering that approximates the highest attainable bleu score given a reference and a set of reordering constraints __</s>__ __<s>__ we present an empirical evaluation of popular reordering constraints: local constraints the ibm constraints and the inversion transduction grammar constraints __</s>__ __<s>__ we present results for a germanenglish translation task and show that reordering under the itg constraints can improve over the baseline by more than 7 5 bleu points __</s>__ __<s>__ this paper describes a new method to compare reordering constraints for statistical machine translation __</s>__ __<s>__ we investigate the best possible bleu score achievable under different reordering constraints __</s>__ __<s>__ using dynamic programming we efficiently find a reordering that approximates the highest attainable bleu score given a reference and a set of reordering constraints __</s>__ __<s>__ we present an empirical evaluation of popular reordering constraints: local constraints the ibm constraints and the inversion transduction grammar constraints __</s>__ __<s>__ we present results for a germanenglish translation task and show that reordering under the itg constraints can improve over the baseline by more than 7 5 bleu points __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: this paper describes a new method to compare reordering constraints for statistical machine translation we investigate the best possible bleu score achievable under different reordering constraints using dynamic programming we efficiently find a reordering that approximates the highest attainable bleu score given a reference and a set of reordering constraints we present an empirical evaluation of popular reordering constraints: local constraints the ibm constraints and the inversion transduction grammar constraints we present results for a germanenglish translation task and show that reordering under the itg constraints can improve over the baseline by more than 7 5 bleu points\n",
            "INFO:tensorflow:GENERATED SUMMARY: to on can is training . table paper a experiments also ignore the brevity penalty which cancels out . function update ngrams has access to the reference the given monotone order . both shortand longdistance , we recursively extend a subpath word by word , following the paths the given monotone order . both shortand longdistance , we recursively extend a subpath word by word , following the paths\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "\n",
            "INFO:tensorflow:ARTICLE:  a target set sentence is considered to be attracted to the feedback set containing the sentence to which it shows the highest similarity . the fscore is defined as values and values of zero . these details are provided in a previous publication . ideally , this would produce a compounding of benefits . however , human help may be the only chance these sentences have to be clustered at all . the third iteration option gave slightly better results than the last iteration option , but since the difference was minor , we opted for the stability of sending everything after the last iteration . this paper focuses on the use of active learning using this model . we omit some details about the algorithm here which do not affect our discussion about active learning . as per , cf . to deal with this issue provides automatic methods to clean up the feedback sets during the clustering algorithm . the ke algorithm is based on the principle of attraction however , in order to be able to compare the __humanonly__ scenario to the active learning scenario , we must find what the average fscore of the manual process is . average precision is the average of literal and nonliteral precision ; similarly for average recall . each sentence in the target set is attracted to the feedback set with which it has the most words in common . our identification model for literal vs. nonliteral usage of verbs is described in detail in a previous publication . sending everything after the first iteration produced an average accuracy of __66.8__ % ; sending everything after the third iteration , 65.2 % ; sending a small amount at each iteration , __60.8__ % ; sending everything after the last iteration , 64.9 % . since we are attempting to reduce the problem of literal/nonliteral recognition to one of wordsense disambiguation , we use an existing similaritybased wordsense disambiguation algorithm developed by , henceforth ke . each feedback set however we do exploit a noisy source of seed data in a nearly unsupervised approach augmented with active learning . note that sending examples that score zero to the human may not help attract new sentences to either of the feedback sets . we now discuss the various active learning experiments we performed using our model and a human expert annotator . experiments were performed to determine the best time to send up to 30 % of the sentences to the human annotator . the resulting transitive similarity allows us to defeat the knowledge acquisition bottleneck – i.e . the low likelihood of finding all possible usages of a word in a single corpus . running the algorithm in this way produces new clusters and a reweighted model augmented with newly clustered sentences . our interannotator agreement on the annotations used as test data in the experiments in this paper is quite high . literal recall is defined as . we suggest that this approach produces a large saving of effort compared to creating such an annotated corpus manually . the algorithms were evaluated based on how accurately they clustered the handannotated sentences . __<s>__ in this paper we present an active learning approach used to create an annotated corpus of literal and nonliteral usages of verbs __</s>__ __<s>__ the model uses nearly unsupervised wordsense disambiguation and clustering techniques __</s>__ __<s>__ we report on experiments in which a human expert is asked to correct system predictions in different stages of learning: after the last iteration when the clustering step has converged or during each iteration of the clustering algorithm __</s>__ __<s>__ the model obtains an fscore of 53 8% on a dataset in which literal/nonliteral usages of 25 verbs were annotated by human experts __</s>__ __<s>__ in comparison the same model augmented with active learning obtains 64 91% __</s>__ __<s>__ we also measure the number of examples required when model confidence is used to select examples for human correction as compared to random selection __</s>__ __<s>__ the results of this active learning system have been compiled into a freely available annotated corpus of literal/nonliteral usage of verbs in context __</s>__ __<s>__ in this paper we present an active learning approach used to create an annotated corpus of literal and nonliteral usages of verbs __</s>__ __<s>__ the model uses nearly unsupervised wordsense disambiguation and clustering techniques __</s>__ __<s>__ we report on experiments in which a human expert is asked to correct system predictions in different stages of learning: after the last iteration when the clustering step has converged or during each iteration of the clustering algorithm __</s>__ __<s>__ the model obtains an fscore of 53 8% on a dataset in which literal/nonliteral usages of 25 verbs were annotated by human experts __</s>__ __<s>__ in comparison the same model augmented with active learning obtains 64 91% __</s>__ __<s>__ we also measure the number of examples required when model confidence is used to select examples for human correction as compared to random selection __</s>__ __<s>__ the results of this active learning system have been compiled into a freely available annotated corpus of literal/nonliteral usage of verbs in context __</s>__\n",
            "INFO:tensorflow:REFERENCE SUMMARY: in this paper we present an active learning approach used to create an annotated corpus of literal and nonliteral usages of verbs the model uses nearly unsupervised wordsense disambiguation and clustering techniques we report on experiments in which a human expert is asked to correct system predictions in different stages of learning: after the last iteration when the clustering step has converged or during each iteration of the clustering algorithm the model obtains an fscore of 53 8% on a dataset in which literal/nonliteral usages of 25 verbs were annotated by human experts in comparison the same model augmented with active learning obtains 64 91% we also measure the number of examples required when model confidence is used to select examples for human correction as compared to random selection the results of this active learning system have been compiled into a freely available annotated corpus of literal/nonliteral usage of verbs in context\n",
            "INFO:tensorflow:GENERATED SUMMARY: % ; sending everything after the third iteration , 65.2 % ; sending a small amount at each iteration , 65.2 % ; sending a small amount at each iteration , 65.2 % ; sending a small amount at each iteration , 65.2 % ; sending a small amount at each iteration , 65.2 % ; sending a small amount at each iteration , 65.2 % ; sending a small amount at each iteration , 65.2 % ; sending a small amount at each iteration , 65.2 % ; sending a small amount at each iteration , 65.2 % ;\n",
            "\n",
            "INFO:tensorflow:Wrote visualization data to /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode/attn_vis_data.json\n",
            "INFO:tensorflow:We've been decoding with same checkpoint for 65 seconds. Time to load new checkpoint\n",
            "INFO:tensorflow:Loading checkpoint /content/drive/My Drive/MA_colab/PG_Model2/logs/myexperiment/train/model.ckpt-15726\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/MA_colab/PG_Model2/logs/myexperiment/train/model.ckpt-15726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEproyCBgvmK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ce15786-1d3e-4abe-a2f5-1197b32ef211"
      },
      "source": [
        "#evaluation\n",
        "#validation\n",
        "class flags_:\n",
        "  pass\n",
        "FLAGS = flags_()\n",
        "\n",
        "# Where to find data\n",
        "#FLAGS.data_path= default_path + 'finished_files/chunked/train_*'\t#, 'Path expression to tf.Example datafiles. Can include wildcards to access multiple datafiles.')\n",
        "FLAGS.data_path= default_path + 'finished_files/chunked/val_*'\t#, 'Path expression to tf.Example datafiles. Can include wildcards to access multiple datafiles.')\n",
        "FLAGS.vocab_path= default_path + 'finished_files/vocab'\t#, 'Path expression to text vocabulary file.')\n",
        "\n",
        "# Important settings\n",
        "FLAGS.mode= 'decode' #'train'#, 'must be one of train/eval/decode')\n",
        "FLAGS.single_pass= True #False\n",
        "#, 'For decode mode only. If True, run eval on the full dataset using a fixed checkpoint, i.e. take the current checkpoint, \n",
        "#and use it to produce one summary for each example in the dataset, write \n",
        "#the summaries to file and then get ROUGE scores for the whole dataset.\n",
        "#If False (default), run concurrent decoding, \n",
        "#i.e. repeatedly load latest checkpoint,\n",
        "#use it to produce summaries for randomly-chosen examples and log the results to screen, indefinitely.')\n",
        "\n",
        "# Where to save output\n",
        "FLAGS.log_root= default_path +'logs'#, 'Root directory for all logging.')\n",
        "FLAGS.exp_name= 'myexperiment'#, 'Name for experiment. Logs will be saved in a directory with this name, under log_root.')\n",
        "\n",
        "# Hyperparameters\n",
        "FLAGS.hidden_dim= 256#, 'dimension of RNN hidden states')\n",
        "FLAGS.emb_dim= 128#, 'dimension of word embeddings')\n",
        "FLAGS.batch_size= 16#, 'minibatch size')\n",
        "FLAGS.max_enc_steps= 300#, 'max timesteps of encoder (max source text tokens)')\n",
        "FLAGS.max_dec_steps= 100#, 'max timesteps of decoder (max summary tokens)')\n",
        "FLAGS.beam_size= 8 #8#, 'beam size for beam search decoding.')\n",
        "FLAGS.min_dec_steps= 50#, 'Minimum sequence length of generated summary. Applies only for beam search decoding mode')\n",
        "FLAGS.vocab_size= 50000#, 'Size of vocabulary. These will be read from the vocabulary file in order. If the vocabulary file contains fewer words than this number, or if this number is set to 0, will take all words in the vocabulary file.')\n",
        "FLAGS.lr= 0.1#, 'learning rate')\n",
        "FLAGS.adagrad_init_acc= 0.1#, 'initial accumulator value for Adagrad')\n",
        "FLAGS.rand_unif_init_mag= 0.02#, 'magnitude for lstm cells random uniform inititalization')\n",
        "FLAGS.trunc_norm_init_std= 1e-4#, 'std of trunc norm init, used for initializing everything else')\n",
        "FLAGS.max_grad_norm= 2.0#, 'for gradient clipping')\n",
        "\n",
        "# Pointer-generator or baseline model\n",
        "FLAGS.pointer_gen= True#, 'If True, use pointer-generator model. If False, use baseline model.')\n",
        "\n",
        "# Coverage hyperparameters\n",
        "FLAGS.coverage= False#, 'Use coverage mechanism. Note, the experiments reported in the ACL paper train WITHOUT coverage until converged, and then train for a short phase WITH coverage afterwards. i.e. to reproduce the results in the ACL paper, turn this off for most of training then turn on for a short phase at the end.')\n",
        "FLAGS.cov_loss_wt= 1.0#, 'Weight of coverage loss (lambda in the paper). If zero, then no incentive to minimize coverage loss.')\n",
        "\n",
        "# Utility flags, for restoring and changing checkpoints\n",
        "FLAGS.convert_to_coverage_model= False#, 'Convert a non-coverage model to a coverage model. Turn this on and run in train mode. Your current training model will be copied to a new version (same name with _cov_init appended) that will be ready to run with coverage flag turned on, for the coverage training stage.')\n",
        "FLAGS.restore_best_model= False#, 'Restore the best model in the eval/ dir and save it in the train/ dir, ready to be used for further training. Useful for early stopping, or if your training checkpoint has become corrupted with e.g. NaN values.')\n",
        "\n",
        "# Debugging. See https://www.tensorflow.org/programmers_guide/debugger\n",
        "FLAGS.debug= False#, \"Run in tensorflow's debug mode (watches for NaN/inf values)\")\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting seq2seq_attention in decode mode...\n",
            "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
            "Finished constructing vocabulary of 50000 total words. Last word added: nonconfrontational\n",
            "INFO:tensorflow:Building graph...\n",
            "WARNING:tensorflow:From <ipython-input-7-c6f8ee501824>:68: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-7-c6f8ee501824>:70: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:966: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:970: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "INFO:tensorflow:Adding attention_decoder timestep 0 of 1\n",
            "INFO:tensorflow:Time to build graph: 1 seconds\n",
            "INFO:tensorflow:Loading checkpoint /content/drive/My Drive/MA_colab/PG_Model2/logs/myexperiment/train/model.ckpt-15726\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/MA_colab/PG_Model2/logs/myexperiment/train/model.ckpt-15726\n",
            "INFO:tensorflow:Wrote example 0 to file\n",
            "INFO:tensorflow:Wrote example 1 to file\n",
            "INFO:tensorflow:Wrote example 2 to file\n",
            "INFO:tensorflow:Wrote example 3 to file\n",
            "INFO:tensorflow:Wrote example 4 to file\n",
            "INFO:tensorflow:Wrote example 5 to file\n",
            "INFO:tensorflow:Wrote example 6 to file\n",
            "INFO:tensorflow:Wrote example 7 to file\n",
            "INFO:tensorflow:Wrote example 8 to file\n",
            "INFO:tensorflow:Wrote example 9 to file\n",
            "INFO:tensorflow:Wrote example 10 to file\n",
            "INFO:tensorflow:Wrote example 11 to file\n",
            "INFO:tensorflow:Wrote example 12 to file\n",
            "INFO:tensorflow:Wrote example 13 to file\n",
            "INFO:tensorflow:Wrote example 14 to file\n",
            "INFO:tensorflow:Wrote example 15 to file\n",
            "INFO:tensorflow:Wrote example 16 to file\n",
            "INFO:tensorflow:Wrote example 17 to file\n",
            "INFO:tensorflow:Wrote example 18 to file\n",
            "INFO:tensorflow:Wrote example 19 to file\n",
            "INFO:tensorflow:Wrote example 20 to file\n",
            "INFO:tensorflow:Wrote example 21 to file\n",
            "INFO:tensorflow:Wrote example 22 to file\n",
            "INFO:tensorflow:Wrote example 23 to file\n",
            "INFO:tensorflow:Wrote example 24 to file\n",
            "INFO:tensorflow:Wrote example 25 to file\n",
            "INFO:tensorflow:Wrote example 26 to file\n",
            "INFO:tensorflow:Wrote example 27 to file\n",
            "INFO:tensorflow:Wrote example 28 to file\n",
            "INFO:tensorflow:Wrote example 29 to file\n",
            "INFO:tensorflow:Wrote example 30 to file\n",
            "INFO:tensorflow:Wrote example 31 to file\n",
            "INFO:tensorflow:Wrote example 32 to file\n",
            "INFO:tensorflow:Wrote example 33 to file\n",
            "INFO:tensorflow:Wrote example 34 to file\n",
            "INFO:tensorflow:Wrote example 35 to file\n",
            "INFO:tensorflow:Wrote example 36 to file\n",
            "INFO:tensorflow:Wrote example 37 to file\n",
            "INFO:tensorflow:Wrote example 38 to file\n",
            "INFO:tensorflow:Wrote example 39 to file\n",
            "INFO:tensorflow:Wrote example 40 to file\n",
            "INFO:tensorflow:Wrote example 41 to file\n",
            "INFO:tensorflow:Wrote example 42 to file\n",
            "INFO:tensorflow:Wrote example 43 to file\n",
            "INFO:tensorflow:Wrote example 44 to file\n",
            "INFO:tensorflow:Wrote example 45 to file\n",
            "INFO:tensorflow:Wrote example 46 to file\n",
            "INFO:tensorflow:Wrote example 47 to file\n",
            "INFO:tensorflow:Wrote example 48 to file\n",
            "INFO:tensorflow:Wrote example 49 to file\n",
            "INFO:tensorflow:Wrote example 50 to file\n",
            "INFO:tensorflow:Wrote example 51 to file\n",
            "INFO:tensorflow:Wrote example 52 to file\n",
            "INFO:tensorflow:Wrote example 53 to file\n",
            "INFO:tensorflow:Wrote example 54 to file\n",
            "INFO:tensorflow:Wrote example 55 to file\n",
            "INFO:tensorflow:Wrote example 56 to file\n",
            "INFO:tensorflow:Wrote example 57 to file\n",
            "INFO:tensorflow:Wrote example 58 to file\n",
            "INFO:tensorflow:Wrote example 59 to file\n",
            "INFO:tensorflow:Wrote example 60 to file\n",
            "INFO:tensorflow:Wrote example 61 to file\n",
            "INFO:tensorflow:Wrote example 62 to file\n",
            "INFO:tensorflow:Wrote example 63 to file\n",
            "INFO:tensorflow:Wrote example 64 to file\n",
            "INFO:tensorflow:Wrote example 65 to file\n",
            "INFO:tensorflow:Wrote example 66 to file\n",
            "INFO:tensorflow:Wrote example 67 to file\n",
            "INFO:tensorflow:Wrote example 68 to file\n",
            "INFO:tensorflow:Wrote example 69 to file\n",
            "INFO:tensorflow:Wrote example 70 to file\n",
            "INFO:tensorflow:Wrote example 71 to file\n",
            "INFO:tensorflow:Wrote example 72 to file\n",
            "INFO:tensorflow:Wrote example 73 to file\n",
            "INFO:tensorflow:Wrote example 74 to file\n",
            "INFO:tensorflow:Wrote example 75 to file\n",
            "INFO:tensorflow:Wrote example 76 to file\n",
            "INFO:tensorflow:Wrote example 77 to file\n",
            "INFO:tensorflow:Wrote example 78 to file\n",
            "INFO:tensorflow:Wrote example 79 to file\n",
            "INFO:tensorflow:Wrote example 80 to file\n",
            "INFO:tensorflow:Wrote example 81 to file\n",
            "INFO:tensorflow:Wrote example 82 to file\n",
            "INFO:tensorflow:Wrote example 83 to file\n",
            "INFO:tensorflow:Wrote example 84 to file\n",
            "INFO:tensorflow:Wrote example 85 to file\n",
            "INFO:tensorflow:Wrote example 86 to file\n",
            "INFO:tensorflow:Wrote example 87 to file\n",
            "INFO:tensorflow:Wrote example 88 to file\n",
            "INFO:tensorflow:Wrote example 89 to file\n",
            "INFO:tensorflow:Wrote example 90 to file\n",
            "INFO:tensorflow:Wrote example 91 to file\n",
            "INFO:tensorflow:Wrote example 92 to file\n",
            "INFO:tensorflow:Wrote example 93 to file\n",
            "INFO:tensorflow:Wrote example 94 to file\n",
            "INFO:tensorflow:Wrote example 95 to file\n",
            "INFO:tensorflow:Wrote example 96 to file\n",
            "INFO:tensorflow:Wrote example 97 to file\n",
            "example_generator completed reading all datafiles. No more data.\n",
            "INFO:tensorflow:The example generator for this example queue filling thread has exhausted data.\n",
            "INFO:tensorflow:single_pass mode is on, so we've finished reading dataset. This thread is stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:267: DeprecationWarning: generator 'Batcher.text_generator' raised StopIteration\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Wrote example 98 to file\n",
            "INFO:tensorflow:Wrote example 99 to file\n",
            "INFO:tensorflow:Wrote example 100 to file\n",
            "INFO:tensorflow:Wrote example 101 to file\n",
            "INFO:tensorflow:Wrote example 102 to file\n",
            "INFO:tensorflow:Wrote example 103 to file\n",
            "INFO:tensorflow:Wrote example 104 to file\n",
            "INFO:tensorflow:Wrote example 105 to file\n",
            "INFO:tensorflow:Wrote example 106 to file\n",
            "INFO:tensorflow:Wrote example 107 to file\n",
            "INFO:tensorflow:Wrote example 108 to file\n",
            "INFO:tensorflow:Wrote example 109 to file\n",
            "INFO:tensorflow:Wrote example 110 to file\n",
            "INFO:tensorflow:Wrote example 111 to file\n",
            "INFO:tensorflow:Wrote example 112 to file\n",
            "INFO:tensorflow:Wrote example 113 to file\n",
            "INFO:tensorflow:Wrote example 114 to file\n",
            "INFO:tensorflow:Wrote example 115 to file\n",
            "INFO:tensorflow:Wrote example 116 to file\n",
            "INFO:tensorflow:Wrote example 117 to file\n",
            "INFO:tensorflow:Wrote example 118 to file\n",
            "INFO:tensorflow:Wrote example 119 to file\n",
            "INFO:tensorflow:Wrote example 120 to file\n",
            "INFO:tensorflow:Wrote example 121 to file\n",
            "INFO:tensorflow:Wrote example 122 to file\n",
            "INFO:tensorflow:Wrote example 123 to file\n",
            "INFO:tensorflow:Wrote example 124 to file\n",
            "INFO:tensorflow:Wrote example 125 to file\n",
            "INFO:tensorflow:Wrote example 126 to file\n",
            "INFO:tensorflow:Wrote example 127 to file\n",
            "INFO:tensorflow:Wrote example 128 to file\n",
            "INFO:tensorflow:Wrote example 129 to file\n",
            "INFO:tensorflow:Wrote example 130 to file\n",
            "INFO:tensorflow:Wrote example 131 to file\n",
            "INFO:tensorflow:Wrote example 132 to file\n",
            "INFO:tensorflow:Wrote example 133 to file\n",
            "INFO:tensorflow:Wrote example 134 to file\n",
            "INFO:tensorflow:Wrote example 135 to file\n",
            "INFO:tensorflow:Wrote example 136 to file\n",
            "INFO:tensorflow:Wrote example 137 to file\n",
            "INFO:tensorflow:Wrote example 138 to file\n",
            "INFO:tensorflow:Wrote example 139 to file\n",
            "INFO:tensorflow:Wrote example 140 to file\n",
            "INFO:tensorflow:Wrote example 141 to file\n",
            "INFO:tensorflow:Wrote example 142 to file\n",
            "INFO:tensorflow:Wrote example 143 to file\n",
            "INFO:tensorflow:Wrote example 144 to file\n",
            "INFO:tensorflow:Wrote example 145 to file\n",
            "INFO:tensorflow:Wrote example 146 to file\n",
            "INFO:tensorflow:Wrote example 147 to file\n",
            "INFO:tensorflow:Wrote example 148 to file\n",
            "INFO:tensorflow:Wrote example 149 to file\n",
            "INFO:tensorflow:Wrote example 150 to file\n",
            "INFO:tensorflow:Wrote example 151 to file\n",
            "INFO:tensorflow:Wrote example 152 to file\n",
            "INFO:tensorflow:Wrote example 153 to file\n",
            "INFO:tensorflow:Wrote example 154 to file\n",
            "INFO:tensorflow:Wrote example 155 to file\n",
            "INFO:tensorflow:Wrote example 156 to file\n",
            "INFO:tensorflow:Wrote example 157 to file\n",
            "INFO:tensorflow:Wrote example 158 to file\n",
            "INFO:tensorflow:Wrote example 159 to file\n",
            "INFO:tensorflow:Wrote example 160 to file\n",
            "INFO:tensorflow:Wrote example 161 to file\n",
            "INFO:tensorflow:Wrote example 162 to file\n",
            "INFO:tensorflow:Wrote example 163 to file\n",
            "INFO:tensorflow:Wrote example 164 to file\n",
            "INFO:tensorflow:Wrote example 165 to file\n",
            "INFO:tensorflow:Wrote example 166 to file\n",
            "INFO:tensorflow:Wrote example 167 to file\n",
            "INFO:tensorflow:Wrote example 168 to file\n",
            "INFO:tensorflow:Wrote example 169 to file\n",
            "INFO:tensorflow:Wrote example 170 to file\n",
            "INFO:tensorflow:Wrote example 171 to file\n",
            "INFO:tensorflow:Wrote example 172 to file\n",
            "INFO:tensorflow:Wrote example 173 to file\n",
            "INFO:tensorflow:Wrote example 174 to file\n",
            "INFO:tensorflow:Wrote example 175 to file\n",
            "INFO:tensorflow:Wrote example 176 to file\n",
            "INFO:tensorflow:Wrote example 177 to file\n",
            "INFO:tensorflow:Wrote example 178 to file\n",
            "INFO:tensorflow:Wrote example 179 to file\n",
            "INFO:tensorflow:Wrote example 180 to file\n",
            "INFO:tensorflow:Wrote example 181 to file\n",
            "INFO:tensorflow:Wrote example 182 to file\n",
            "INFO:tensorflow:Wrote example 183 to file\n",
            "INFO:tensorflow:Wrote example 184 to file\n",
            "INFO:tensorflow:Wrote example 185 to file\n",
            "INFO:tensorflow:Wrote example 186 to file\n",
            "INFO:tensorflow:Wrote example 187 to file\n",
            "INFO:tensorflow:Wrote example 188 to file\n",
            "INFO:tensorflow:Wrote example 189 to file\n",
            "INFO:tensorflow:Wrote example 190 to file\n",
            "INFO:tensorflow:Wrote example 191 to file\n",
            "INFO:tensorflow:Wrote example 192 to file\n",
            "INFO:tensorflow:Wrote example 193 to file\n",
            "INFO:tensorflow:Wrote example 194 to file\n",
            "INFO:tensorflow:Wrote example 195 to file\n",
            "INFO:tensorflow:Wrote example 196 to file\n",
            "INFO:tensorflow:Wrote example 197 to file\n",
            "INFO:tensorflow:Wrote example 198 to file\n",
            "INFO:tensorflow:Wrote example 199 to file\n",
            "INFO:tensorflow:Wrote example 200 to file\n",
            "INFO:tensorflow:Wrote example 201 to file\n",
            "INFO:tensorflow:Wrote example 202 to file\n",
            "INFO:tensorflow:Wrote example 203 to file\n",
            "INFO:tensorflow:Wrote example 204 to file\n",
            "INFO:tensorflow:Wrote example 205 to file\n",
            "INFO:tensorflow:Wrote example 206 to file\n",
            "INFO:tensorflow:Wrote example 207 to file\n",
            "INFO:tensorflow:Wrote example 208 to file\n",
            "INFO:tensorflow:Wrote example 209 to file\n",
            "INFO:tensorflow:Wrote example 210 to file\n",
            "INFO:tensorflow:Wrote example 211 to file\n",
            "INFO:tensorflow:Wrote example 212 to file\n",
            "INFO:tensorflow:Wrote example 213 to file\n",
            "INFO:tensorflow:Wrote example 214 to file\n",
            "INFO:tensorflow:Wrote example 215 to file\n",
            "INFO:tensorflow:Wrote example 216 to file\n",
            "INFO:tensorflow:Wrote example 217 to file\n",
            "INFO:tensorflow:Wrote example 218 to file\n",
            "INFO:tensorflow:Wrote example 219 to file\n",
            "INFO:tensorflow:Wrote example 220 to file\n",
            "INFO:tensorflow:Wrote example 221 to file\n",
            "INFO:tensorflow:Wrote example 222 to file\n",
            "INFO:tensorflow:Wrote example 223 to file\n",
            "INFO:tensorflow:Wrote example 224 to file\n",
            "INFO:tensorflow:Wrote example 225 to file\n",
            "INFO:tensorflow:Wrote example 226 to file\n",
            "INFO:tensorflow:Wrote example 227 to file\n",
            "INFO:tensorflow:Wrote example 228 to file\n",
            "INFO:tensorflow:Wrote example 229 to file\n",
            "INFO:tensorflow:Wrote example 230 to file\n",
            "INFO:tensorflow:Wrote example 231 to file\n",
            "INFO:tensorflow:Wrote example 232 to file\n",
            "INFO:tensorflow:Wrote example 233 to file\n",
            "INFO:tensorflow:Wrote example 234 to file\n",
            "INFO:tensorflow:Wrote example 235 to file\n",
            "INFO:tensorflow:Wrote example 236 to file\n",
            "INFO:tensorflow:Wrote example 237 to file\n",
            "INFO:tensorflow:Wrote example 238 to file\n",
            "INFO:tensorflow:Wrote example 239 to file\n",
            "INFO:tensorflow:Wrote example 240 to file\n",
            "INFO:tensorflow:Wrote example 241 to file\n",
            "INFO:tensorflow:Wrote example 242 to file\n",
            "INFO:tensorflow:Wrote example 243 to file\n",
            "INFO:tensorflow:Wrote example 244 to file\n",
            "INFO:tensorflow:Wrote example 245 to file\n",
            "INFO:tensorflow:Wrote example 246 to file\n",
            "INFO:tensorflow:Wrote example 247 to file\n",
            "INFO:tensorflow:Wrote example 248 to file\n",
            "INFO:tensorflow:Wrote example 249 to file\n",
            "INFO:tensorflow:Wrote example 250 to file\n",
            "INFO:tensorflow:Wrote example 251 to file\n",
            "INFO:tensorflow:Wrote example 252 to file\n",
            "INFO:tensorflow:Wrote example 253 to file\n",
            "INFO:tensorflow:Wrote example 254 to file\n",
            "INFO:tensorflow:Wrote example 255 to file\n",
            "INFO:tensorflow:Wrote example 256 to file\n",
            "INFO:tensorflow:Wrote example 257 to file\n",
            "INFO:tensorflow:Wrote example 258 to file\n",
            "INFO:tensorflow:Wrote example 259 to file\n",
            "INFO:tensorflow:Wrote example 260 to file\n",
            "INFO:tensorflow:Wrote example 261 to file\n",
            "INFO:tensorflow:Wrote example 262 to file\n",
            "INFO:tensorflow:Wrote example 263 to file\n",
            "INFO:tensorflow:Wrote example 264 to file\n",
            "INFO:tensorflow:Wrote example 265 to file\n",
            "INFO:tensorflow:Wrote example 266 to file\n",
            "INFO:tensorflow:Wrote example 267 to file\n",
            "INFO:tensorflow:Wrote example 268 to file\n",
            "INFO:tensorflow:Wrote example 269 to file\n",
            "INFO:tensorflow:Wrote example 270 to file\n",
            "INFO:tensorflow:Wrote example 271 to file\n",
            "INFO:tensorflow:Wrote example 272 to file\n",
            "INFO:tensorflow:Wrote example 273 to file\n",
            "INFO:tensorflow:Wrote example 274 to file\n",
            "INFO:tensorflow:Wrote example 275 to file\n",
            "INFO:tensorflow:Wrote example 276 to file\n",
            "INFO:tensorflow:Wrote example 277 to file\n",
            "INFO:tensorflow:Wrote example 278 to file\n",
            "INFO:tensorflow:Wrote example 279 to file\n",
            "INFO:tensorflow:Wrote example 280 to file\n",
            "INFO:tensorflow:Wrote example 281 to file\n",
            "INFO:tensorflow:Wrote example 282 to file\n",
            "INFO:tensorflow:Wrote example 283 to file\n",
            "INFO:tensorflow:Wrote example 284 to file\n",
            "INFO:tensorflow:Wrote example 285 to file\n",
            "INFO:tensorflow:Wrote example 286 to file\n",
            "INFO:tensorflow:Wrote example 287 to file\n",
            "INFO:tensorflow:Wrote example 288 to file\n",
            "INFO:tensorflow:Wrote example 289 to file\n",
            "INFO:tensorflow:Wrote example 290 to file\n",
            "INFO:tensorflow:Wrote example 291 to file\n",
            "INFO:tensorflow:Wrote example 292 to file\n",
            "INFO:tensorflow:Wrote example 293 to file\n",
            "INFO:tensorflow:Wrote example 294 to file\n",
            "INFO:tensorflow:Wrote example 295 to file\n",
            "INFO:tensorflow:Wrote example 296 to file\n",
            "INFO:tensorflow:Wrote example 297 to file\n",
            "INFO:tensorflow:Wrote example 298 to file\n",
            "INFO:tensorflow:Wrote example 299 to file\n",
            "INFO:tensorflow:Wrote example 300 to file\n",
            "INFO:tensorflow:Wrote example 301 to file\n",
            "INFO:tensorflow:Wrote example 302 to file\n",
            "INFO:tensorflow:Wrote example 303 to file\n",
            "INFO:tensorflow:Wrote example 304 to file\n",
            "INFO:tensorflow:Wrote example 305 to file\n",
            "INFO:tensorflow:Wrote example 306 to file\n",
            "INFO:tensorflow:Wrote example 307 to file\n",
            "INFO:tensorflow:Wrote example 308 to file\n",
            "INFO:tensorflow:Wrote example 309 to file\n",
            "INFO:tensorflow:Wrote example 310 to file\n",
            "INFO:tensorflow:Wrote example 311 to file\n",
            "INFO:tensorflow:Wrote example 312 to file\n",
            "INFO:tensorflow:Wrote example 313 to file\n",
            "INFO:tensorflow:Wrote example 314 to file\n",
            "INFO:tensorflow:Wrote example 315 to file\n",
            "INFO:tensorflow:Wrote example 316 to file\n",
            "INFO:tensorflow:Wrote example 317 to file\n",
            "INFO:tensorflow:Wrote example 318 to file\n",
            "INFO:tensorflow:Wrote example 319 to file\n",
            "INFO:tensorflow:Wrote example 320 to file\n",
            "INFO:tensorflow:Wrote example 321 to file\n",
            "INFO:tensorflow:Wrote example 322 to file\n",
            "INFO:tensorflow:Wrote example 323 to file\n",
            "INFO:tensorflow:Wrote example 324 to file\n",
            "INFO:tensorflow:Wrote example 325 to file\n",
            "INFO:tensorflow:Wrote example 326 to file\n",
            "INFO:tensorflow:Wrote example 327 to file\n",
            "INFO:tensorflow:Wrote example 328 to file\n",
            "INFO:tensorflow:Wrote example 329 to file\n",
            "INFO:tensorflow:Wrote example 330 to file\n",
            "INFO:tensorflow:Wrote example 331 to file\n",
            "INFO:tensorflow:Wrote example 332 to file\n",
            "INFO:tensorflow:Wrote example 333 to file\n",
            "INFO:tensorflow:Wrote example 334 to file\n",
            "INFO:tensorflow:Wrote example 335 to file\n",
            "INFO:tensorflow:Wrote example 336 to file\n",
            "INFO:tensorflow:Wrote example 337 to file\n",
            "INFO:tensorflow:Wrote example 338 to file\n",
            "INFO:tensorflow:Wrote example 339 to file\n",
            "INFO:tensorflow:Wrote example 340 to file\n",
            "INFO:tensorflow:Wrote example 341 to file\n",
            "INFO:tensorflow:Wrote example 342 to file\n",
            "INFO:tensorflow:Wrote example 343 to file\n",
            "INFO:tensorflow:Wrote example 344 to file\n",
            "INFO:tensorflow:Wrote example 345 to file\n",
            "INFO:tensorflow:Wrote example 346 to file\n",
            "INFO:tensorflow:Wrote example 347 to file\n",
            "INFO:tensorflow:Wrote example 348 to file\n",
            "INFO:tensorflow:Wrote example 349 to file\n",
            "INFO:tensorflow:Wrote example 350 to file\n",
            "INFO:tensorflow:Wrote example 351 to file\n",
            "INFO:tensorflow:Wrote example 352 to file\n",
            "INFO:tensorflow:Wrote example 353 to file\n",
            "INFO:tensorflow:Wrote example 354 to file\n",
            "INFO:tensorflow:Wrote example 355 to file\n",
            "INFO:tensorflow:Wrote example 356 to file\n",
            "INFO:tensorflow:Wrote example 357 to file\n",
            "INFO:tensorflow:Wrote example 358 to file\n",
            "INFO:tensorflow:Wrote example 359 to file\n",
            "INFO:tensorflow:Wrote example 360 to file\n",
            "INFO:tensorflow:Wrote example 361 to file\n",
            "INFO:tensorflow:Wrote example 362 to file\n",
            "INFO:tensorflow:Wrote example 363 to file\n",
            "INFO:tensorflow:Wrote example 364 to file\n",
            "INFO:tensorflow:Wrote example 365 to file\n",
            "INFO:tensorflow:Wrote example 366 to file\n",
            "INFO:tensorflow:Wrote example 367 to file\n",
            "INFO:tensorflow:Wrote example 368 to file\n",
            "INFO:tensorflow:Wrote example 369 to file\n",
            "INFO:tensorflow:Wrote example 370 to file\n",
            "INFO:tensorflow:Wrote example 371 to file\n",
            "INFO:tensorflow:Wrote example 372 to file\n",
            "INFO:tensorflow:Wrote example 373 to file\n",
            "INFO:tensorflow:Wrote example 374 to file\n",
            "INFO:tensorflow:Wrote example 375 to file\n",
            "INFO:tensorflow:Wrote example 376 to file\n",
            "INFO:tensorflow:Wrote example 377 to file\n",
            "INFO:tensorflow:Wrote example 378 to file\n",
            "INFO:tensorflow:Wrote example 379 to file\n",
            "INFO:tensorflow:Wrote example 380 to file\n",
            "INFO:tensorflow:Wrote example 381 to file\n",
            "INFO:tensorflow:Wrote example 382 to file\n",
            "INFO:tensorflow:Wrote example 383 to file\n",
            "INFO:tensorflow:Wrote example 384 to file\n",
            "INFO:tensorflow:Wrote example 385 to file\n",
            "INFO:tensorflow:Wrote example 386 to file\n",
            "INFO:tensorflow:Wrote example 387 to file\n",
            "INFO:tensorflow:Wrote example 388 to file\n",
            "INFO:tensorflow:Wrote example 389 to file\n",
            "INFO:tensorflow:Wrote example 390 to file\n",
            "INFO:tensorflow:Wrote example 391 to file\n",
            "INFO:tensorflow:Wrote example 392 to file\n",
            "INFO:tensorflow:Wrote example 393 to file\n",
            "INFO:tensorflow:Wrote example 394 to file\n",
            "INFO:tensorflow:Wrote example 395 to file\n",
            "INFO:tensorflow:Wrote example 396 to file\n",
            "INFO:tensorflow:Wrote example 397 to file\n",
            "INFO:tensorflow:Wrote example 398 to file\n",
            "INFO:tensorflow:Wrote example 399 to file\n",
            "INFO:tensorflow:Wrote example 400 to file\n",
            "INFO:tensorflow:Wrote example 401 to file\n",
            "INFO:tensorflow:Wrote example 402 to file\n",
            "INFO:tensorflow:Wrote example 403 to file\n",
            "INFO:tensorflow:Wrote example 404 to file\n",
            "INFO:tensorflow:Wrote example 405 to file\n",
            "INFO:tensorflow:Wrote example 406 to file\n",
            "INFO:tensorflow:Wrote example 407 to file\n",
            "INFO:tensorflow:Wrote example 408 to file\n",
            "INFO:tensorflow:Wrote example 409 to file\n",
            "INFO:tensorflow:Wrote example 410 to file\n",
            "INFO:tensorflow:Wrote example 411 to file\n",
            "INFO:tensorflow:Wrote example 412 to file\n",
            "INFO:tensorflow:Wrote example 413 to file\n",
            "INFO:tensorflow:Wrote example 414 to file\n",
            "INFO:tensorflow:Wrote example 415 to file\n",
            "INFO:tensorflow:Wrote example 416 to file\n",
            "INFO:tensorflow:Wrote example 417 to file\n",
            "INFO:tensorflow:Wrote example 418 to file\n",
            "INFO:tensorflow:Wrote example 419 to file\n",
            "INFO:tensorflow:Wrote example 420 to file\n",
            "INFO:tensorflow:Wrote example 421 to file\n",
            "INFO:tensorflow:Wrote example 422 to file\n",
            "INFO:tensorflow:Wrote example 423 to file\n",
            "INFO:tensorflow:Wrote example 424 to file\n",
            "INFO:tensorflow:Wrote example 425 to file\n",
            "INFO:tensorflow:Wrote example 426 to file\n",
            "INFO:tensorflow:Wrote example 427 to file\n",
            "INFO:tensorflow:Wrote example 428 to file\n",
            "INFO:tensorflow:Wrote example 429 to file\n",
            "INFO:tensorflow:Wrote example 430 to file\n",
            "INFO:tensorflow:Wrote example 431 to file\n",
            "INFO:tensorflow:Wrote example 432 to file\n",
            "INFO:tensorflow:Wrote example 433 to file\n",
            "INFO:tensorflow:Wrote example 434 to file\n",
            "INFO:tensorflow:Wrote example 435 to file\n",
            "INFO:tensorflow:Wrote example 436 to file\n",
            "INFO:tensorflow:Wrote example 437 to file\n",
            "INFO:tensorflow:Wrote example 438 to file\n",
            "INFO:tensorflow:Wrote example 439 to file\n",
            "INFO:tensorflow:Wrote example 440 to file\n",
            "INFO:tensorflow:Wrote example 441 to file\n",
            "INFO:tensorflow:Wrote example 442 to file\n",
            "INFO:tensorflow:Wrote example 443 to file\n",
            "INFO:tensorflow:Wrote example 444 to file\n",
            "INFO:tensorflow:Wrote example 445 to file\n",
            "INFO:tensorflow:Wrote example 446 to file\n",
            "INFO:tensorflow:Wrote example 447 to file\n",
            "INFO:tensorflow:Wrote example 448 to file\n",
            "INFO:tensorflow:Wrote example 449 to file\n",
            "INFO:tensorflow:Wrote example 450 to file\n",
            "INFO:tensorflow:Wrote example 451 to file\n",
            "INFO:tensorflow:Wrote example 452 to file\n",
            "INFO:tensorflow:Wrote example 453 to file\n",
            "INFO:tensorflow:Wrote example 454 to file\n",
            "INFO:tensorflow:Wrote example 455 to file\n",
            "INFO:tensorflow:Wrote example 456 to file\n",
            "INFO:tensorflow:Wrote example 457 to file\n",
            "INFO:tensorflow:Wrote example 458 to file\n",
            "INFO:tensorflow:Wrote example 459 to file\n",
            "INFO:tensorflow:Wrote example 460 to file\n",
            "INFO:tensorflow:Wrote example 461 to file\n",
            "INFO:tensorflow:Wrote example 462 to file\n",
            "INFO:tensorflow:Wrote example 463 to file\n",
            "INFO:tensorflow:Wrote example 464 to file\n",
            "INFO:tensorflow:Wrote example 465 to file\n",
            "INFO:tensorflow:Wrote example 466 to file\n",
            "INFO:tensorflow:Wrote example 467 to file\n",
            "INFO:tensorflow:Wrote example 468 to file\n",
            "INFO:tensorflow:Wrote example 469 to file\n",
            "INFO:tensorflow:Wrote example 470 to file\n",
            "INFO:tensorflow:Wrote example 471 to file\n",
            "INFO:tensorflow:Wrote example 472 to file\n",
            "INFO:tensorflow:Wrote example 473 to file\n",
            "INFO:tensorflow:Wrote example 474 to file\n",
            "INFO:tensorflow:Wrote example 475 to file\n",
            "INFO:tensorflow:Wrote example 476 to file\n",
            "INFO:tensorflow:Wrote example 477 to file\n",
            "INFO:tensorflow:Wrote example 478 to file\n",
            "INFO:tensorflow:Wrote example 479 to file\n",
            "INFO:tensorflow:Wrote example 480 to file\n",
            "INFO:tensorflow:Wrote example 481 to file\n",
            "INFO:tensorflow:Wrote example 482 to file\n",
            "INFO:tensorflow:Wrote example 483 to file\n",
            "INFO:tensorflow:Wrote example 484 to file\n",
            "INFO:tensorflow:Wrote example 485 to file\n",
            "INFO:tensorflow:Wrote example 486 to file\n",
            "INFO:tensorflow:Wrote example 487 to file\n",
            "INFO:tensorflow:Wrote example 488 to file\n",
            "INFO:tensorflow:Wrote example 489 to file\n",
            "INFO:tensorflow:Wrote example 490 to file\n",
            "INFO:tensorflow:Wrote example 491 to file\n",
            "INFO:tensorflow:Wrote example 492 to file\n",
            "INFO:tensorflow:Wrote example 493 to file\n",
            "INFO:tensorflow:Wrote example 494 to file\n",
            "INFO:tensorflow:Wrote example 495 to file\n",
            "INFO:tensorflow:Wrote example 496 to file\n",
            "INFO:tensorflow:Wrote example 497 to file\n",
            "INFO:tensorflow:Wrote example 498 to file\n",
            "INFO:tensorflow:Wrote example 499 to file\n",
            "INFO:tensorflow:Wrote example 500 to file\n",
            "INFO:tensorflow:Wrote example 501 to file\n",
            "INFO:tensorflow:Wrote example 502 to file\n",
            "INFO:tensorflow:Wrote example 503 to file\n",
            "INFO:tensorflow:Wrote example 504 to file\n",
            "INFO:tensorflow:Wrote example 505 to file\n",
            "INFO:tensorflow:Wrote example 506 to file\n",
            "INFO:tensorflow:Wrote example 507 to file\n",
            "INFO:tensorflow:Wrote example 508 to file\n",
            "INFO:tensorflow:Wrote example 509 to file\n",
            "INFO:tensorflow:Wrote example 510 to file\n",
            "INFO:tensorflow:Wrote example 511 to file\n",
            "INFO:tensorflow:Wrote example 512 to file\n",
            "INFO:tensorflow:Wrote example 513 to file\n",
            "INFO:tensorflow:Wrote example 514 to file\n",
            "INFO:tensorflow:Wrote example 515 to file\n",
            "INFO:tensorflow:Wrote example 516 to file\n",
            "INFO:tensorflow:Wrote example 517 to file\n",
            "INFO:tensorflow:Wrote example 518 to file\n",
            "INFO:tensorflow:Wrote example 519 to file\n",
            "INFO:tensorflow:Wrote example 520 to file\n",
            "INFO:tensorflow:Wrote example 521 to file\n",
            "INFO:tensorflow:Wrote example 522 to file\n",
            "INFO:tensorflow:Wrote example 523 to file\n",
            "INFO:tensorflow:Wrote example 524 to file\n",
            "INFO:tensorflow:Wrote example 525 to file\n",
            "INFO:tensorflow:Wrote example 526 to file\n",
            "INFO:tensorflow:Wrote example 527 to file\n",
            "INFO:tensorflow:Wrote example 528 to file\n",
            "INFO:tensorflow:Wrote example 529 to file\n",
            "INFO:tensorflow:Wrote example 530 to file\n",
            "INFO:tensorflow:Wrote example 531 to file\n",
            "INFO:tensorflow:Wrote example 532 to file\n",
            "INFO:tensorflow:Wrote example 533 to file\n",
            "INFO:tensorflow:Wrote example 534 to file\n",
            "INFO:tensorflow:Wrote example 535 to file\n",
            "INFO:tensorflow:Wrote example 536 to file\n",
            "INFO:tensorflow:Wrote example 537 to file\n",
            "INFO:tensorflow:Wrote example 538 to file\n",
            "INFO:tensorflow:Wrote example 539 to file\n",
            "INFO:tensorflow:Wrote example 540 to file\n",
            "INFO:tensorflow:Wrote example 541 to file\n",
            "INFO:tensorflow:Wrote example 542 to file\n",
            "INFO:tensorflow:Wrote example 543 to file\n",
            "INFO:tensorflow:Wrote example 544 to file\n",
            "INFO:tensorflow:Wrote example 545 to file\n",
            "INFO:tensorflow:Wrote example 546 to file\n",
            "INFO:tensorflow:Wrote example 547 to file\n",
            "INFO:tensorflow:Wrote example 548 to file\n",
            "INFO:tensorflow:Wrote example 549 to file\n",
            "INFO:tensorflow:Wrote example 550 to file\n",
            "INFO:tensorflow:Wrote example 551 to file\n",
            "INFO:tensorflow:Wrote example 552 to file\n",
            "INFO:tensorflow:Wrote example 553 to file\n",
            "INFO:tensorflow:Wrote example 554 to file\n",
            "INFO:tensorflow:Wrote example 555 to file\n",
            "INFO:tensorflow:Wrote example 556 to file\n",
            "INFO:tensorflow:Wrote example 557 to file\n",
            "INFO:tensorflow:Wrote example 558 to file\n",
            "INFO:tensorflow:Wrote example 559 to file\n",
            "INFO:tensorflow:Wrote example 560 to file\n",
            "INFO:tensorflow:Wrote example 561 to file\n",
            "INFO:tensorflow:Wrote example 562 to file\n",
            "INFO:tensorflow:Wrote example 563 to file\n",
            "INFO:tensorflow:Wrote example 564 to file\n",
            "INFO:tensorflow:Wrote example 565 to file\n",
            "INFO:tensorflow:Wrote example 566 to file\n",
            "INFO:tensorflow:Wrote example 567 to file\n",
            "INFO:tensorflow:Wrote example 568 to file\n",
            "INFO:tensorflow:Wrote example 569 to file\n",
            "INFO:tensorflow:Wrote example 570 to file\n",
            "INFO:tensorflow:Wrote example 571 to file\n",
            "INFO:tensorflow:Wrote example 572 to file\n",
            "INFO:tensorflow:Wrote example 573 to file\n",
            "INFO:tensorflow:Wrote example 574 to file\n",
            "INFO:tensorflow:Wrote example 575 to file\n",
            "INFO:tensorflow:Wrote example 576 to file\n",
            "INFO:tensorflow:Wrote example 577 to file\n",
            "INFO:tensorflow:Wrote example 578 to file\n",
            "INFO:tensorflow:Wrote example 579 to file\n",
            "INFO:tensorflow:Wrote example 580 to file\n",
            "INFO:tensorflow:Wrote example 581 to file\n",
            "INFO:tensorflow:Wrote example 582 to file\n",
            "INFO:tensorflow:Wrote example 583 to file\n",
            "INFO:tensorflow:Wrote example 584 to file\n",
            "INFO:tensorflow:Wrote example 585 to file\n",
            "INFO:tensorflow:Wrote example 586 to file\n",
            "INFO:tensorflow:Wrote example 587 to file\n",
            "INFO:tensorflow:Wrote example 588 to file\n",
            "INFO:tensorflow:Wrote example 589 to file\n",
            "INFO:tensorflow:Wrote example 590 to file\n",
            "INFO:tensorflow:Wrote example 591 to file\n",
            "INFO:tensorflow:Wrote example 592 to file\n",
            "INFO:tensorflow:Wrote example 593 to file\n",
            "INFO:tensorflow:Wrote example 594 to file\n",
            "INFO:tensorflow:Wrote example 595 to file\n",
            "INFO:tensorflow:Wrote example 596 to file\n",
            "INFO:tensorflow:Wrote example 597 to file\n",
            "INFO:tensorflow:Wrote example 598 to file\n",
            "INFO:tensorflow:Wrote example 599 to file\n",
            "INFO:tensorflow:Wrote example 600 to file\n",
            "INFO:tensorflow:Wrote example 601 to file\n",
            "INFO:tensorflow:Wrote example 602 to file\n",
            "INFO:tensorflow:Wrote example 603 to file\n",
            "INFO:tensorflow:Wrote example 604 to file\n",
            "INFO:tensorflow:Wrote example 605 to file\n",
            "INFO:tensorflow:Wrote example 606 to file\n",
            "INFO:tensorflow:Wrote example 607 to file\n",
            "INFO:tensorflow:Wrote example 608 to file\n",
            "INFO:tensorflow:Wrote example 609 to file\n",
            "INFO:tensorflow:Wrote example 610 to file\n",
            "INFO:tensorflow:Wrote example 611 to file\n",
            "INFO:tensorflow:Wrote example 612 to file\n",
            "INFO:tensorflow:Wrote example 613 to file\n",
            "INFO:tensorflow:Wrote example 614 to file\n",
            "INFO:tensorflow:Wrote example 615 to file\n",
            "INFO:tensorflow:Wrote example 616 to file\n",
            "INFO:tensorflow:Wrote example 617 to file\n",
            "INFO:tensorflow:Wrote example 618 to file\n",
            "INFO:tensorflow:Wrote example 619 to file\n",
            "INFO:tensorflow:Wrote example 620 to file\n",
            "INFO:tensorflow:Wrote example 621 to file\n",
            "INFO:tensorflow:Wrote example 622 to file\n",
            "INFO:tensorflow:Wrote example 623 to file\n",
            "INFO:tensorflow:Wrote example 624 to file\n",
            "INFO:tensorflow:Wrote example 625 to file\n",
            "INFO:tensorflow:Wrote example 626 to file\n",
            "INFO:tensorflow:Wrote example 627 to file\n",
            "INFO:tensorflow:Wrote example 628 to file\n",
            "INFO:tensorflow:Wrote example 629 to file\n",
            "INFO:tensorflow:Wrote example 630 to file\n",
            "INFO:tensorflow:Wrote example 631 to file\n",
            "INFO:tensorflow:Wrote example 632 to file\n",
            "INFO:tensorflow:Wrote example 633 to file\n",
            "INFO:tensorflow:Wrote example 634 to file\n",
            "INFO:tensorflow:Wrote example 635 to file\n",
            "INFO:tensorflow:Wrote example 636 to file\n",
            "INFO:tensorflow:Wrote example 637 to file\n",
            "INFO:tensorflow:Wrote example 638 to file\n",
            "INFO:tensorflow:Wrote example 639 to file\n",
            "INFO:tensorflow:Wrote example 640 to file\n",
            "INFO:tensorflow:Wrote example 641 to file\n",
            "INFO:tensorflow:Wrote example 642 to file\n",
            "INFO:tensorflow:Wrote example 643 to file\n",
            "INFO:tensorflow:Wrote example 644 to file\n",
            "INFO:tensorflow:Wrote example 645 to file\n",
            "INFO:tensorflow:Wrote example 646 to file\n",
            "INFO:tensorflow:Wrote example 647 to file\n",
            "INFO:tensorflow:Wrote example 648 to file\n",
            "INFO:tensorflow:Wrote example 649 to file\n",
            "INFO:tensorflow:Wrote example 650 to file\n",
            "INFO:tensorflow:Wrote example 651 to file\n",
            "INFO:tensorflow:Wrote example 652 to file\n",
            "INFO:tensorflow:Wrote example 653 to file\n",
            "INFO:tensorflow:Wrote example 654 to file\n",
            "INFO:tensorflow:Wrote example 655 to file\n",
            "INFO:tensorflow:Wrote example 656 to file\n",
            "INFO:tensorflow:Wrote example 657 to file\n",
            "INFO:tensorflow:Wrote example 658 to file\n",
            "INFO:tensorflow:Wrote example 659 to file\n",
            "INFO:tensorflow:Wrote example 660 to file\n",
            "INFO:tensorflow:Wrote example 661 to file\n",
            "INFO:tensorflow:Wrote example 662 to file\n",
            "INFO:tensorflow:Wrote example 663 to file\n",
            "INFO:tensorflow:Wrote example 664 to file\n",
            "INFO:tensorflow:Wrote example 665 to file\n",
            "INFO:tensorflow:Wrote example 666 to file\n",
            "INFO:tensorflow:Wrote example 667 to file\n",
            "INFO:tensorflow:Wrote example 668 to file\n",
            "INFO:tensorflow:Wrote example 669 to file\n",
            "INFO:tensorflow:Wrote example 670 to file\n",
            "INFO:tensorflow:Wrote example 671 to file\n",
            "INFO:tensorflow:Wrote example 672 to file\n",
            "INFO:tensorflow:Wrote example 673 to file\n",
            "INFO:tensorflow:Wrote example 674 to file\n",
            "INFO:tensorflow:Wrote example 675 to file\n",
            "INFO:tensorflow:Wrote example 676 to file\n",
            "INFO:tensorflow:Wrote example 677 to file\n",
            "INFO:tensorflow:Wrote example 678 to file\n",
            "INFO:tensorflow:Wrote example 679 to file\n",
            "INFO:tensorflow:Wrote example 680 to file\n",
            "INFO:tensorflow:Wrote example 681 to file\n",
            "INFO:tensorflow:Wrote example 682 to file\n",
            "INFO:tensorflow:Wrote example 683 to file\n",
            "INFO:tensorflow:Wrote example 684 to file\n",
            "INFO:tensorflow:Wrote example 685 to file\n",
            "INFO:tensorflow:Wrote example 686 to file\n",
            "INFO:tensorflow:Wrote example 687 to file\n",
            "INFO:tensorflow:Wrote example 688 to file\n",
            "INFO:tensorflow:Wrote example 689 to file\n",
            "INFO:tensorflow:Wrote example 690 to file\n",
            "INFO:tensorflow:Wrote example 691 to file\n",
            "INFO:tensorflow:Wrote example 692 to file\n",
            "INFO:tensorflow:Wrote example 693 to file\n",
            "INFO:tensorflow:Wrote example 694 to file\n",
            "INFO:tensorflow:Wrote example 695 to file\n",
            "INFO:tensorflow:Wrote example 696 to file\n",
            "INFO:tensorflow:Wrote example 697 to file\n",
            "INFO:tensorflow:Wrote example 698 to file\n",
            "INFO:tensorflow:Wrote example 699 to file\n",
            "INFO:tensorflow:Wrote example 700 to file\n",
            "INFO:tensorflow:Wrote example 701 to file\n",
            "INFO:tensorflow:Wrote example 702 to file\n",
            "INFO:tensorflow:Wrote example 703 to file\n",
            "INFO:tensorflow:Wrote example 704 to file\n",
            "INFO:tensorflow:Wrote example 705 to file\n",
            "INFO:tensorflow:Wrote example 706 to file\n",
            "INFO:tensorflow:Wrote example 707 to file\n",
            "INFO:tensorflow:Wrote example 708 to file\n",
            "INFO:tensorflow:Wrote example 709 to file\n",
            "INFO:tensorflow:Wrote example 710 to file\n",
            "INFO:tensorflow:Wrote example 711 to file\n",
            "INFO:tensorflow:Wrote example 712 to file\n",
            "INFO:tensorflow:Wrote example 713 to file\n",
            "INFO:tensorflow:Wrote example 714 to file\n",
            "INFO:tensorflow:Wrote example 715 to file\n",
            "INFO:tensorflow:Wrote example 716 to file\n",
            "INFO:tensorflow:Wrote example 717 to file\n",
            "INFO:tensorflow:Wrote example 718 to file\n",
            "INFO:tensorflow:Wrote example 719 to file\n",
            "INFO:tensorflow:Wrote example 720 to file\n",
            "INFO:tensorflow:Wrote example 721 to file\n",
            "INFO:tensorflow:Wrote example 722 to file\n",
            "INFO:tensorflow:Wrote example 723 to file\n",
            "INFO:tensorflow:Wrote example 724 to file\n",
            "INFO:tensorflow:Wrote example 725 to file\n",
            "INFO:tensorflow:Wrote example 726 to file\n",
            "INFO:tensorflow:Wrote example 727 to file\n",
            "INFO:tensorflow:Wrote example 728 to file\n",
            "INFO:tensorflow:Wrote example 729 to file\n",
            "INFO:tensorflow:Wrote example 730 to file\n",
            "INFO:tensorflow:Wrote example 731 to file\n",
            "INFO:tensorflow:Wrote example 732 to file\n",
            "INFO:tensorflow:Wrote example 733 to file\n",
            "INFO:tensorflow:Wrote example 734 to file\n",
            "INFO:tensorflow:Wrote example 735 to file\n",
            "INFO:tensorflow:Wrote example 736 to file\n",
            "INFO:tensorflow:Wrote example 737 to file\n",
            "INFO:tensorflow:Wrote example 738 to file\n",
            "INFO:tensorflow:Wrote example 739 to file\n",
            "INFO:tensorflow:Wrote example 740 to file\n",
            "INFO:tensorflow:Wrote example 741 to file\n",
            "INFO:tensorflow:Wrote example 742 to file\n",
            "INFO:tensorflow:Wrote example 743 to file\n",
            "INFO:tensorflow:Wrote example 744 to file\n",
            "INFO:tensorflow:Wrote example 745 to file\n",
            "INFO:tensorflow:Wrote example 746 to file\n",
            "INFO:tensorflow:Wrote example 747 to file\n",
            "INFO:tensorflow:Wrote example 748 to file\n",
            "INFO:tensorflow:Wrote example 749 to file\n",
            "INFO:tensorflow:Wrote example 750 to file\n",
            "INFO:tensorflow:Wrote example 751 to file\n",
            "INFO:tensorflow:Wrote example 752 to file\n",
            "INFO:tensorflow:Wrote example 753 to file\n",
            "INFO:tensorflow:Wrote example 754 to file\n",
            "INFO:tensorflow:Wrote example 755 to file\n",
            "INFO:tensorflow:Wrote example 756 to file\n",
            "INFO:tensorflow:Wrote example 757 to file\n",
            "INFO:tensorflow:Wrote example 758 to file\n",
            "INFO:tensorflow:Wrote example 759 to file\n",
            "INFO:tensorflow:Wrote example 760 to file\n",
            "INFO:tensorflow:Wrote example 761 to file\n",
            "INFO:tensorflow:Wrote example 762 to file\n",
            "INFO:tensorflow:Wrote example 763 to file\n",
            "INFO:tensorflow:Wrote example 764 to file\n",
            "INFO:tensorflow:Wrote example 765 to file\n",
            "INFO:tensorflow:Wrote example 766 to file\n",
            "INFO:tensorflow:Wrote example 767 to file\n",
            "INFO:tensorflow:Wrote example 768 to file\n",
            "INFO:tensorflow:Wrote example 769 to file\n",
            "INFO:tensorflow:Wrote example 770 to file\n",
            "INFO:tensorflow:Wrote example 771 to file\n",
            "INFO:tensorflow:Wrote example 772 to file\n",
            "INFO:tensorflow:Wrote example 773 to file\n",
            "INFO:tensorflow:Wrote example 774 to file\n",
            "INFO:tensorflow:Wrote example 775 to file\n",
            "INFO:tensorflow:Wrote example 776 to file\n",
            "INFO:tensorflow:Wrote example 777 to file\n",
            "INFO:tensorflow:Wrote example 778 to file\n",
            "INFO:tensorflow:Wrote example 779 to file\n",
            "INFO:tensorflow:Wrote example 780 to file\n",
            "INFO:tensorflow:Wrote example 781 to file\n",
            "INFO:tensorflow:Wrote example 782 to file\n",
            "INFO:tensorflow:Wrote example 783 to file\n",
            "INFO:tensorflow:Wrote example 784 to file\n",
            "INFO:tensorflow:Wrote example 785 to file\n",
            "INFO:tensorflow:Wrote example 786 to file\n",
            "INFO:tensorflow:Wrote example 787 to file\n",
            "INFO:tensorflow:Wrote example 788 to file\n",
            "INFO:tensorflow:Wrote example 789 to file\n",
            "INFO:tensorflow:Wrote example 790 to file\n",
            "INFO:tensorflow:Wrote example 791 to file\n",
            "INFO:tensorflow:Wrote example 792 to file\n",
            "INFO:tensorflow:Wrote example 793 to file\n",
            "INFO:tensorflow:Wrote example 794 to file\n",
            "INFO:tensorflow:Wrote example 795 to file\n",
            "INFO:tensorflow:Wrote example 796 to file\n",
            "INFO:tensorflow:Wrote example 797 to file\n",
            "INFO:tensorflow:Wrote example 798 to file\n",
            "INFO:tensorflow:Wrote example 799 to file\n",
            "INFO:tensorflow:Wrote example 800 to file\n",
            "INFO:tensorflow:Wrote example 801 to file\n",
            "INFO:tensorflow:Wrote example 802 to file\n",
            "INFO:tensorflow:Wrote example 803 to file\n",
            "INFO:tensorflow:Wrote example 804 to file\n",
            "INFO:tensorflow:Wrote example 805 to file\n",
            "INFO:tensorflow:Wrote example 806 to file\n",
            "INFO:tensorflow:Wrote example 807 to file\n",
            "INFO:tensorflow:Wrote example 808 to file\n",
            "INFO:tensorflow:Wrote example 809 to file\n",
            "INFO:tensorflow:Wrote example 810 to file\n",
            "INFO:tensorflow:Wrote example 811 to file\n",
            "INFO:tensorflow:Wrote example 812 to file\n",
            "INFO:tensorflow:Wrote example 813 to file\n",
            "INFO:tensorflow:Wrote example 814 to file\n",
            "INFO:tensorflow:Wrote example 815 to file\n",
            "INFO:tensorflow:Wrote example 816 to file\n",
            "INFO:tensorflow:Wrote example 817 to file\n",
            "INFO:tensorflow:Wrote example 818 to file\n",
            "INFO:tensorflow:Wrote example 819 to file\n",
            "INFO:tensorflow:Wrote example 820 to file\n",
            "INFO:tensorflow:Wrote example 821 to file\n",
            "INFO:tensorflow:Wrote example 822 to file\n",
            "INFO:tensorflow:Wrote example 823 to file\n",
            "INFO:tensorflow:Wrote example 824 to file\n",
            "INFO:tensorflow:Wrote example 825 to file\n",
            "INFO:tensorflow:Wrote example 826 to file\n",
            "INFO:tensorflow:Wrote example 827 to file\n",
            "INFO:tensorflow:Wrote example 828 to file\n",
            "INFO:tensorflow:Wrote example 829 to file\n",
            "INFO:tensorflow:Wrote example 830 to file\n",
            "INFO:tensorflow:Wrote example 831 to file\n",
            "INFO:tensorflow:Wrote example 832 to file\n",
            "INFO:tensorflow:Wrote example 833 to file\n",
            "INFO:tensorflow:Wrote example 834 to file\n",
            "INFO:tensorflow:Wrote example 835 to file\n",
            "INFO:tensorflow:Wrote example 836 to file\n",
            "INFO:tensorflow:Wrote example 837 to file\n",
            "INFO:tensorflow:Wrote example 838 to file\n",
            "INFO:tensorflow:Wrote example 839 to file\n",
            "INFO:tensorflow:Wrote example 840 to file\n",
            "INFO:tensorflow:Wrote example 841 to file\n",
            "INFO:tensorflow:Wrote example 842 to file\n",
            "INFO:tensorflow:Wrote example 843 to file\n",
            "INFO:tensorflow:Wrote example 844 to file\n",
            "INFO:tensorflow:Wrote example 845 to file\n",
            "INFO:tensorflow:Wrote example 846 to file\n",
            "INFO:tensorflow:Wrote example 847 to file\n",
            "INFO:tensorflow:Wrote example 848 to file\n",
            "INFO:tensorflow:Wrote example 849 to file\n",
            "INFO:tensorflow:Wrote example 850 to file\n",
            "INFO:tensorflow:Wrote example 851 to file\n",
            "INFO:tensorflow:Wrote example 852 to file\n",
            "INFO:tensorflow:Wrote example 853 to file\n",
            "INFO:tensorflow:Wrote example 854 to file\n",
            "INFO:tensorflow:Wrote example 855 to file\n",
            "INFO:tensorflow:Wrote example 856 to file\n",
            "INFO:tensorflow:Wrote example 857 to file\n",
            "INFO:tensorflow:Wrote example 858 to file\n",
            "INFO:tensorflow:Wrote example 859 to file\n",
            "INFO:tensorflow:Wrote example 860 to file\n",
            "INFO:tensorflow:Wrote example 861 to file\n",
            "INFO:tensorflow:Wrote example 862 to file\n",
            "INFO:tensorflow:Wrote example 863 to file\n",
            "INFO:tensorflow:Wrote example 864 to file\n",
            "INFO:tensorflow:Wrote example 865 to file\n",
            "INFO:tensorflow:Wrote example 866 to file\n",
            "INFO:tensorflow:Wrote example 867 to file\n",
            "INFO:tensorflow:Wrote example 868 to file\n",
            "INFO:tensorflow:Wrote example 869 to file\n",
            "INFO:tensorflow:Wrote example 870 to file\n",
            "INFO:tensorflow:Wrote example 871 to file\n",
            "INFO:tensorflow:Wrote example 872 to file\n",
            "INFO:tensorflow:Wrote example 873 to file\n",
            "INFO:tensorflow:Wrote example 874 to file\n",
            "INFO:tensorflow:Wrote example 875 to file\n",
            "INFO:tensorflow:Wrote example 876 to file\n",
            "INFO:tensorflow:Wrote example 877 to file\n",
            "INFO:tensorflow:Wrote example 878 to file\n",
            "INFO:tensorflow:Wrote example 879 to file\n",
            "INFO:tensorflow:Wrote example 880 to file\n",
            "INFO:tensorflow:Wrote example 881 to file\n",
            "INFO:tensorflow:Wrote example 882 to file\n",
            "INFO:tensorflow:Wrote example 883 to file\n",
            "INFO:tensorflow:Wrote example 884 to file\n",
            "INFO:tensorflow:Wrote example 885 to file\n",
            "INFO:tensorflow:Wrote example 886 to file\n",
            "INFO:tensorflow:Wrote example 887 to file\n",
            "INFO:tensorflow:Wrote example 888 to file\n",
            "INFO:tensorflow:Wrote example 889 to file\n",
            "INFO:tensorflow:Wrote example 890 to file\n",
            "INFO:tensorflow:Wrote example 891 to file\n",
            "INFO:tensorflow:Wrote example 892 to file\n",
            "INFO:tensorflow:Wrote example 893 to file\n",
            "INFO:tensorflow:Wrote example 894 to file\n",
            "INFO:tensorflow:Wrote example 895 to file\n",
            "INFO:tensorflow:Wrote example 896 to file\n",
            "INFO:tensorflow:Wrote example 897 to file\n",
            "INFO:tensorflow:Wrote example 898 to file\n",
            "INFO:tensorflow:Wrote example 899 to file\n",
            "INFO:tensorflow:Wrote example 900 to file\n",
            "INFO:tensorflow:Wrote example 901 to file\n",
            "INFO:tensorflow:Wrote example 902 to file\n",
            "INFO:tensorflow:Wrote example 903 to file\n",
            "INFO:tensorflow:Wrote example 904 to file\n",
            "INFO:tensorflow:Wrote example 905 to file\n",
            "INFO:tensorflow:Wrote example 906 to file\n",
            "INFO:tensorflow:Wrote example 907 to file\n",
            "INFO:tensorflow:Wrote example 908 to file\n",
            "INFO:tensorflow:Wrote example 909 to file\n",
            "INFO:tensorflow:Wrote example 910 to file\n",
            "INFO:tensorflow:Wrote example 911 to file\n",
            "INFO:tensorflow:Wrote example 912 to file\n",
            "INFO:tensorflow:Wrote example 913 to file\n",
            "INFO:tensorflow:Wrote example 914 to file\n",
            "INFO:tensorflow:Wrote example 915 to file\n",
            "INFO:tensorflow:Wrote example 916 to file\n",
            "INFO:tensorflow:Wrote example 917 to file\n",
            "INFO:tensorflow:Wrote example 918 to file\n",
            "INFO:tensorflow:Wrote example 919 to file\n",
            "INFO:tensorflow:Wrote example 920 to file\n",
            "INFO:tensorflow:Wrote example 921 to file\n",
            "INFO:tensorflow:Wrote example 922 to file\n",
            "INFO:tensorflow:Wrote example 923 to file\n",
            "INFO:tensorflow:Wrote example 924 to file\n",
            "INFO:tensorflow:Wrote example 925 to file\n",
            "INFO:tensorflow:Wrote example 926 to file\n",
            "INFO:tensorflow:Wrote example 927 to file\n",
            "INFO:tensorflow:Wrote example 928 to file\n",
            "INFO:tensorflow:Wrote example 929 to file\n",
            "INFO:tensorflow:Wrote example 930 to file\n",
            "INFO:tensorflow:Wrote example 931 to file\n",
            "INFO:tensorflow:Wrote example 932 to file\n",
            "INFO:tensorflow:Wrote example 933 to file\n",
            "INFO:tensorflow:Wrote example 934 to file\n",
            "INFO:tensorflow:Wrote example 935 to file\n",
            "INFO:tensorflow:Wrote example 936 to file\n",
            "INFO:tensorflow:Wrote example 937 to file\n",
            "INFO:tensorflow:Wrote example 938 to file\n",
            "INFO:tensorflow:Wrote example 939 to file\n",
            "INFO:tensorflow:Wrote example 940 to file\n",
            "INFO:tensorflow:Wrote example 941 to file\n",
            "INFO:tensorflow:Wrote example 942 to file\n",
            "INFO:tensorflow:Wrote example 943 to file\n",
            "INFO:tensorflow:Wrote example 944 to file\n",
            "INFO:tensorflow:Wrote example 945 to file\n",
            "INFO:tensorflow:Wrote example 946 to file\n",
            "INFO:tensorflow:Wrote example 947 to file\n",
            "INFO:tensorflow:Wrote example 948 to file\n",
            "INFO:tensorflow:Wrote example 949 to file\n",
            "INFO:tensorflow:Wrote example 950 to file\n",
            "INFO:tensorflow:Wrote example 951 to file\n",
            "INFO:tensorflow:Wrote example 952 to file\n",
            "INFO:tensorflow:Wrote example 953 to file\n",
            "INFO:tensorflow:Wrote example 954 to file\n",
            "INFO:tensorflow:Wrote example 955 to file\n",
            "INFO:tensorflow:Wrote example 956 to file\n",
            "INFO:tensorflow:Wrote example 957 to file\n",
            "INFO:tensorflow:Wrote example 958 to file\n",
            "INFO:tensorflow:Wrote example 959 to file\n",
            "INFO:tensorflow:Wrote example 960 to file\n",
            "INFO:tensorflow:Wrote example 961 to file\n",
            "INFO:tensorflow:Wrote example 962 to file\n",
            "INFO:tensorflow:Wrote example 963 to file\n",
            "INFO:tensorflow:Wrote example 964 to file\n",
            "INFO:tensorflow:Wrote example 965 to file\n",
            "INFO:tensorflow:Wrote example 966 to file\n",
            "INFO:tensorflow:Wrote example 967 to file\n",
            "INFO:tensorflow:Wrote example 968 to file\n",
            "INFO:tensorflow:Wrote example 969 to file\n",
            "INFO:tensorflow:Wrote example 970 to file\n",
            "INFO:tensorflow:Wrote example 971 to file\n",
            "INFO:tensorflow:Wrote example 972 to file\n",
            "INFO:tensorflow:Wrote example 973 to file\n",
            "INFO:tensorflow:Wrote example 974 to file\n",
            "INFO:tensorflow:Wrote example 975 to file\n",
            "INFO:tensorflow:Wrote example 976 to file\n",
            "INFO:tensorflow:Wrote example 977 to file\n",
            "INFO:tensorflow:Wrote example 978 to file\n",
            "INFO:tensorflow:Wrote example 979 to file\n",
            "INFO:tensorflow:Wrote example 980 to file\n",
            "INFO:tensorflow:Wrote example 981 to file\n",
            "INFO:tensorflow:Wrote example 982 to file\n",
            "INFO:tensorflow:Wrote example 983 to file\n",
            "INFO:tensorflow:Wrote example 984 to file\n",
            "INFO:tensorflow:Wrote example 985 to file\n",
            "INFO:tensorflow:Wrote example 986 to file\n",
            "INFO:tensorflow:Wrote example 987 to file\n",
            "INFO:tensorflow:Wrote example 988 to file\n",
            "INFO:tensorflow:Wrote example 989 to file\n",
            "INFO:tensorflow:Wrote example 990 to file\n",
            "INFO:tensorflow:Wrote example 991 to file\n",
            "INFO:tensorflow:Wrote example 992 to file\n",
            "INFO:tensorflow:Wrote example 993 to file\n",
            "INFO:tensorflow:Wrote example 994 to file\n",
            "INFO:tensorflow:Wrote example 995 to file\n",
            "INFO:tensorflow:Wrote example 996 to file\n",
            "INFO:tensorflow:Wrote example 997 to file\n",
            "INFO:tensorflow:Wrote example 998 to file\n",
            "INFO:tensorflow:Wrote example 999 to file\n",
            "WARNING:tensorflow:Bucket input queue is empty when calling next_batch. Bucket queue size: 0, Input queue size: 0\n",
            "INFO:tensorflow:Finished reading dataset in single_pass mode.\n",
            "INFO:tensorflow:Decoder has finished reading dataset for single_pass.\n",
            "INFO:tensorflow:Output has been saved in /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode_val_300maxenc_8beam_50mindec_100maxdec_ckpt-15726test/reference and /content/drive/My Drive/MA_colab/PG_TR/logs/myexperiment/decode_val_300maxenc_8beam_50mindec_100maxdec_ckpt-15726test/decoded. Now starting ROUGE eval...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0d5dcf456bff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;31m#, \"Run in tensorflow's debug mode (watches for NaN/inf values)\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-30267193495a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummarizationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_model_hps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeamSearchDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# decode indefinitely (unless single_pass=True, in which case deocde the dataset exactly once)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The 'mode' flag must be one of train/eval/decode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-13113f15e14a>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Decoder has finished reading dataset for single_pass.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output has been saved in %s and %s. Now starting ROUGE eval...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rouge_ref_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rouge_dec_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mresults_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrouge_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rouge_ref_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rouge_dec_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mrouge_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-13113f15e14a>\u001b[0m in \u001b[0;36mrouge_eval\u001b[0;34m(ref_dir, dec_dir)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrouge_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m   \u001b[0;34m\"\"\"Evaluate the files in ref_dir and dec_dir with pyrouge, returning results_dict\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m   \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyrouge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRouge155\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m   \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_filename_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'#ID#_reference.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m   \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem_filename_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'(\\d+)_decoded.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyrouge/Rouge155.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rouge_dir, rouge_args)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_config_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_rouge_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouge_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__clean_rouge_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouge_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_filename_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyrouge/Rouge155.py\u001b[0m in \u001b[0;36m__set_rouge_dir\u001b[0;34m(self, home_dir)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \"\"\"\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhome_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_home_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_rouge_home_dir_from_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_home_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhome_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyrouge/Rouge155.py\u001b[0m in \u001b[0;36m__get_rouge_home_dir_from_settings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_rouge_home_dir_from_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfigParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read_file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.pyrouge/settings.ini'"
          ]
        }
      ]
    }
  ]
}