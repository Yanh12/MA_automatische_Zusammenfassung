{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P_G-data_preprocessing_model2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOAmpW1wLPPodlMLaF5pE5t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yanh12/MA_automatische_Zusammenfassung/blob/master/abstraktive_Phase/Model3/P_G_data_preprocessing_model2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e93mJysoy2lZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "071bcbec-92f2-486b-d31a-3dac11487a8f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/MA_colab/PG_textRank')\n",
        "default_path = '/content/drive/My Drive/MA_colab/PG_textRank/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZxcOwChzhc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import hashlib\n",
        "import struct\n",
        "import subprocess\n",
        "import collections\n",
        "import tensorflow as tf\n",
        "from tensorflow.core.example import example_pb2\n",
        "import nltk\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwHpXBTMzDhg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d947b26c-3d0e-411d-a1a2-5ad056a4a990"
      },
      "source": [
        "import pickle, re\n",
        "import nltk\n",
        "#tokenize the source text\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "with open('/content/drive/My Drive/MA_colab/textRank_texts.p', 'rb') as filehandle:\n",
        "  #list of list, each inner list contains several sentences\n",
        "  cores_ngram = pickle.load(filehandle)\n",
        "cores_ngram_tokenized = [word_tokenize(' '.join(x)) for x in cores_ngram]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7_xeYdMoub1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "e6454974-6172-43f9-9ec1-1f5c7a8b381a"
      },
      "source": [
        "for i in range (400,410):\n",
        "    print(cores_ngram_tokenized[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['compared', 'with', 'the', 'baseline', 'which', 'only', 'uses', 'the', 'forward', 'language', 'model', ',', 'our', 'experimental', 'results', 'show', 'that', 'the', 'additional', 'backward', 'language', 'model', 'is', 'able', 'to', 'gain', 'about', '0.5', 'bleu', 'points', ',', 'while', 'the', 'mi', 'trigger', 'model', 'gains', 'about', '0.4', 'bleu', 'points', '.', 'therefore', 'our', 'mi', 'trigger', 'model', 'selects', '“', 'is', '”', 'rather', 'than', '“', 'was', '”', '.9', 'this', 'example', 'illustrates', 'that', 'the', 'mi', 'trigger', 'model', 'is', 'capable', 'of', 'selecting', 'correct', 'words', 'by', 'using', 'longdistance', 'trigger', 'pairs', '.', 'and', 'both', 'the', 'numerator', 'and', 'denominator', 'have', 'up', 'to', '2', 'terminal', 'symbols', '.', 'where', 'the', 'approximation', 'is', 'based', 'on', 'the', 'nth', 'order', 'markov', 'assumption', '.', 'table', '3', 'shows', 'an', 'example', 'from', 'our', 'test', 'set', '.', 'for', 'example', ',', 'a', 'verb', 'in', 'the', 'past', 'tense', 'triggers', 'another', 'verb', 'also', 'in', 'the', 'past', 'tense', 'rather', 'than', 'the', 'present', 'tense', '.', 'the', 'item', 'indicates', 'that', 'a', 'btg', 'node', 'a', 'has', 'been', 'constructed', 'spanning', 'from', 'i', 'to', 'j', 'on', 'the', 'source', 'side', 'with', 'the', 'leftmost|rightmost', 'n', '−1', 'words', 'l|r', 'on', 'the', 'target', 'side', '.', 'previous', 'work', 'devoted', 'to', 'improving', 'language', 'models', 'in', 'smt', 'mostly', 'focus', 'on', 'two', 'categories', 'as', 'we', 'mentioned', 'beforel', ':', 'large', 'language', 'models', 'and', 'syntaxbased', 'language', 'models', '.', 'wi−1', 'it', 'can', 'also', 'be', 'denoted', 'in', 'a', 'more', 'visual', 'manner', 'as', 'x', '→', 'y', 'with', 'x', 'being', 'the', 'trigger', 'and', 'y', 'the', 'triggered', 'words', '.', 'by', 'these', 'triggers', ',', 'we', 'can', 'capture', 'longdistance', 'dependencies', 'that', 'are', 'outside', 'the', 'scope', 'of', 'forward', 'ngrams', '.', 'in', 'the', 'end', ',', 'we', 'conclude', 'in', 'section', '7.', 'for', 'the', 'convenience', 'of', 'training', ',', 'we', 'invert', 'the', 'order', 'in', 'each', 'sentence', 'in', 'the', 'training', 'data', ',', 'i.e.', ',', 'from', 'the', 'original', 'order', 'to', 'the', 'reverse', 'order', '.', 'finch', 'and', 'sumita', 'use', 'the', 'blm', 'in', 'their', 'reverse', 'translation', 'decoder', 'where', 'source', 'sentences', 'are', 'proceeded', 'from', 'the', 'ending', 'to', 'the', 'beginning', '.', 'al', '.', 'we', 'calculate', 'the', 'forward/backward', 'language', 'model', 'score', 'for', 'the', 'italic', 'words', 'in', 'both', 'the', 'baseline', 'and', 'f+b', 'hypothesis', 'according', 'to', 'the', 'trained', 'language', 'models', '.', ',', 'we', 'conduct', 'largescale', 'experiments', 'on', 'nist', 'chinesetoenglish', 'translation', 'tasks', 'to', 'evaluate', 'the', 'effectiveness', 'of', 'the', 'proposed', 'backward', 'language', 'model', 'and', 'mi', 'trigger', 'model', 'in', 'smt', '.', 'al', '.', 'need', 'to', 'be', 'recalculated', 'since', 'they', 'have', 'complete', 'ngrams', 'after', 'the', 'concatenation', '.', 'the', 'words', 'are', 'the', 'same', 'but', 'the', 'order', 'is', 'completely', 'reversed', '.', 'the', 'translation', 'model', 'mt', 'consists', 'of', 'widely', 'used', 'phrase', 'and', 'lexical', 'translation', 'probabilities', '.', 'only', 'the', 'probabilities', 'of', 'boundary', 'words', 'g', 'in', 'eq', '.', ')', 'further', 'study', 'of', 'the', 'two', 'models', 'indicates', 'that', 'backward', 'ngrams', 'and', 'longdistance', 'triggers', 'provide', 'useful', 'information', 'to', 'improve', 'translation', 'quality', '.', 'different', 'from', 'the', 'backward', 'ngram', 'language', 'model', ',', 'the', 'mi', 'trigger', 'model', 'still', 'looks', 'at', 'previous', 'contexts', ',', 'which', 'however', 'go', 'beyond', 'the', 'scope', 'of', 'forward', 'ngrams', '.', 'again', ',', 'shen', 'table', '1', 'shows', 'values', 'of', 'p', ',', 'g', ',', 'and', 'r', 'in', 'a', '3gram', 'example', 'which', 'helps', 'to', 'verify', 'eq', '.', '.', 'however', ',', 'this', 'is', 'not', 'a', 'problem', 'for', 'the', 'mi', 'trigger', 'model', '.', 'pfl', 'is', 'the', 'standard', 'forward', 'ngram', 'language', 'model', '.', 'however', ',', 'the', 'mi', 'triggers', 'are', 'capable', 'of', 'detecting', 'dependencies', 'between', 'wi', 'and', 'words', 'from', 'w1', 'to', 'wi−n', '.']\n",
            "['the', 'remaining', 'patterns', 'are', 'ranked', 'according', 'to', 'reliability', 'and', 'relevance', ',', 'and', 'the', 'topn', 'patterns', 'are', 'then', 'added', 'to', 'the', 'pattern', 'set.1', 'for', 'tuple', 'extraction', ',', 'a', 'relation', 'r1,2', 'is', 'constrained', 'to', 'only', 'consider', 'candidates', 'where', 'either', 't1', 'or', 't2', 'has', 'previously', 'been', 'extracted', 'into', 'c1', 'or', 'c2', ',', 'respectively', '.', 'we', 'demonstrate', 'that', 'this', 'relation', 'guidance', 'effectively', 'reduces', 'semantic', 'drift', ',', 'with', 'performance', 'approaching', 'manuallycrafted', 'constraints', '.', 'in', 'particular', ',', 'interrgb', 'significantly', 'improves', 'upon', 'wmeb', 'with', 'no', 'negative', 'categories', '.', 'that', 'is', ',', 'the', 'term', 'must', 'match', 'at', 'least', 'one', 'pattern', 'assigned', 'to', 'the', 'category', 'and', 'not', 'match', 'patterns', 'assigned', 'to', 'another', 'category', '.', 'for', 'example', ',', 'a', 'candidate', 'of', 'the', 'relation', 'isceoof', 'will', 'only', 'be', 'extracted', 'if', 'its', 'arguments', 'can', 'be', 'extracted', 'into', 'the', 'ceo', 'and', 'company', 'lexicons', 'and', 'a', 'ceo', 'is', 'constrained', 'to', 'not', 'be', 'a', 'celebrity', 'or', 'politician', '.', 'using', 'the', 'initial', 'relation', 'patterns', ',', 'the', 'topn', 'mutually', 'exclusive', 'seed', 'tuples', 'are', 'identified', 'for', 'the', 'relation', 'r1,2', '.', 'semantic', 'drift', 'is', 'reduced', 'by', 'forcing', 'the', 'categories', 'to', 'be', 'mutually', 'exclusive', '.', 'the', 'sizes', 'of', 'the', 'resulting', 'datasets', 'are', 'shown', 'in', 'table', '1.', 'et', 'the', 'remaining', 'terms', 'are', 'ranked', 'and', 'the', 'topn', 'terms', 'are', 'added', 'to', 'the', 'lexicon', '.', 'improve', 'textrunner', 'precision', 'by', 'using', 'deep', 'parsing', 'information', 'via', 'semantic', 'role', 'labelling', '.', 'the', 'term', 'extraction', 'data', 'is', 'formed', 'from', 'the', 'raw', '5grams', ',', 'where', 'the', 'set', 'of', 'candidate', 'terms', 'correspond', 'to', 'the', 'middle', 'tokens', 'and', 'the', 'patterns', 'are', 'formed', 'from', 'the', 'surrounding', 'tokens', '.', 'the', '5gm+4gm', 'data', ',', 'which', 'doubles', 'the', 'number', 'of', 'possible', 'candidate', 'relation', 'patterns', ',', 'performs', 'similarly', 'to', 'the', '5gm', 'representation', '.', 'rather', 'than', 'relying', 'on', 'manuallycrafted', 'category', 'and', 'relation', 'constraints', ',', 'relation', 'guided', 'bootstrapping', 'automatically', 'detects', ',', 'seeds', 'and', 'bootstraps', 'open', 'relations', 'between', 'the', 'target', 'categories', '.', 'to', 'extract', 'a', 'candidate', 'tuple', 'with', 'an', 'unknown', 'term', ',', 'the', 'term', 'must', 'also', 'be', 'a', 'valid', 'candidate', 'of', 'its', 'associated', 'category', '.', 'rgb', 'alternates', 'between', 'two', 'phases', 'of', 'wmeb', ',', 'one', 'for', 'terms', 'and', 'the', 'other', 'for', 'relations', ',', 'with', 'a', 'oneoff', 'relation', 'discovery', 'phase', 'in', 'between', '.', 'the', 'relation', 'extraction', 'phase', 'involves', 'running', 'wmeb', 'over', 'tuples', 'rather', 'than', 'terms', '.', 'the', 'additional', 'constraints', 'provided', 'by', 'anchoring', 'two', 'categories', 'appear', 'to', 'make', 'interrgb', 'less', 'susceptible', 'to', 'drift', '.', 'in', 'each', 'iteration', ',', 'a', 'relation', '’', 's', 'set', 'of', 'tuples', 'is', 'used', 'to', 'identify', 'candidate', 'relation', 'patterns', ',', 'as', 'for', 'term', 'extraction', '.', 'if', 'multiple', 'relations', 'are', 'found', ',', 'e.g', '.', 'r1,2', 'and', 'r2,3', ',', 'these', 'are', 'bootstrapped', 'simultaneously', ',', 'competing', 'with', 'each', 'other', 'for', 'tuples', 'and', 'relation', 'patterns', '.', 'if', 'fewer', 'than', 'n', 'relation', 'patterns', 'are', 'found', 'for', 'r1,2', ',', 'it', 'is', 'discarded', '.', 'in', 'cpl', ',', 'relation', 'bootstrapping', 'is', 'coupled', 'with', 'lexicon', 'bootstrapping', 'in', 'order', 'to', 'control', 'semantic', 'drift', 'in', 'the', 'target', 'relation', '’', 's', 'arguments', '.', 'negative', 'examples', 'such', 'as', 'isceoof', 'are', 'also', 'introduced', 'to', 'clarify', 'boundary', 'conditions', '.', 'these', 'use', 'a', 'small', 'set', 'of', 'seed', 'examples', 'from', 'the', 'target', 'lexicon', 'to', 'identify', 'contextual', 'patterns', 'which', 'are', 'then', 'used', 'to', 'extract', 'new', 'lexicon', 'items', '.', 'multicategory', 'bootstrappers', ',', 'such', 'as', 'nomen', 'and', 'wmeb', ',', 'reduce', 'semantic', 'drift', 'by', 'extracting', 'multiple', 'categories', 'simultaneously', 'in', 'competition', '.', 'unfortunately', ',', 'negative', 'categories', 'are', 'difficult', 'to', 'design', ',', 'introducing', 'a', 'substantial', 'amount', 'of', 'human', 'expertise', 'into', 'an', 'otherwise', 'unsupervised', 'framework', '.']\n",
            "['we', 'draw', 'the', 'conclusions', 'from', 'our', 'work', 'in', 'section', '7.', 'this', 'results', 'underscores', 'the', 'benefit', 'of', 'joint', 'annotation', ',', 'which', 'leverages', 'capitalization', 'and', 'pos', 'tagging', 'to', 'improve', 'the', 'quality', 'of', 'the', 'segmentation', '.', 'similarly', 'to', 'this', 'work', 'in', 'nlp', ',', 'we', 'demon2007', ';', 'jones', 'et', 'al.', ',', '2006', ';', 'guo', 'et', 'al.', ',', '2008', ';', 'hastrate', 'that', 'a', 'joint', 'approach', 'for', 'modeling', 'the', 'linguisover', 'the', 'main', 'benefits', 'of', 'these', 'two', 'annotation', 'methods', 'are', 'that', 'they', 'can', 'be', 'easily', 'implemented', 'using', 'standard', 'software', 'tools', ',', 'do', 'not', 'require', 'any', 'labeled', 'data', ',', 'and', 'provide', 'reasonable', 'annotation', 'accuracy', '.', 'we', 'are', 'encouraged', 'by', 'the', 'success', 'of', 'our', 'joint', 'query', 'annotation', 'technique', ',', 'and', 'intend', 'to', 'pursue', 'the', 'investigation', 'of', 'its', 'utility', 'for', 'ir', 'applications', '.', 'for', 'the', 'segmentation', 'task', ',', 'the', 'performance', 'is', 'at', 'its', 'best', 'for', 'the', 'question', 'and', 'keyword', 'queries', ',', 'and', 'at', 'its', 'worst', 'for', 'the', 'verbal', 'phrases', '.', 'in', 'addition', ',', 'while', 'all', 'the', 'dent', 'annotations', 'in', 'eq', '.', '1', 'is', 'methods', 'proposed', 'by', 'guo', 'et', 'al', '.', 'query', 'is', 'a', 'combination', 'of', 'an', 'artist', 'name', 'and', 'a', 'song', 'title', 'and', 'should', 'be', 'interpreted', 'as', 'kindred', '—', '“', 'where', 'would', 'i', 'be', '”', '.', 'the', 'most', 'straightforward', 'way', 'to', 'estimate', 'the', 'conditional', 'probabilities', 'in', 'eq', '.', '1', 'is', 'using', 'the', 'query', 'itself', '.', 'jqry', 'and', 'jprf', 'differ', 'in', 'their', 'choice', 'of', 'the', 'initial', 'independent', 'annotation', 'set', 'z', 'any', 'opinions', ',', 'findings', 'and', 'conclusions', 'or', 'recommendations', 'expressed', 'in', 'this', 'material', 'are', 'those', 'of', 'the', 'authors', 'and', 'do', 'not', 'necessarily', 'reflect', 'those', 'of', 'the', 'sponsor', '.', 'in', 'section', '4', 'we', 'describe', 'two', 'types', 'of', 'independent', 'query', 'annotations', 'that', 'are', 'used', 'as', 'input', 'for', 'the', 'joint', 'query', 'annotation', '.', 'query', 'in', 'figure', '1', 'is', 'a', 'whquestion', ',', 'and', 'it', 'contains', 'a', 'capitalized', 'concept', ',', 'a', 'single', 'verb', ',', 'and', 'four', 'segments', '.', 'et', 'that', 'is', ',', 'it', 'is', 'assumed', 'that', 'the', 'optimal', 'linguistic', 'annotation', 'zq', 'is', 'the', 'annotation', 'that', 'has', 'the', 'highest', 'probability', 'given', 'the', 'query', 'q', ',', 'regardless', 'of', 'the', 'other', 'annotations', 'in', 'the', 'set', 'zq', '.', 'however', ',', 'in', 'current', 'query', 'annotation', 'systems', ',', 'even', 'sentencelike', 'queries', 'are', 'often', 'hard', 'to', 'parse', 'and', 'annotate', ',', 'as', 'they', 'are', 'prone', 'to', 'contain', 'misspellings', 'and', 'idiosyncratic', 'grammatical', 'structures', '.', 'for', 'question', 'queries', 'the', 'performance', 'is', 'the', 'best', ',', 'since', 'they', 'resemble', 'sentences', 'encountered', 'in', 'traditional', 'corpora', '.', 'table', '1', 'shows', 'the', 'summary', 'of', 'the', 'performance', 'of', 'the', 'two', 'independent', 'and', 'two', 'joint', 'annotation', 'methods', 'for', 'the', 'entire', 'set', 'of', '250', 'queries', '.', 'is', 'the', 'work', 'on', 'joint', 'structure', 'modelinformation', 'about', 'unigrams', 'and', 'bigrams', 'from', '2008', ')', 'and', 'stacked', 'classification', '.', 'in', 'this', 'crf', 'model', ',', 'the', 'optimal', 'annotation', 'z∗', 'qis', 'the', 'label', 'we', 'are', 'trying', 'to', 'predict', ',', 'and', 'the', 'set', 'of', 'independent', 'annotations', 'z∗', 'p', '≈', '1', ':', 'p', 'p', '.', 'to', 'this', 'end', ',', 'we', 'proposed', 'a', 'probabilistic', 'approach', 'for', 'performing', 'joint', 'query', 'annotation', 'that', 'takes', 'into', 'account', 'the', 'dependencies', 'that', 'exist', 'between', 'the', 'different', 'annotation', 'types', '.', 'p', 'is', 'a', 'smoothed', 'estimator', 'that', 'combines', 'the', 'an', 'additional', 'research', 'area', 'which', 'is', 'relevant', 'to', 'information', 'from', 'the', 'retrieved', 'sentence', 'r', 'with', 'the', 'this', 'paper', 'in', 'all', 'but', 'one', 'case', ',', 'the', 'jprf', 'method', ',', 'which', 'uses', 'these', 'annotations', 'as', 'features', ',', 'outperforms', 'the', 'jqry', 'method', 'that', 'only', 'uses', 'the', 'annotation', 'done', 'by', 'iqry', '.']\n",
            "['2', 'learning', 'subword', 'units', 'given', 'raw', 'text', ',', 'our', 'objective', 'is', 'to', 'produce', 'a', 'lexicon', 'of', 'subword', 'units', 'that', 'can', 'be', 'used', 'by', 'a', 'hybrid', 'system', 'for', 'open', 'vocabulary', 'speech', 'recognition', '.', 'inference', 'is', 'challenging', 'since', 'the', 'lexicon', 'prior', 'renders', 'all', 'word', 'segmentations', 'interdependent', '.', 'we', 'assume', 'there', 'is', 'a', 'latent', 'segmentation', 's', 'of', 'this', 'corpus', 'which', 'impacts', 'y', '.', 'for', 'comparison', 'we', 'evaluate', 'a', 'baseline', 'method', 'for', 'selecting', 'units', '.', 'the', 'new', 'assignment', 'is', 'accepted', 'with', 'probability', ':', 'we', 'choose', 'the', 'proposal', 'distribution', 'q', 'as', 'eq', '.', 'furthermore', ',', 'we', 'have', 'confirmed', 'previous', 'work', 'that', 'hybrid', 'systems', 'achieve', 'better', 'phone', 'accuracy', ',', 'and', 'our', 'model', 'makes', 'modest', 'improvements', 'over', 'a', 'baseline', 'with', 'a', 'similarly', 'sized', 'subword', 'lexicon', '.', 'numerous', 'segmentations', 'are', 'possible', ';', 'each', 'word', 'has', '2n−1', 'possible', 'segmentations', ',', 'where', 'n', 'is', 'the', 'number', 'of', 'phones', 'in', 'its', 'pronunciation', '.', 'w', ':', 'p', '.', 'hybrid', 'recognizers', 'vary', 'in', 'a', 'number', 'of', 'ways', ':', 'subword', 'unit', 'type', ':', 'variablelength', 'phoneme', 'units', 'or', 'joint', 'letter', 'sound', 'subwords', ';', 'unit', 'creation', ':', 'datadriven', 'or', 'linguistically', 'motivated', ';', 'and', 'how', 'they', 'are', 'incorporated', 'in', 'lvcsr', 'systems', ':', 'hierarchical', 'or', 'flat', 'models', '.', 'we', 'used', 'rastrow', 'et', 'al', '.', 'to', 'obtain', 'the', 'best', 'segmentation', ',', 'we', 'use', 'deterministic', 'annealing', '.', 'we', 'begin', 'by', 'presenting', 'our', 'loglinear', 'model', 'for', 'learning', 'subword', 'units', 'with', 'a', 'simple', 'but', 'effective', 'inference', 'procedure', '.', 'learning', 'maximizes', 'the', 'log', 'likelihood', 'of', 'the', 'observed', 'labels', 'the', 'lexicon', 'prior', 'favors', 'smaller', 'lexicons', 'by', 'placing', 'an', 'exponential', 'prior', 'with', 'negative', 'weight', 'on', 'the', 'length', 'of', 'the', 'lexicon', '&', '|σ|', ',', 'where', '|σ', '|is', 'the', 'length', 'of', 'the', 'unit', 'σ', 'in', 'number', 'of', 'phones', '.', 'this', 'work', 'was', 'funded', 'by', 'a', 'google', 'phd', 'fellowship', '.', 'we', 'sample', 'a', 'segmentation', 'efficiently', 'using', 'dynamic', 'programming', '.', 'note', 'that', 'the', 'features', 'used', 'so', 'far', 'do', 'not', 'necessarily', 'provide', 'an', 'advantage', 'for', 'unobserved', 'versus', 'observed', 'oovs', ',', 'since', 'they', 'ignore', 'the', 'decoded', 'word/subword', 'sequence', '.', 'we', 'use', 'the', 'stateoftheart', 'oov', 'detection', 'model', 'of', 'parada', 'et', 'al', '.', ',', 'a', 'second', 'order', 'crf', 'with', 'features', 'based', 'on', 'the', 'output', 'of', 'a', 'hybrid', 'recognizer', '.', 'oov', 'detection', 'aims', 'to', 'identify', 'regions', 'in', 'the', 'lvcsr', 'output', 'where', 'oovs', 'were', 'uttered', '.', 'in', 'the', 'next', 'section', 'we', 'detail', 'the', 'oov', 'detection', 'approach', 'we', 'employ', ',', 'which', 'combines', 'hybrid', 'and', 'confidencebased', 'models', ',', 'achieving', 'stateofthe', 'art', 'performance', 'for', 'this', 'task', '.', 'interestingly', ',', 'the', 'average', 'subword', 'length', 'for', 'the', 'proposed', 'units', 'exceeded', 'that', 'of', 'the', 'baseline', 'units', 'by', '0.3', 'phones', '.', 'our', 'approach', 'yields', 'improvements', 'over', 'stateoftheart', 'results', '.', 'the', 'latter', 'is', 'more', 'useful', 'for', 'automatically', 'recovering', 'the', 'word', '’', 's', 'orthographic', 'form', ',', 'identifying', 'that', 'an', 'oov', 'was', 'spoken', ',', 'or', 'improving', 'performance', 'of', 'a', 'spoken', 'term', 'detection', 'system', 'with', 'oov', 'queries', '.', 'the', 'example', 'corpus', 'demonstrates', 'how', 'unit', 'features', 'f', ',', ',y', 'and', 'this', 'detector', 'processes', 'hybrid', 'recognizer', 'output', ',', 'so', 'we', 'can', 'evaluate', 'different', 'subword', 'unit', 'lexicons', 'for', 'the', 'hybrid', 'recognizer', 'and', 'measure', 'the', 'change', 'in', 'oov', 'detection', 'accuracy', '.', 'consider', 'a', 'simple', 'two', 'word', 'corpus', ':', 'cesar', ',', 'and', 'cesium', '.', 'a', 'mapping', 'of', 'words', 'to', 'classes']\n",
            "['which', 'is', 'to', 'be', 'predicted', 'and', 'thus', 'ranges', 'over', 'all', 'possible', 'words', 'of', 'some', 'vocabulary', '.', 'sga', 'is', 'an', 'online', 'optimization', 'method', 'which', 'iteratively', 'computes', 'the', 'gradient', 'v', 'for', 'each', 'instance', 'and', 'takes', 'a', 'step', 'of', 'size', 'η', 'in', 'the', 'direction', 'of', 'that', 'gradient', ':', 'these', 'features', 'are', 'referred', 'to', 'as', 'the', 'active', 'features', 'and', 'predictions', 'are', 'based', 'on', 'them', '.', 'specifically', ',', 'this', 'feature', 'set', 'additionally', 'includes', 'all', 'unigram', 'bag', 'features', 'up', 'to', 'a', 'distance', 'd', '=', '9.', 'has', 'the', 'following', 'intuitive', 'interpretation', '.', 'in', 'addition', ',', 'it', 'is', 'simple', 'to', 'implement', 'and', 'no', 'feature', 'selection', 'is', 'required', '.', 'in', 'a', 'first', 'stage', ',', 'the', 'categorical', 'parameters', 'are', 'computed', 'independently', 'for', 'each', 'feature', ',', 'as', 'the', 'maximum', 'likelihood', 'estimates', ',', 'smoothed', 'using', 'absolute', 'discounting', ':', 'wherec', \"'\", 'j', ',', 'k', 'is', 'the', 'smoothed', 'count', 'of', 'how', 'many', 'times', 'y', 'takes', 'value', 'yj', 'stochastic', 'gradient', 'yields', 'best', 'results', 'with', 'a', 'single', 'pass', 'through', 'all', 'instances', '.', 'we', 'assume', 'sparse', 'features', ',', 'such', 'that', 'typically', 'only', 'a', 'small', 'number', 'of', 'the', 'binary', 'features', 'take', 'value', '1.', 'and', 'thus', 'this', 'is', 'not', 'a', 'conventional', 'mixture', 'model', ',', 'the', 'vocabulary', 'was', 'compiled', 'by', 'selecting', 'all', 'words', 'which', 'occur', 'more', 'than', 'four', 'times', 'in', 'the', 'data', 'of', 'week', '31', ',', 'which', 'was', 'not', 'otherwise', 'used', 'for', 'training', 'or', 'testing', '.', 'in', 'other', 'words', ',', 'we', 'subtract', 'the', 'counts', 'for', 'a', 'particular', 'instance', 'before', 'computing', 'the', 'update', 'and', 'add', 'them', 'back', 'when', 'the', 'update', 'has', 'been', 'executed', '.', 'xi', 'of', 'm', 'features', '.', 'because', 'directly', 'using', 'the', 'maximumlikelihood', 'estimate', 'would', 'result', 'in', 'poor', 'predictions', ',', 'smoothing', 'techniques', 'are', 'applied', '.', 'thus', 'richer', 'feature', 'sets', 'consistently', 'lead', 'to', 'higher', 'model', 'accuracy', '.', 'we', 'are', 'concerned', 'with', 'estimating', 'a', 'probability', 'distribution', 'p', 'over', 'a', 'categorical', 'class', 'variable', 'y', 'with', 'range', 'y', ',', 'conditional', 'on', 'a', 'feature', 'vector', 'x', '=', ',', 'containing', 'the', 'feature', 'values', 'the', 'order', '5', 'models', 'would', 'not', 'fit', 'into', 'the', 'available', 'ram', 'which', 'is', 'why', 'for', 'order', '5', 'we', 'can', 'only', 'report', 'scores', 'for', 'd1', 'and', 'd2', '.', 'vmlr', 'in', 'addition', 'contains', 'longerdistance', 'features', ',', 'beyond', 'the', 'order', 'of', 'the', 'corresponding', 'ngram', 'models', '.', '∂θm', ')', 'computed', 'for', 'sga', 'contains', 'the', 'firstorder', 'derivatives', 'of', 'the', 'data', 'loglikelihood', 'of', 'a', 'particular', 'instance', 'with', 'respect', 'to', 'the', 'θparameters', 'which', 'are', 'given', 'by', 'the', 'resulting', 'parameterupdate', 'equation', '8', 'this', 'reflects', 'the', 'fact', 'that', 'certain', 'features', 'are', 'much', 'more', 'predictive', 'than', 'others', 'but', 'the', 'predictive', 'strength', 'for', 'a', 'particular', 'feature', 'often', 'doesn', '’', 't', 'vary', 'much', 'across', 'classes', 'and', 'can', 'thus', 'be', 'assumed', 'constant', '.', 'x', 'are', 'extracted', 'from', 'the', 'conditioning', 'context', 'the', 'vmm', 'has', 'two', 'types', 'of', 'parameters', ':', 'the', 'two', 'types', 'of', 'parameters', 'are', 'estimated', 'from', 'a', 'training', 'dataset', ',', 'consisting', 'of', 'instances', ',', 'x', ')', '.', 'once', 'the', 'categorical', 'parameters', 'have', 'been', 'computed', ',', 'we', 'proceed', 'by', 'estimating', 'the', 'predictive', 'strengths', 'θ', '=', '.', 'd', 'is', 'the', 'discount', 'constant', 'chosen', 'in', '.', 'section', '3', 'shows', 'that', 'a', 'generative1', 'lm', 'built', 'with', 'our', 'classifier', 'is', 'competitive', 'to', 'modified', 'kneserney', 'smoothing', 'and', 'can', 'outperform', 'it', 'if', 'sufficiently', 'rich', 'features', 'are', 'incorporated', '.', 'the', 'task', 'of', 'a', 'generative', 'lm', 'is', 'to', 'assign', 'a', 'probability', 'p', 'to', 'a', 'sequence', 'of', 'words', 'w']\n",
            "['the', 'coarsetofine', 'scheme', 'significantly', 'improves', 'the', 'efficiency', 'of', 'decoding', '.', 'al', '.', 'table', '3', 'shows', 'the', 'performance', 'on', 'the', 'development', 'data', 'set', 'of', 'the', 'three', 'coarsegrained', 'solvers', '.', 'we', 'will', 'discuss', 'the', 'details', 'in', 'the', 'next', 'section', '.', 'section', '2', 'gives', 'a', 'brief', 'introduction', 'to', 'the', 'problem', 'and', 'reviews', 'the', 'relevant', 'previous', 'research', '.', 'empirically', ',', 'the', 'decoding', 'over', 'subwords', 'is', '1.69', 'x', 'the', 'pos', 'information', 'provided', 'by', 'the', 'local', 'classifier', 'is', 'inaccurate', ';', 'the', 'structured', 'learning', 'of', 'the', 'subword', 'tagger', 'can', 'use', 'real', 'predicted', 'subword', 'labels', 'during', 'its', 'decoding', 'time', ',', 'since', 'this', 'learning', 'algorithm', 'does', 'inference', 'during', 'the', 'training', 'time', '.', 'in', 'our', 'model', ',', 'segmentation', 'and', 'pos', 'tagging', 'interact', 'with', 'each', 'other', 'in', 'two', 'processes', '.', 'a', 'first', 'order', 'maxmargin', 'markov', 'networks', 'model', 'is', 'used', 'to', 'resolve', 'the', 'sequence', 'tagging', 'problem', '.', 'the', 'idea', 'is', 'to', 'include', 'two', '“', 'levels', '”', 'of', 'predictors', '.', 'if', 'a', 'string', 'is', 'consistently', 'segmented', 'as', 'a', 'word', 'by', 'the', 'three', 'segmenters', ',', 'it', 'will', 'be', 'a', 'correct', 'word', 'prediction', 'with', 'a', 'very', 'high', 'probability', '.', 'as', 'a', 'result', ',', 'it', 'is', 'very', 'hard', 'to', 'recognize', 'outofvocabulary', 'idioms', 'for', 'word', 'segmentation', '.', 'table', '6', 'compares', 'the', 'performance', 'of', '“', 'c', ':', '±3', 't', ':', '±1', '”', 'models', 'trained', 'with', 'no', 'stacking', 'as', 'well', 'as', 'different', 'folds', 'of', 'crossvalidation', '.', 'the', 'remaining', 'part', 'of', 'the', 'paper', 'is', 'organized', 'as', 'follows', '.', 'second', ',', 'segmenters', 'designed', 'with', 'different', 'views', 'have', 'complementary', 'strength', '.', 'then', 'functions', 'g1', ',', '...', ',', 'gl', ')', 'are', 'seperately', 'trained', 'on', 's', '−', 'sl', ',', 'and', 'are', 'used', 'to', 'construct', 'the', 'augmented', 'dataset', 's�', '=', '{', ',', 'yt', ')', ':', 'this', 'strategy', 'makes', 'sure', 'that', 'it', 'is', 'still', 'possible', 'to', 'resegment', 'the', 'strings', 'of', 'which', 'the', 'boundaries', 'are', 'disagreed', 'with', 'by', 'the', 'coarsegrained', 'segmenters', 'in', 'the', 'finegrained', 'tagging', 'stage', '.', 'previous', 'research', 'has', 'shown', 'that', 'the', 'integrated', 'methods', 'outperformed', 'pipelined', 'systems', '.', 'the', 'statistics', 'will', 'also', 'empirically', 'show', 'that', 'subwords', 'are', 'significantly', 'larger', 'than', 'characters', 'and', 'only', 'slightly', 'smaller', 'than', 'words', '.', 'this', 'kind', 'of', 'solver', 'sequentially', 'decides', 'whether', 'the', 'local', 'sequence', 'of', 'characters', 'makes', 'up', 'a', 'word', 'as', 'well', 'as', 'its', 'possible', 'pos', 'tag', '.', 'the', 'three', 'coarsegrained', 'solvers', 'segw', ',', 'segc', 'and', 'segtagl', 'are', 'directly', 'trained', 'on', 'the', 'original', 'training', 'data', '.', 'the', 'average', 'length', 'of', 'subwords', 'on', 'the', 'development', 'set', 'is', '1.64', ',', 'while', 'the', 'average', 'length', 'of', 'words', 'is', '1.69.', 'in', 'this', 'work', ',', 'stacked', 'learning', 'is', 'used', 'to', 'acquire', 'extended', 'training', 'data', 'for', 'subword', 'tagging', '.', '{', 'c', '}', 's.t', '.', 'if', 'a', 'high', 'performance', 'subword', 'tagger', 'can', 'be', 'constructed', ',', 'the', 'whole', 'task', 'can', 'be', 'well', 'resolved', '.', 'based', 'on', 'the', 'subword', 'structure', ',', 'joint', 'word', 'segmentation', 'and', 'pos', 'tagging', 'is', 'addressed', 'as', 'a', 'two', 'step', 'process', '.', 'for', 'example', ',', 'a', 'simple', 'maximum', 'matching', 'segmenter', 'can', 'achieve', 'an', 'fscore', 'of', 'about', '90.', 'as', 'a', 'result', ',', 'the', 'search', 'space', 'of', 'the', 'subword', 'tagging', 'is', 'significantly', 'shrunken', ',', 'and', 'exact', 'viterbi', 'decoding', 'without', 'approximately', 'pruning', 'can', 'be', 'efficiently', 'processed', '.', 'although', 'the', 'stacked', 'subword', 'model', 'is', 'an', 'ad', 'hoc', 'solution', 'for', 'a', 'particular', 'problem', ',', 'namely', 'joint', 'word', 'segmentation', 'and', 'pos', 'tagging', ',', 'the', 'idea', 'to', 'employ', 'system', 'ensemble', 'and', 'stacked', 'learning', 'in', 'general', 'provides', 'an', 'alternative', 'for', 'structured', 'problems', '.', 'the', 'lines', 'segw', ',', 'segc', 'and', 'segtagl', 'are', 'the', 'predictions', 'of', 'the', 'three', 'coarsegrained', 'solvers', '.']\n",
            "['et', '1st', 'order', 'hmm', '.', 'we', 'used', 'an', 'inhouse', 'dependency', 'parser', 'to', 'extract', 'the', 'prepositional', 'constructions', 'from', 'the', 'data', '.', 'as', 'a', 'baseline', ',', 'we', 'simply', 'label', 'all', 'word', 'types', 'with', 'the', 'same', 'sense', ',', 'i.e.', ',', 'each', 'preposition', 'token', 'is', 'labeled', 'with', 'its', 'respective', 'name', '.', 'here', ',', 'the', 'goal', 'is', 'to', 'increase', 'the', 'data', 'likelihood', 'while', 'keeping', 'the', 'number', 'of', 'parameters', 'small', '.', 'knowledge', 'about', 'semantic', 'constraints', 'of', 'prepositional', 'constructions', 'would', 'not', 'only', 'provide', 'better', 'label', 'accuracy', ',', 'but', 'also', 'aid', 'in', 'resolving', 'prepositional', 'attachment', 'problems', '.', 'we', 'use', 'the', 'em', 'algorithm', 'as', 'a', 'baseline', '.', 'a', ')', 'since', 'unsupervised', 'methods', 'use', 'the', 'provided', 'labels', 'indiscriminately', ',', 'we', 'have', 'to', 'map', 'the', 'resulting', 'predictions', 'to', 'the', 'gold', 'labels', '.', '=0.0025', '.', 'the', 'senses', 'of', 'each', 'element', 'are', 'denoted', 'by', 'a', 'barred', 'letter', ',', 'i.e.', ',', 'p�', 'denotes', 'the', 'preposition', 'sense', ',', 'h', 'denotes', 'the', 'sense', 'of', 'the', 'head', 'word', ',', 'and', 'o�', 'the', 'sense', 'of', 'the', 'object', '.', 'the', 'joint', 'distribution', 'over', 'the', 'network', 'can', 'thus', 'be', 'written', 'as', 'we', 'want', 'to', 'incorporate', 'as', 'much', 'information', 'as', 'possible', 'into', 'the', 'model', 'to', 'constrain', 'the', 'choices', '.', 'it', 'defines', 'senses', 'for', 'each', 'of', 'the', '34', 'most', 'frequent', 'prepositions', '.', 'the', 'best', 'current', 'supervised', 'system', 'we', 'are', 'aware', 'of', 'reaches', '84.8', '%', '.', 'there', 'are', 'on', 'average', '9.76', 'senses', 'per', 'preposition', '.', 'this', 'is', 'a', 'significant', 'improvement', 'over', 'the', 'baseline', 'and', 'vanilla', 'em', '.', 'in', 'order', 'to', 'constrain', 'the', 'argument', 'senses', ',', 'we', 'construct', 'a', 'dictionary', 'that', 'lists', 'for', 'each', 'word', 'all', 'the', 'possible', 'lexicographer', 'senses', 'according', 'to', 'wordnet', '.', 'we', 'ran', 'em', 'on', 'each', 'model', 'for', '100', 'iterations', ',', 'or', 'until', 'the', 'perplexity', 'stopped', 'decreasing', 'below', 'a', 'threshold', 'of', '10−6', '.', 'accuracy', 'might', 'improve', 'with', 'more', 'restarts', '.', 'al', '.', '.', 'repeated', 'random', 'restarts', 'help', 'escape', 'unfavorable', 'initializations', 'that', 'lead', 'to', 'local', 'maxima', '.', 'learning', 'by', 'reading', 'approaches', 'also', 'crucially', 'depend', 'on', 'unsupervised', 'techniques', 'as', 'the', 'ones', 'described', 'here', 'for', 'textual', 'enrichment', '.', 'unless', 'specified', 'otherwise', ',', 'we', 'initialized', 'all', 'models', 'uniformly', ',', 'and', 'trained', 'until', 'the', 'perplexity', 'rate', 'stopped', 'increasing', 'or', 'a', 'predefined', 'number', 'of', 'iterations', 'was', 'reached', '.', 'we', 'hope', 'to', 'present', 'results', 'for', 'the', 'joint', 'disambiguation', 'of', 'preposition', 'and', 'arguments', 'in', 'a', 'future', 'paper', '.', 'in', 'addition', 'to', 'the', 'baseline', ',', 'we', 'ran', '100', 'restarts', 'with', 'random', 'initialization', 'and', 'smoothed', 'the', 'fractional', 'counts', 'by', 'adding', '0.1', 'before', 'normalizing', '.', 'prepositions', 'are', 'ubiquitous—they', 'account', 'for', 'more', 'than', '10', '%', 'of', 'the', '1.16m', 'words', 'in', 'the', 'brown', 'corpus—and', 'highly', 'ambiguous', '.', '<', '.001.', 'disambiguating', 'prepositions', 'is', 'thus', 'a', 'challenging', 'and', 'interesting', 'task', 'in', 'itself', ')', ',', 'and', 'holds', 'promise', 'for', 'nlp', 'applications', 'such', 'as', 'information', 'extraction', 'or', 'machine', 'translation.1', 'given', 'a', 'sentence', 'such', 'as', 'the', 'following', ':', 'in', 'the', 'morning', ',', 'he', 'shopped', 'in', 'rome', 'we', 'ultimately', 'want', 'to', 'be', 'able', 'to', 'annotate', 'it', 'as', 'here', ',', 'the', 'preposition', 'in', 'has', 'two', 'distinct', 'meanings', ',', 'namely', 'a', 'temporal', 'and', 'a', 'locative', 'one', '.', 'this', 'should', 'also', 'improve', 'disambiguation', 'of', 'the', 'words', 'linked', 'by', 'the', 'prepositions', '.', 'carmel', 'provides', 'options', 'for', 'both', 'smoothing', 'and', 'restarts', '.', 'our', 'contributions', 'are', ':', 'a', 'preposition', 'p', 'acts', 'as', 'a', 'link', 'between', 'two', 'words', ',', 'h', 'and', 'o.', 'we', 'note', 'that', 'em', 'performs', 'better', 'with', 'the', 'less', 'complex', 'hmm', '.']\n",
            "['to', 'detect', 'events', ',', 'we', 'maintain', 'the', 'eight', 'tokenrole', 'classifiers', 'from', 'the', 'previous', 'system', ',', 'but', 'they', 'become', 'subclassifiers', 'of', 'our', 'joint', 'system', '.', 'in', 'the', 'context', 'of', 'vinea', ',', 'these', 'two', 'features', 'allow', 'the', 'system', 'to', 'learn', 'tagpairspecific', 'limits', 'on', 'arc', 'length', '.', 'in', 'figure', '1', ',', 'a', 'positive', 'assignment', 'to', 'any', 'of', 'the', 'indicated', 'tokenroles', 'is', 'sufficient', 'to', 'filter', 'the', 'dotted', 'arc', '.', 'we', 'had', 'hoped', 'that', 'the', 'benefits', 'of', 'joint', 'training', 'would', 'outweigh', 'this', 'drawback', ',', 'but', 'our', 'results', 'show', 'that', 'they', 'do', 'not', '.', 'the', 'dynamic', 'threshold', 'also', 'alters', 'our', 'interpretation', 'of', 'filtering', 'events', ':', 'where', 'before', 'they', 'were', 'either', 'active', 'or', 'inactive', ',', 'events', 'are', 'now', 'assigned', 'scores', ',', 'which', 'are', 'compared', 'with', 'the', 'threshold', 'to', 'make', 'final', 'filtering', 'decisions', '.3', 'one', 'stage', 'of', 'this', 'cascade', 'operates', 'one', 'token', 'at', 'a', 'time', ',', 'labeling', 'each', 'token', 't', 'according', 'to', 'various', 'roles', 'in', 'the', 'tree', ':', 'similar', 'to', 'roark', 'and', 'hollingshead', ',', 'each', 'role', 'has', 'a', 'corresponding', 'binary', 'classifier', '.', 'co.', 'these', 'allow', 'us', 'to', 'tune', 'our', 'system', 'for', 'high', 'precision', 'by', 'increasing', 'the', 'cost', 'of', 'misclassifying', 'an', 'arc', 'that', 'should', 'not', 'be', 'filtered', '.', 'a', 'if', 'detected.2', 'figure', '1', 'shows', 'that', 'zthe', ',', ',hiss', '=', '{', 'nah3', ',', 'htr*6', ',', 'htr56', ',', 'htr16', ',', 'htl161', '.', 'for', 'the', 'tokenrole', 'and', 'vine', 'subclassifiers', ',', 'we', 'compare', 'against', 'an', 'independentlytrained', 'ensemble', 'of', 'the', 'same', 'classifiers.9', 'note', 'that', 'none', 'can', 'not', 'be', 'trained', 'independently', ',', 'as', 'its', 'shared', 'dynamic', 'threshold', 'considers', 'arc', 'and', 'token', 'views', 'of', 'the', 'data', 'simultaneously', '.', 'in', 'our', 'previous', 'system', ',', 'filtering', 'is', 'conducted', 'by', 'training', 'a', 'separate', 'svm', 'classifier', 'for', 'each', 'of', 'the', 'eight', 'tokenroles', 'described', 'in', 'section', '1.', 'z', 'is', 'now', 'conditioned', 'on', 'the', 'label', 'y', ':', 'za|1', '=', 'za', ',', 'and', 'za|0', '=', '{', 'nonea', '}', '.', 'figure', '1', 'depicts', 'five', 'of', 'the', 'eight', 'token', 'roles', ',', 'along', 'with', 'their', 'truth', 'values', '.', 'parsing', 'experiments', 'are', 'carried', 'out', 'using', 'the', 'mst', 'parser', ',7', 'which', 'we', 'have', 'modified', 'to', 'filter', 'arcs', 'before', 'carrying', 'out', 'feature', 'extraction', '.', 'the', 'improvement', 'is', 'less', 'impressive', 'for', 'mst2', ',', 'where', 'the', 'overhead', 'for', 'filter', 'application', 'is', 'a', 'less', 'substantial', 'fraction', 'of', 'parsing', 'time', ';', 'however', ',', 'our', 'training', 'framework', 'also', 'has', 'other', 'benefits', 'with', 'respect', 'to', 'r+l+q', ',', 'including', 'a', 'single', 'unified', 'training', 'algo10results', 'are', 'not', 'identical', 'to', 'those', 'reported', 'in', 'our', 'previous', 'paper', ',', 'due', 'to', 'our', 'use', 'of', 'a', 'different', 'partofspeech', 'tagger', '.', 'the', 'role', 'labelers', 'can', 'be', 'tuned', 'for', 'high', 'precision', 'with', 'labelspecific', 'cost', 'parameters', ';', 'these', 'are', 'tuned', 'separately', 'for', 'each', 'classifier', '.', 'in', 'the', 'context', 'of', 'nonea', ',', 'these', 'features', 'protect', 'short', 'arcs', 'and', 'arcs', 'that', 'connect', 'frequentlylinked', 'tagpairs', ',', 'allowing', 'our', 'tokenrole', 'filters', 'to', 'be', 'more', 'aggressive', 'on', 'arcs', 'that', 'do', 'not', 'have', 'these', 'characteristics', '.', 'to', 'do', 'so', ',', 'we', 'propose', 'a', 'classification', 'scheme', 'focused', 'on', 'arcs.1', 'during', 'training', ',', 'each', 'arc', 'is', 'assigned', 'a', 'filtering', 'event', 'as', 'a', 'latent', 'variable', '.', 'therefore', ',', 'an', 'nah', 'decision', 'that', 'filters', 'thirty', 'arcs', 'is', 'given', 'more', 'weight', 'than', 'an', 'htl5', 'decision', 'that', 'filters', 'only', 'one', ',', 'unless', 'those', 'thirty', 'arcs', 'are', 'already', 'filtered', 'by', 'higherscoring', 'subclassifiers', '.']\n",
            "['note', 'that', 'rss', 'can', 'be', 'regarded', 'as', 'a', 'metric', 'since', 'the', 'sum', 'of', 'each', 'metric', 'becomes', 'also', 'a', 'metric', 'by', 'constructing', 'a', '1norm', 'product', 'metric', '.', 'shi', 'et', 'al', '.', 'in', 'order', 'to', 'show', 'distortion', ',', 'we', 'want', 'to', 'use', 'chebyshev', '’', 's', 'inequality', '.', 'let', 'ψ', 'be', 'the', 'sum', 'of', 'ψ', 'for', 'any', 'observed', 'pair', 'of', 'x', ',', 'y', ',', 'each', 'of', 'which', 'expresses', 'the', 'difference', 'between', 'an', 'example', 'and', 'its', 'corresponding', 'centroid', '.', 'because', 'the', 'variance', 'of', 'the', 'sum', 'of', 'random', 'variables', 'derives', 'from', 'each', 'covariance', 'between', 'pairs', 'of', 'variables', ',', 'first', 'we', 'show', 'the', 'covariance', 'between', 'the', 'squared', 'hash', 'length', 'of', 'two', 'vectors', '.', 'this', 'trick', 'greatly', 'reduces', 'the', 'size', 'of', 'dense', 'vectors', ',', 'since', 'the', 'maximum', 'index', 'value', 'becomes', 'equivalent', 'to', 'the', 'maximum', 'hash', 'value', 'of', 'h.', 'furthermore', ',', 'unlike', 'random', 'projection', ',', 'feature', 'hashing', 'retains', 'sparsity', 'of', 'sparse', 'input', 'vectors', '.', 'in', 'this', 'kind', 'of', 'hashing', 'tricks', ',', 'an', 'index', 'of', 'inputs', 'do', 'not', 'have', 'to', 'be', 'an', 'integer', 'but', 'can', 'be', 'any', 'hashable', 'value', ',', 'including', 'a', 'string', '.', 'for', 'example', ',', 'if', 'frequencies', 'of', 'words', 'are', 'used', 'as', 'features', ',', 'function', 'words', 'should', 'be', 'ignored', 'not', 'only', 'because', 'they', 'give', 'no', 'information', 'for', 'clustering', 'but', 'also', 'because', 'their', 'high', 'frequencies', 'magnify', 'distortion', '.', 'the', 'paper', 'also', 'showed', 'that', 'hashing', 'tricks', 'are', 'useful', 'to', 'eliminate', 'alphabet', 'storage', '.', 'below', 'is', 'the', 'definition', '.', 'let', 'us', 'define', 'x', 'µk', 'be', 'their', 'corresponding', 'centroids', 'in', 'the', 'original', 'space', ',', 'φ', 'be', 'a', 'hashed', 'feature', 'map', ',', 'and', 'µo', ',', '...', ',', 'µk', 'be', 'their', 'corresponding', 'centroids', 'in', 'the', 'hashed', 'space', '.', 'such', 'that', 'they', 'locally', 'minimize', 'the', 'residual', 'sum', 'of', 'squares', 'which', 'is', 'defined', 'as', '||x', '−', 'µk||2', '.', 'although', 'we', 'do', 'not', 'describe', 'the', 'famous', 'algorithm', 'of', 'kmeans', 'here', ',', 'we', 'remind', 'the', 'reader', 'of', 'its', 'overall', 'objective', 'for', 'later', 'analysis', '.', 'ωk', 'and', 'their', 'corresponding', 'vectors', 'µ1', ',', '...', ',', 'µk', 'al', '.', 'e', '∈ωk', 'φ', '=', 'φ', 'to', 'this', 'end', ',', 'it', 'is', 'vital', 'to', 'know', 'the', 'expectation', 'and', 'variance', 'of', 'the', 'sum', 'of', 'squared', 'hash', 'lengths', '.', '=', '42', ',', 'we', 'make', 'a', 'new', 'vector', 'y', 'by', 'setting', 'y42', '=', 'reapplying', 'linearlity', 'to', 'this', 'result', ',', 'we', 'have', '||φ', '−µφk||2', '=', 'its', 'additive', 'distortion', 'is', 'the', 'infimum', 'of', 'c', 'which', ',', 'for', 'any', 'observed', 'x', ',', 'y', '∈', 'x', ',', 'satisfies', 'the', 'following', 'condition', ':', 'for', 'instance', ',', 'a', 'feature', '‘', 'the', 'current', 'word', 'ends', 'with', 'ing', '’', 'can', 'be', 'expressed', 'as', 'a', 'string', 'cur', ':', 'end', ':', 'ing', '.', 'the', 'hash', 'kernel', 'h·', ',', '·iφ', 'is', 'defined', 'as', 'hx', ',', \"x'iφ\", '=', 'hφ', ',', 'φ', 'i.', 'for', 'dimensionreduction', 'to', 'kmeans', ',', 'we', 'propose', 'a', 'new', 'method', 'hashed', 'kmeans', '.', 'an', 'additional', 'useful', 'trait', 'for', 'nlp', 'tasks', 'is', 'that', 'it', 'can', 'save', 'much', 'memory', 'by', 'eliminating', 'an', 'alphabet', 'storage', '.', 'among', 'them', ',', 'kmeans', 'is', '“', 'the', 'most', 'important', 'flat', 'clustering', 'algorithm', '”', 'both', 'for', 'its', 'simplicity', 'and', 'performance', '.', 'since', 'indices', 'of', 'dense', 'vectors', 'must', 'be', 'integers', ',', 'traditionally', 'we', 'need', 'a', 'dictionary', 'to', 'map', 'these', 'strings', 'to', 'integers', ',', 'which', 'may', 'waste', 'much', 'memory', '.', '=', 'φ', '.', 'thus', ',', 'it', 'is', 'infeasible', 'to', 'store', 'them', 'in', 'memory', 'and', 'slow', 'to', 'compute', 'if', 'the', 'dimension', 'of', 'inputs', 'is', 'huge', ',', 'as', 'is', 'often', 'the', 'case', 'with', 'nlp', 'and', 'text', 'mining', 'tasks', '.', 'suppose', 'you', 'have', 'n', 'input', 'vectors', 'x1', ',', '...', ',', 'xn', '.']\n",
            "['framenet', 'is', 'a', 'knowledgebase', 'of', 'frames', 'describing', 'prototypical', 'situations', ',', 'and', 'the', 'role', 'of', 'the', 'participants', 'they', 'involve', '.', 'in', 'addition', ',', 'in', 'order', 'to', 'experiment', 'with', 'different', 'phrase', 'tables', 'providing', 'different', 'degrees', 'of', 'coverage', 'and', 'precision', ',', 'we', 'extracted', '7', 'phrase', 'tables', 'by', 'pruning', 'the', 'initial', 'one', 'on', 'the', 'direct', 'phrase', 'translation', 'probabilities', 'of', '0.01', ',', '0.05', ',', '0.1', ',', '0.2', ',', '0.3', ',', '0.4', 'and', '0.5.', 'the', 'limited', 'availability', 'of', 'multilingual', 'lexical', 'resources', '.', 'for', 'comparison', 'with', 'the', 'extracted', 'phrase', 'and', 'paraphrase', 'tables', ',', 'we', 'use', 'a', 'large', 'bilingual', 'dictionary', 'and', 'multiwordnet', 'as', 'alternative', 'sources', 'of', 'lexical', 'knowledge', '.', 'our', 'results', 'are', 'calculated', 'over', '800', 'test', 'pairs', 'of', 'our', 'clte', 'corpus', ',', 'after', 'training', 'the', 'svm', 'classifier', 'over', '800', 'development', 'pairs', '.', 'they', 'proved', 'to', 'be', 'useful', 'in', 'a', 'number', 'of', 'nlp', 'applications', 'such', 'as', 'natural', 'language', 'generation', ',', 'multidocument', 'summarization', ',', 'automatic', 'evaluation', 'of', 'mt', ',', 'and', 'te', '.', 'addressing', 'clte', 'we', 'have', 'to', 'face', 'additional', 'and', 'more', 'problematic', 'issues', 'related', 'to', ':', 'i', ')', 'the', 'stronger', 'need', 'of', 'lexical', 'knowledge', ',', 'and', 'ii', ')', 'the', 'remainder', 'of', 'this', 'paper', 'is', 'structured', 'as', 'follows', '.', 'overall', ',', 'our', 'results', 'suggest', 'that', 'the', 'lexical', 'knowledge', 'extracted', 'from', 'parallel', 'data', 'can', 'be', 'successfully', 'used', 'to', 'approach', 'the', 'clte', 'task', '.', 'these', 'include', ',', 'just', 'to', 'mention', 'the', 'most', 'popular', 'ones', ',', 'dirt', ',', 'verbocean', ',', 'framenet', ',', 'and', 'wikipedia', '.', ',', '5grams', 'extracted', 'from', 'h', 'that', 'match', 'with', 'ngrams', 'in', 't.', 'phrasal', 'matches', 'are', 'performed', 'either', 'at', 'the', 'level', 'of', 'tokens', ',', 'lemmas', ',', 'or', 'stems', ',', 'can', 'be', 'of', 'two', 'types', ':', 'for', 'each', 'phrase', 'in', 'h', ',', 'we', 'first', 'search', 'for', 'exact', 'matches', 'at', 'the', 'level', 'of', 'token', 'with', 'phrases', 'in', 't.', 'if', 'no', 'match', 'is', 'found', 'at', 'a', 'token', 'level', ',', 'the', 'other', 'levels', 'are', 'attempted', '.', 'the', 'final', 'entailment', 'decision', 'for', 'a', 't/h', 'pair', 'is', 'assigned', 'considering', 'a', 'model', 'learned', 'from', 'the', 'similarity', 'scores', 'based', 'on', 'the', 'identified', 'phrasal', 'matches', '.', 'as', 'regards', 'wikipedia', ',', 'the', 'crosslingual', 'links', 'between', 'pages', 'in', 'different', 'languages', 'offer', 'a', 'possibility', 'to', 'extract', 'lexical', 'knowledge', 'useful', 'for', 'clte', '.', 'it', 'consists', 'of', '1600', 'pairs', 'derived', 'from', 'the', 'rte3', 'development', 'and', 'test', 'sets', '.', 'the', 'resulting', 'dictionary', 'features', '53,958', 'entries', ',', 'with', 'an', 'average', 'length', 'of', '1.2', 'words', '.', 'hypernymy/hyponymy', 'chains', 'can', 'provide', 'entailmentpreserving', 'relations', 'between', 'concepts', ',', 'indicating', 'that', 'a', 'word', 'in', 'the', 'hypothesis', 'can', 'be', 'replaced', 'by', 'a', 'word', 'from', 'the', 'text', '.', 'overall', ',', 'these', 'results', 'confirm', 'our', 'claim', 'that', 'increasing', 'the', 'coverage', 'using', 'context', 'sensitive', 'phrase', 'pairs', 'obtained', 'from', 'large', 'parallel', 'corpora', ',', 'results', 'in', 'better', 'performance', 'not', 'only', 'in', 'clte', ',', 'but', 'also', 'in', 'the', 'monolingual', 'scenario', '.', ':', '7,425', 'entries', '.', 'third', ',', 'the', 'accuracy', 'obtained', 'over', 'the', 'clte', 'corpus', 'using', 'combined', 'phrase', 'and', 'paraphrase', 'tables', 'is', 'comparable', 'to', 'the', 'best', 'result', 'gained', 'over', 'the', 'automatically', 'translated', 'dataset', '.', 'besides', 'wordnet', ',', 'the', 'rte', 'literature', 'documents', 'the', 'use', 'of', 'a', 'variety', 'of', 'lexical', 'information', 'sources', '.', 'along', 'this', 'direction', ',', 'we', 'on', 'monolingual', 'rte', 'datasets', 'using', 'paraphrase', 'tables', 'extracted', 'from', 'bilingual', 'parallel', 'corpora', '.', '4.', 'map', 'such', 'paraphrases', 'to', 'phrases', 'in', 't.', 'with', 'the', 'second', 'method', ',', 'phrasal', 'matches', 'between', 'the', 'text', 'and', 'the', 'resulting', 'paraphrase', 'tables', 'contain', 'pairs', 'of', 'corresponding', 'phrases', 'in', 'the', 'same', 'language', ',', 'possibly', 'associated', 'with', 'probabilities', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRjuW_aDzGv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #sentence-wise segment the summary\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
        "senttokenizer = PunktSentenceTokenizer()\n",
        "with open ('/content/drive/My Drive/MA_colab/abs_strV2.p', 'rb') as filehandle:\n",
        "  abs_selected = pickle.load(filehandle)\n",
        "for ab in abs_selected:\n",
        "    ab = ab.lower()\n",
        "    senttokenizer.train(ab)\n",
        "abs_selected = [senttokenizer.tokenize(x) for x in abs_selected]\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "regTokenizer =  RegexpTokenizer(pattern=r\"\\s|[\\]\", gaps=True)\n",
        "\n",
        "abs_sent = []\n",
        "for ab in abs_selected:\n",
        "    ab_sent = []\n",
        "    for s in ab:\n",
        "        s_new = '<s> ' + s + ' </s> '\n",
        "        ab_sent.append(s_new)\n",
        "    abs_sent.append(' '.join(ab_sent))\n",
        "abs_sent = [regTokenizer.tokenize(x) for x in abs_sent]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeR6j3cBo6by",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "ee6148f3-9fac-417b-a793-f4082131acbc"
      },
      "source": [
        "for i in range(768,777):\n",
        "    print(abs_sent[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s>', 'given', 'the', 'large', 'amounts', 'of', 'online', 'textual', 'documents', 'available', 'these', 'days', 'e', 'g', 'news', 'articles', 'weblogs', 'and', 'scientific', 'papers', 'effective', 'methods', 'for', 'extracting', 'keyphrases', 'which', 'provide', 'a', 'highlevel', 'topic', 'description', 'of', 'a', 'document', 'are', 'greatly', 'needed', '</s>', '<s>', 'in', 'this', 'paper', 'we', 'propose', 'a', 'supervised', 'model', 'for', 'keyphrase', 'extraction', 'from', 'research', 'papers', 'which', 'are', 'embedded', 'in', 'citation', 'networks', '</s>', '<s>', 'to', 'this', 'end', 'we', 'design', 'novel', 'features', 'based', 'on', 'citation', 'network', 'information', 'and', 'use', 'them', 'in', 'conjunction', 'with', 'traditional', 'features', 'for', 'keyphrase', 'extraction', 'to', 'obtain', 'remarkable', 'improvements', 'in', 'performance', 'over', 'strong', 'baselines', '</s>']\n",
            "['<s>', 'numerous', 'works', 'in', 'statistical', 'machine', 'translation', 'have', 'attempted', 'to', 'identify', 'better', 'translation', 'hypotheses', 'obtained', 'by', 'an', 'initial', 'decoding', 'using', 'an', 'improved', 'but', 'more', 'costly', 'scoring', 'function', '</s>', '<s>', 'in', 'this', 'work', 'we', 'introduce', 'an', 'approach', 'that', 'takes', 'the', 'hypotheses', 'produced', 'by', 'a', 'stateoftheart', 'reranked', 'phrasebased', 'smt', 'system', 'and', 'explores', 'new', 'parts', 'of', 'the', 'search', 'space', 'by', 'applying', 'rewriting', 'rules', 'selected', 'on', 'the', 'basis', 'of', 'posterior', 'phraselevel', 'confidence', '</s>', '<s>', 'in', 'the', 'medical', 'domain', 'we', 'obtain', 'a', '1', '9', 'bleu', 'improvement', 'over', 'a', 'reranked', 'baseline', 'exploiting', 'the', 'same', 'scoring', 'function', 'corresponding', 'to', 'a', '5', '4', 'bleu', 'improvement', 'over', 'the', 'original', 'moses', 'baseline', '</s>', '<s>', 'we', 'show', 'that', 'if', 'an', 'indication', 'of', 'which', 'phrases', 'require', 'rewriting', 'is', 'provided', 'our', 'automatic', 'rewriting', 'procedure', 'yields', 'an', 'additional', 'improvement', 'of', '1', '5', 'bleu', '</s>', '<s>', 'various', 'analyses', 'including', 'a', 'manual', 'error', 'analysis', 'further', 'illustrate', 'the', 'good', 'performance', 'and', 'potential', 'for', 'improvement', 'of', 'our', 'approach', 'in', 'spite', 'of', 'its', 'simplicity', '</s>']\n",
            "['<s>', 'the', 'current', 'stateoftheart', 'singledocument', 'summarization', 'method', 'generates', 'a', 'summary', 'by', 'solving', 'a', 'tree', 'knapsack', 'problem', 'which', 'is', 'the', 'problem', 'of', 'finding', 'the', 'optimal', 'rooted', 'subtree', 'of', 'the', 'dependencybased', 'discourse', 'tree', 'of', 'a', 'document', '</s>', '<s>', 'we', 'can', 'obtain', 'a', 'gold', 'depdt', 'by', 'transforming', 'a', 'gold', 'rhetorical', 'structure', 'theorybased', 'discourse', 'tree', '</s>', '<s>', 'however', 'there', 'is', 'still', 'a', 'large', 'difference', 'between', 'the', 'rouge', 'scores', 'of', 'a', 'system', 'with', 'a', 'gold', 'depdt', 'and', 'a', 'system', 'with', 'a', 'depdt', 'obtained', 'from', 'an', 'automatically', 'parsed', 'rstdt', '</s>', '<s>', 'to', 'improve', 'the', 'rouge', 'score', 'we', 'propose', 'a', 'novel', 'discourse', 'parser', 'that', 'directly', 'generates', 'the', 'depdt', '</s>', '<s>', 'the', 'evaluation', 'results', 'showed', 'that', 'the', 'tkp', 'with', 'our', 'parser', 'outperformed', 'that', 'with', 'the', 'stateoftheart', 'rstdt', 'parser', 'and', 'achieved', 'almost', 'equivalent', 'rouge', 'scores', 'to', 'the', 'tkp', 'with', 'the', 'gold', 'depdt', '</s>']\n",
            "['<s>', 'in', 'this', 'paper', 'we', 'present', 'a', 'novel', 'approach', 'for', 'identifying', 'argumentative', 'discourse', 'structures', 'in', 'persuasive', 'essays', '</s>', '<s>', 'the', 'structure', 'of', 'argumentation', 'consists', 'of', 'several', 'components', 'that', 'are', 'connected', 'with', 'argumentative', 'relations', '</s>', '<s>', 'we', 'consider', 'this', 'task', 'in', 'two', 'consecutive', 'steps', '</s>', '<s>', 'first', 'we', 'identify', 'the', 'components', 'of', 'arguments', 'using', 'multiclass', 'classification', '</s>', '<s>', 'second', 'we', 'classify', 'a', 'pair', 'of', 'argument', 'components', 'as', 'either', 'support', 'or', 'nonsupport', 'for', 'identifying', 'the', 'structure', 'of', 'argumentative', 'discourse', '</s>', '<s>', 'for', 'both', 'tasks', 'we', 'evaluate', 'several', 'classifiers', 'and', 'propose', 'novel', 'feature', 'sets', 'including', 'structural', 'lexical', 'syntactic', 'and', 'contextual', 'features', '</s>', '<s>', 'in', 'our', 'experiments', 'we', 'obtain', 'a', 'macro', 'f1score', 'of', '0', '726', 'for', 'identifying', 'argument', 'components', 'and', '0', '722', 'for', 'argumentative', 'relations', '</s>']\n",
            "['<s>', 'a', 'major', 'challenge', 'in', 'document', 'clustering', 'research', 'arises', 'from', 'the', 'growing', 'amount', 'of', 'text', 'data', 'written', 'in', 'different', 'languages', '</s>', '<s>', 'previous', 'approaches', 'depend', 'on', 'languagespecific', 'solutions', 'to', 'evaluate', 'document', 'similarities', 'and', 'the', 'required', 'transformations', 'may', 'alter', 'the', 'original', 'document', 'semantics', '</s>', '<s>', 'to', 'cope', 'with', 'this', 'issue', 'we', 'propose', 'a', 'new', 'document', 'clustering', 'approach', 'for', 'multilingual', 'corpora', 'that', 'exploits', 'a', 'largescale', 'multilingual', 'knowledge', 'base', 'takes', 'advantage', 'of', 'the', 'multitopic', 'nature', 'of', 'the', 'text', 'documents', 'and', 'employs', 'a', 'tensorbased', 'model', 'to', 'deal', 'with', 'high', 'dimensionality', 'and', 'sparseness', '</s>', '<s>', 'results', 'have', 'shown', 'the', 'significance', 'of', 'our', 'approach', 'and', 'its', 'better', 'performance', 'w', 'r', 't', '</s>', '<s>', 'classic', 'document', 'clustering', 'approaches', 'in', 'both', 'a', 'balanced', 'and', 'an', 'unbalanced', 'corpus', 'evaluation', '</s>']\n",
            "['<s>', 'keyboard', 'layout', 'errors', 'and', 'homoglyphs', 'in', 'crosslanguage', 'queries', 'impact', 'our', 'ability', 'to', 'correctly', 'interpret', 'user', 'information', 'needs', 'and', 'offer', 'relevant', 'results', '</s>', '<s>', 'we', 'present', 'a', 'machine', 'learning', 'approach', 'to', 'correcting', 'these', 'errors', 'based', 'largely', 'on', 'characterlevel', 'ngram', 'features', '</s>', '<s>', 'we', 'demonstrate', 'superior', 'performance', 'over', 'rulebased', 'methods', 'as', 'well', 'as', 'a', 'significant', 'reduction', 'in', 'the', 'number', 'of', 'queries', 'that', 'yield', 'null', 'search', 'results', '</s>']\n",
            "['<s>', 'latent', 'dirichlet', 'allocation', 'is', 'a', 'topic', 'model', 'that', 'has', 'been', 'applied', 'to', 'various', 'fields', 'including', 'user', 'profiling', 'and', 'event', 'summarization', 'on', 'twitter', '</s>', '<s>', 'when', 'lda', 'is', 'applied', 'to', 'tweet', 'collections', 'it', 'generally', 'treats', 'all', 'aggregated', 'tweets', 'of', 'a', 'user', 'as', 'a', 'single', 'document', '</s>', '<s>', 'twitterlda', 'which', 'assumes', 'a', 'single', 'tweet', 'consists', 'of', 'a', 'single', 'topic', 'has', 'been', 'proposed', 'and', 'has', 'shown', 'that', 'it', 'is', 'superior', 'in', 'topic', 'semantic', 'coherence', '</s>', '<s>', 'however', 'twitterlda', 'is', 'not', 'capable', 'of', 'online', 'inference', '</s>', '<s>', 'in', 'this', 'study', 'we', 'extend', 'twitterlda', 'in', 'the', 'following', 'two', 'ways', '</s>', '<s>', 'first', 'we', 'model', 'the', 'generation', 'process', 'of', 'tweets', 'more', 'accurately', 'by', 'estimating', 'the', 'ratio', 'between', 'topic', 'words', 'and', 'general', 'words', 'for', 'each', 'user', '</s>', '<s>', 'second', 'we', 'enable', 'it', 'to', 'estimate', 'the', 'dynamics', 'of', 'user', 'interests', 'and', 'topic', 'trends', 'online', 'based', 'on', 'the', 'topic', 'tracking', 'model', 'which', 'models', 'consumer', 'purchase', 'behaviors', '</s>']\n",
            "['<s>', 'previous', 'work', 'often', 'used', 'a', 'pipelined', 'framework', 'where', 'chinese', 'word', 'segmentation', 'is', 'followed', 'by', 'term', 'extraction', 'and', 'keyword', 'extraction', '</s>', '<s>', 'such', 'framework', 'suffers', 'from', 'error', 'propagation', 'and', 'is', 'unable', 'to', 'leverage', 'information', 'in', 'later', 'modules', 'for', 'prior', 'components', '</s>', '<s>', 'in', 'this', 'paper', 'we', 'propose', 'a', 'fourlevel', 'dirichlet', 'process', 'based', 'model', 'to', 'jointly', 'learn', 'the', 'word', 'distributions', 'from', 'the', 'corpus', 'domain', 'and', 'document', 'levels', 'simultaneously', '</s>', '<s>', 'based', 'on', 'the', 'dp4', 'model', 'a', 'sentencewise', 'gibbs', 'sampler', 'is', 'adopted', 'to', 'obtain', 'proper', 'segmentation', 'results', '</s>', '<s>', 'meanwhile', 'terms', 'and', 'keywords', 'are', 'acquired', 'in', 'the', 'sampling', 'process', '</s>', '<s>', 'experimental', 'results', 'have', 'shown', 'the', 'effectiveness', 'of', 'our', 'method', '</s>']\n",
            "['<s>', 'the', 'attention', 'of', 'many', 'researchers', '</s>', '<s>', 'however', 'most', 'of', 'existing', 'work', 'ignore', 'the', 'impor��������', '�������', '�', 'tance', 'of', 'emotion', 'information', 'forevent', 'de��������', '������', 'tection', '</s>', '<s>', 'we', 'argue', 'that', 'people’s', 'emotional', 'reactions', 'immediately', 'reflect', 'the', 'occurring', 'of', 'realworld', 'events', 'and', 'should', 'be', 'important', 'for', 'event', 'detection', '</s>', '<s>', 'in', 'this', 'study', 'we', 'focus', 'on', 'the', 'problem', 'of', 'communityrelated', 'event', 'detection', 'by', 'community', 'emotions', '</s>', '<s>', 'to', 'address', 'the', 'problem', 'we', 'propose', 'a', 'novel', 'framework', 'which', 'include', 'the', 'following', 'three', 'key', 'components:', 'microblog', 'emotion', 'classification', 'community', 'emotion', 'aggregation', 'and', 'community', 'emotion', 'burst', 'detection', '</s>', '<s>', 'we', 'evaluate', 'our', 'approach', 'on', 'real', 'microblog', 'data', 'sets', '</s>', '<s>', 'experimental', 'results', 'demonstrate', 'the', 'effectiveness', 'of', 'the', 'proposed', 'framework', '</s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5XLVdM33k0N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "f2ea65d3-9fda-4ac4-e931-461fbaae8fc6"
      },
      "source": [
        "\n",
        "#write data into .csv with 2 cols: source & summary\n",
        "data = {'text':cores_ngram_tokenized, 'summary':abs_sent}\n",
        "df = pd.DataFrame(data,columns = ['text','summary'])\n",
        "df.info()\n",
        "df.to_csv('/content/drive/My Drive/MA_colab/PG_textRank/textRank.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 16297 entries, 0 to 16296\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   text     16297 non-null  object\n",
            " 1   summary  16297 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 254.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idTxLLlzLYAh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ca627128-0a64-4d03-862f-3c45b07e355b"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[after, annotating, instances, based, on, simp...</td>\n",
              "      <td>[&lt;s&gt;, we, present, a, novel, approach, to, the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[however, ,, these, scores, show, that, having...</td>\n",
              "      <td>[&lt;s&gt;, nearly, all, work, in, unsupervised, gra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[as, shown, in, figure, 1, ,, we, construct, a...</td>\n",
              "      <td>[&lt;s&gt;, stubs, on, wikipedia, often, lack, compr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[lowrank, tensor, representations, following, ...</td>\n",
              "      <td>[&lt;s&gt;, several, compositional, distributional, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[denero, and, uszkoreit, presented, a, preorde...</td>\n",
              "      <td>[&lt;s&gt;, we, present, an, efficient, incremental,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                            summary\n",
              "0  [after, annotating, instances, based, on, simp...  [<s>, we, present, a, novel, approach, to, the...\n",
              "1  [however, ,, these, scores, show, that, having...  [<s>, nearly, all, work, in, unsupervised, gra...\n",
              "2  [as, shown, in, figure, 1, ,, we, construct, a...  [<s>, stubs, on, wikipedia, often, lack, compr...\n",
              "3  [lowrank, tensor, representations, following, ...  [<s>, several, compositional, distributional, ...\n",
              "4  [denero, and, uszkoreit, presented, a, preorde...  [<s>, we, present, an, efficient, incremental,..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dEQ-HqDzW0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dm_single_close_quote = u'\\u2019' # unicode\n",
        "dm_double_close_quote = u'\\u201d'\n",
        "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"] # acceptable ways to end a sentence\n",
        "\n",
        "# We use these to separate the summary sentences in the .bin datafiles\n",
        "SENTENCE_START = '<s>'\n",
        "SENTENCE_END = '</s>'\n",
        "VOCAB_SIZE = 200000\n",
        "CHUNK_SIZE = 500 # num examples per chunk, for the chunked data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjUC9eYrzww9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_train_urls = \"\"\n",
        "all_val_urls = \"\"\n",
        "all_test_urls = \"\"\n",
        "tokenized_dir = default_path + \"tokenized_dir/\"\n",
        "finished_files_dir = default_path + \"finished_files\" #final ouput\n",
        "chunks_dir = os.path.join(finished_files_dir, \"chunked\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_-HgyKXIWfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenized_articles (dataset, path):\n",
        "    for i, row in dataset.iterrows():\n",
        "        filename = str(i) + '.tok'\n",
        "        with open(path + filename ,'w',encoding='utf-8') as fh:\n",
        "            text = row['text']\n",
        "            tok = text\n",
        "            summary = row['summary']\n",
        "            tok.extend(summary)\n",
        "            list = tok.copy()\n",
        "            tok_string = \"\\n\".join(str(x) for x in list)\n",
        "            fh.write(tok_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPfnrghVPDhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_articles(df,tokenized_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmTRN80z23a-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chunk_file(set_name):\n",
        "  in_file = finished_files_dir + '/%s.bin' % set_name\n",
        "  reader = open(in_file, \"rb\")\n",
        "  chunk = 0\n",
        "  finished = False\n",
        "  while not finished:\n",
        "    chunk_fname = os.path.join(chunks_dir, '%s_%03d.bin' % (set_name, chunk)) # new chunk\n",
        "    with open(chunk_fname, 'wb') as writer:\n",
        "      for _ in range(CHUNK_SIZE):\n",
        "        len_bytes = reader.read(8)\n",
        "        if not len_bytes:\n",
        "          finished = True\n",
        "          break\n",
        "        str_len = struct.unpack('q', len_bytes)[0]\n",
        "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
        "        writer.write(struct.pack('q', str_len))\n",
        "        writer.write(struct.pack('%ds' % str_len, example_str))\n",
        "      chunk += 1\n",
        "\n",
        "\n",
        "def chunk_all():\n",
        "  # Make a dir to hold the chunks\n",
        "  if not os.path.isdir(chunks_dir):\n",
        "    os.mkdir(chunks_dir)\n",
        "  # Chunk the data\n",
        "  for set_name in ['train', 'val', 'test']:\n",
        "    print (\"Splitting %s data into chunks...\" % set_name)\n",
        "    chunk_file(set_name)\n",
        "  print (\"Saved chunked data in %s\" % chunks_dir)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32l-p9MBTOTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_art_abs(articles,summaries):\n",
        "    for i in range(len(articles)):\n",
        "        try:\n",
        "            article = \" \".join(articles[i])\n",
        "            abstract = \" \".join(summaries[i])\n",
        "        except IndexError:\n",
        "            print(i)\n",
        "    return article,abstract"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRAJmlFR3SBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_to_bin(file_names, out_file, makevocab=False):\n",
        "  \"\"\"Reads the tokenized .story files corresponding to the urls listed in the url_file and writes them to a out_file.\"\"\"\n",
        "  story_fnames = [str(s) + \".tok\" for s in file_names]\n",
        "  #print(story_fnames)\n",
        "  num_stories = len(story_fnames)\n",
        "  \n",
        "\n",
        "\n",
        "  with open(out_file, 'wb') as writer:\n",
        "      for idx,s in enumerate(story_fnames):\n",
        "          if idx % 500 == 0:\n",
        "            print( \"Writing story %i of %i; %.2f percent done\" % (idx, num_stories, float(idx)*100.0/float(num_stories)))\n",
        "      \n",
        "\n",
        "      for i in file_names:\n",
        "      # Write to tf.Example\n",
        "          article = \" \".join(cores_ngram_tokenized[i])\n",
        "          abstract = \" \".join(abs_sent[i])\n",
        "        \n",
        "      \n",
        "          tf_example = example_pb2.Example()\n",
        "          tf_example.features.feature['article'].bytes_list.value.extend([article.encode('utf-8')])\n",
        "          tf_example.features.feature['abstract'].bytes_list.value.extend([abstract.encode('utf-8')])\n",
        "          tf_example_str = tf_example.SerializeToString()\n",
        "          #print(tf_example_str)\n",
        "          str_len = len(tf_example_str)\n",
        "          writer.write(struct.pack('q', str_len))\n",
        "          writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
        "\n",
        "  print (\"Finished writing file %s\\n\" % out_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA---5TardRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def makevocab (train_text,train_ab):\n",
        "    vocab_counter = collections.Counter()\n",
        "    for i in range(len(train_text)):\n",
        "        abs_tokens = [t.lower() for t in train_ab[i] if t not in [SENTENCE_START, SENTENCE_END]]\n",
        "        train_tokens = [t.lower() for t in train_text[i] if t not in [SENTENCE_START, SENTENCE_END]]\n",
        "        tokens = train_tokens + abs_tokens\n",
        "        tokens = [t.strip() for t in tokens] # strip\n",
        "        tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
        "        vocab_counter.update(tokens)\n",
        "    return vocab_counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc4B19WDsRqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num = df.shape[0]\n",
        "vocab_counter = makevocab(cores_ngram_tokenized[:num-2000],abs_sent[:num-2000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHx6BcNEv2Mi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8b4ff122-b79a-489d-9fcf-175f0470b21c"
      },
      "source": [
        "print(len(vocab_counter))\n",
        "vocab_counter['</s>']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "139923\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r84KYbdRssB8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a9cc1615-f553-4335-8c2c-728798f8605a"
      },
      "source": [
        "print(len(vocab_counter))\n",
        "print (\"Writing vocab file...\")\n",
        "with open(os.path.join(finished_files_dir, \"vocab\"), 'w', encoding=\"utf-8\") as writer:\n",
        "      for word, count in vocab_counter.most_common(VOCAB_SIZE):\n",
        "        writer.write(word + ' ' + str(count) + '\\n')\n",
        "print (\"Finished writing vocab file\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "139923\n",
            "Writing vocab file...\n",
            "Finished writing vocab file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5po4D943XBY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "f85de28e-638f-4a7e-d802-e5c7f340d111"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  num = df.shape[0]\n",
        "\n",
        "\n",
        "  all_train_urls = range(0,num-2000)\n",
        "  all_val_urls = range(num-2000,num-1000)\n",
        "  all_test_urls = range(num-1000,num)\n",
        "\n",
        "  #for testing\n",
        "  #all_train_urls= range(0,5)\n",
        "  #all_val_urls = range(5,10)\n",
        "  #all_test_urls = range(10,15)\n",
        "\n",
        "  # Read the tokenized stories, do a little postprocessing then write to bin files\n",
        "  write_to_bin(all_test_urls, os.path.join(finished_files_dir, \"test.bin\"))\n",
        "  write_to_bin(all_val_urls, os.path.join(finished_files_dir, \"val.bin\"))\n",
        "  write_to_bin(all_train_urls, os.path.join(finished_files_dir, \"train.bin\"), makevocab=True)\n",
        "\n",
        "  # Chunk the data. This splits each of train.bin, val.bin and test.bin into smaller chunks, each containing e.g. 1000 examples, and saves them in finished_files/chunks\n",
        "  chunk_all()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing story 0 of 1000; 0.00 percent done\n",
            "Writing story 500 of 1000; 50.00 percent done\n",
            "Finished writing file /content/drive/My Drive/MA_colab/PG_textRank/finished_files/test.bin\n",
            "\n",
            "Writing story 0 of 1000; 0.00 percent done\n",
            "Writing story 500 of 1000; 50.00 percent done\n",
            "Finished writing file /content/drive/My Drive/MA_colab/PG_textRank/finished_files/val.bin\n",
            "\n",
            "Writing story 0 of 14297; 0.00 percent done\n",
            "Writing story 500 of 14297; 3.50 percent done\n",
            "Writing story 1000 of 14297; 6.99 percent done\n",
            "Writing story 1500 of 14297; 10.49 percent done\n",
            "Writing story 2000 of 14297; 13.99 percent done\n",
            "Writing story 2500 of 14297; 17.49 percent done\n",
            "Writing story 3000 of 14297; 20.98 percent done\n",
            "Writing story 3500 of 14297; 24.48 percent done\n",
            "Writing story 4000 of 14297; 27.98 percent done\n",
            "Writing story 4500 of 14297; 31.48 percent done\n",
            "Writing story 5000 of 14297; 34.97 percent done\n",
            "Writing story 5500 of 14297; 38.47 percent done\n",
            "Writing story 6000 of 14297; 41.97 percent done\n",
            "Writing story 6500 of 14297; 45.46 percent done\n",
            "Writing story 7000 of 14297; 48.96 percent done\n",
            "Writing story 7500 of 14297; 52.46 percent done\n",
            "Writing story 8000 of 14297; 55.96 percent done\n",
            "Writing story 8500 of 14297; 59.45 percent done\n",
            "Writing story 9000 of 14297; 62.95 percent done\n",
            "Writing story 9500 of 14297; 66.45 percent done\n",
            "Writing story 10000 of 14297; 69.94 percent done\n",
            "Writing story 10500 of 14297; 73.44 percent done\n",
            "Writing story 11000 of 14297; 76.94 percent done\n",
            "Writing story 11500 of 14297; 80.44 percent done\n",
            "Writing story 12000 of 14297; 83.93 percent done\n",
            "Writing story 12500 of 14297; 87.43 percent done\n",
            "Writing story 13000 of 14297; 90.93 percent done\n",
            "Writing story 13500 of 14297; 94.43 percent done\n",
            "Writing story 14000 of 14297; 97.92 percent done\n",
            "Finished writing file /content/drive/My Drive/MA_colab/PG_textRank/finished_files/train.bin\n",
            "\n",
            "Splitting train data into chunks...\n",
            "Splitting val data into chunks...\n",
            "Splitting test data into chunks...\n",
            "Saved chunked data in /content/drive/My Drive/MA_colab/PG_textRank/finished_files/chunked\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}